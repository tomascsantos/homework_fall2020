{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q1_MsPacman-v0_14-10-2020_19-06-06 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q1_MsPacman-v0_14-10-2020_19-06-06\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.003747\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0037469863891601562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) 394.426230\n",
      "best mean reward -inf\n",
      "running time 43.737002\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : 394.42622950819674\n",
      "TimeSinceStart : 43.73700189590454\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) 410.800000\n",
      "best mean reward 410.800000\n",
      "running time 86.108413\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : 410.8\n",
      "Train_BestReturn : 410.8\n",
      "TimeSinceStart : 86.1084132194519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) 421.400000\n",
      "best mean reward 421.400000\n",
      "running time 128.830734\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : 421.4\n",
      "Train_BestReturn : 421.4\n",
      "TimeSinceStart : 128.83073353767395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) 421.900000\n",
      "best mean reward 421.900000\n",
      "running time 171.730096\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : 421.9\n",
      "Train_BestReturn : 421.9\n",
      "TimeSinceStart : 171.73009586334229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) 416.900000\n",
      "best mean reward 421.900000\n",
      "running time 214.146295\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : 416.9\n",
      "Train_BestReturn : 421.9\n",
      "TimeSinceStart : 214.1462950706482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) 411.600000\n",
      "best mean reward 421.900000\n",
      "running time 280.303085\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : 411.6\n",
      "Train_BestReturn : 421.9\n",
      "TimeSinceStart : 280.3030846118927\n",
      "Training Loss : 0.06284881383180618\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) 428.200000\n",
      "best mean reward 428.200000\n",
      "running time 344.824422\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : 428.2\n",
      "Train_BestReturn : 428.2\n",
      "TimeSinceStart : 344.8244216442108\n",
      "Training Loss : 0.07793545722961426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) 456.300000\n",
      "best mean reward 456.300000\n",
      "running time 409.216025\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : 456.3\n",
      "Train_BestReturn : 456.3\n",
      "TimeSinceStart : 409.2160246372223\n",
      "Training Loss : 0.11472822725772858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) 471.200000\n",
      "best mean reward 471.200000\n",
      "running time 473.976371\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : 471.2\n",
      "Train_BestReturn : 471.2\n",
      "TimeSinceStart : 473.9763705730438\n",
      "Training Loss : 0.07715030759572983\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) 476.500000\n",
      "best mean reward 476.500000\n",
      "running time 539.074177\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : 476.5\n",
      "Train_BestReturn : 476.5\n",
      "TimeSinceStart : 539.0741765499115\n",
      "Training Loss : 0.0772564485669136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) 507.200000\n",
      "best mean reward 507.200000\n",
      "running time 604.746368\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : 507.2\n",
      "Train_BestReturn : 507.2\n",
      "TimeSinceStart : 604.7463684082031\n",
      "Training Loss : 0.06485063582658768\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) 529.500000\n",
      "best mean reward 529.500000\n",
      "running time 669.967892\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : 529.5\n",
      "Train_BestReturn : 529.5\n",
      "TimeSinceStart : 669.9678921699524\n",
      "Training Loss : 0.04559540003538132\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 470.000000\n",
      "best mean reward 529.500000\n",
      "running time 735.418515\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 470.0\n",
      "Train_BestReturn : 529.5\n",
      "TimeSinceStart : 735.4185147285461\n",
      "Training Loss : 0.0511808916926384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 451.400000\n",
      "best mean reward 529.500000\n",
      "running time 800.314410\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 451.4\n",
      "Train_BestReturn : 529.5\n",
      "TimeSinceStart : 800.3144099712372\n",
      "Training Loss : 0.06275783479213715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 457.700000\n",
      "best mean reward 529.500000\n",
      "running time 865.423941\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 457.7\n",
      "Train_BestReturn : 529.5\n",
      "TimeSinceStart : 865.4239408969879\n",
      "Training Loss : 0.10351940989494324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 523.300000\n",
      "best mean reward 529.500000\n",
      "running time 930.905819\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 523.3\n",
      "Train_BestReturn : 529.5\n",
      "TimeSinceStart : 930.9058194160461\n",
      "Training Loss : 0.09657086431980133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 567.200000\n",
      "best mean reward 567.200000\n",
      "running time 997.022027\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 567.2\n",
      "Train_BestReturn : 567.2\n",
      "TimeSinceStart : 997.0220267772675\n",
      "Training Loss : 0.0815599113702774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 559.900000\n",
      "best mean reward 567.200000\n",
      "running time 1063.635808\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 559.9\n",
      "Train_BestReturn : 567.2\n",
      "TimeSinceStart : 1063.635808467865\n",
      "Training Loss : 0.13179679214954376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 557.000000\n",
      "best mean reward 567.200000\n",
      "running time 1130.134401\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 557.0\n",
      "Train_BestReturn : 567.2\n",
      "TimeSinceStart : 1130.1344013214111\n",
      "Training Loss : 0.23914559185504913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 536.900000\n",
      "best mean reward 567.200000\n",
      "running time 1196.714162\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 536.9\n",
      "Train_BestReturn : 567.2\n",
      "TimeSinceStart : 1196.7141621112823\n",
      "Training Loss : 0.10528481006622314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 643.400000\n",
      "best mean reward 643.400000\n",
      "running time 1259.650042\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 643.4\n",
      "Train_BestReturn : 643.4\n",
      "TimeSinceStart : 1259.6500415802002\n",
      "Training Loss : 0.09005223959684372\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 612.000000\n",
      "best mean reward 643.400000\n",
      "running time 1322.239977\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 612.0\n",
      "Train_BestReturn : 643.4\n",
      "TimeSinceStart : 1322.2399773597717\n",
      "Training Loss : 0.18057435750961304\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 533.100000\n",
      "best mean reward 643.400000\n",
      "running time 1388.373870\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 533.1\n",
      "Train_BestReturn : 643.4\n",
      "TimeSinceStart : 1388.3738701343536\n",
      "Training Loss : 0.09352760016918182\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 548.300000\n",
      "best mean reward 643.400000\n",
      "running time 1454.063586\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 548.3\n",
      "Train_BestReturn : 643.4\n",
      "TimeSinceStart : 1454.0635857582092\n",
      "Training Loss : 0.1391441822052002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 596.900000\n",
      "best mean reward 643.400000\n",
      "running time 1520.306177\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 596.9\n",
      "Train_BestReturn : 643.4\n",
      "TimeSinceStart : 1520.306176662445\n",
      "Training Loss : 0.26353174448013306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 644.300000\n",
      "best mean reward 644.300000\n",
      "running time 1586.429681\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 644.3\n",
      "Train_BestReturn : 644.3\n",
      "TimeSinceStart : 1586.429681301117\n",
      "Training Loss : 0.14623385667800903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 644.900000\n",
      "best mean reward 644.900000\n",
      "running time 1652.842619\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 644.9\n",
      "Train_BestReturn : 644.9\n",
      "TimeSinceStart : 1652.842619419098\n",
      "Training Loss : 0.17330807447433472\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 656.900000\n",
      "best mean reward 656.900000\n",
      "running time 1719.483078\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 656.9\n",
      "Train_BestReturn : 656.9\n",
      "TimeSinceStart : 1719.4830784797668\n",
      "Training Loss : 0.29747113585472107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 602.500000\n",
      "best mean reward 656.900000\n",
      "running time 1785.622916\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 602.5\n",
      "Train_BestReturn : 656.9\n",
      "TimeSinceStart : 1785.6229157447815\n",
      "Training Loss : 0.20274901390075684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 629.100000\n",
      "best mean reward 656.900000\n",
      "running time 1851.658786\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 629.1\n",
      "Train_BestReturn : 656.9\n",
      "TimeSinceStart : 1851.6587858200073\n",
      "Training Loss : 0.27746617794036865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 633.900000\n",
      "best mean reward 656.900000\n",
      "running time 1917.482521\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 633.9\n",
      "Train_BestReturn : 656.9\n",
      "TimeSinceStart : 1917.482521057129\n",
      "Training Loss : 0.24770882725715637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 659.400000\n",
      "best mean reward 659.400000\n",
      "running time 1983.446122\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 659.4\n",
      "Train_BestReturn : 659.4\n",
      "TimeSinceStart : 1983.4461221694946\n",
      "Training Loss : 0.09340119361877441\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 691.500000\n",
      "best mean reward 691.500000\n",
      "running time 2050.558184\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 691.5\n",
      "Train_BestReturn : 691.5\n",
      "TimeSinceStart : 2050.5581839084625\n",
      "Training Loss : 0.1314314305782318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 671.000000\n",
      "best mean reward 691.500000\n",
      "running time 2116.907656\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 671.0\n",
      "Train_BestReturn : 691.5\n",
      "TimeSinceStart : 2116.9076557159424\n",
      "Training Loss : 0.4157913327217102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 755.900000\n",
      "best mean reward 755.900000\n",
      "running time 2183.318122\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 755.9\n",
      "Train_BestReturn : 755.9\n",
      "TimeSinceStart : 2183.3181216716766\n",
      "Training Loss : 0.415176123380661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 834.600000\n",
      "best mean reward 834.600000\n",
      "running time 2249.985523\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 834.6\n",
      "Train_BestReturn : 834.6\n",
      "TimeSinceStart : 2249.985522508621\n",
      "Training Loss : 0.21180999279022217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 780.400000\n",
      "best mean reward 834.600000\n",
      "running time 2316.008043\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 780.4\n",
      "Train_BestReturn : 834.6\n",
      "TimeSinceStart : 2316.0080428123474\n",
      "Training Loss : 0.3877970576286316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 754.300000\n",
      "best mean reward 834.600000\n",
      "running time 2381.900950\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 754.3\n",
      "Train_BestReturn : 834.6\n",
      "TimeSinceStart : 2381.900950193405\n",
      "Training Loss : 0.20458447933197021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 804.000000\n",
      "best mean reward 834.600000\n",
      "running time 2448.134235\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 804.0\n",
      "Train_BestReturn : 834.6\n",
      "TimeSinceStart : 2448.13423538208\n",
      "Training Loss : 0.12672241032123566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 886.000000\n",
      "best mean reward 886.000000\n",
      "running time 2514.049683\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 886.0\n",
      "Train_BestReturn : 886.0\n",
      "TimeSinceStart : 2514.049682855606\n",
      "Training Loss : 0.3626347780227661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 844.400000\n",
      "best mean reward 886.000000\n",
      "running time 2581.020495\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 844.4\n",
      "Train_BestReturn : 886.0\n",
      "TimeSinceStart : 2581.020495414734\n",
      "Training Loss : 0.23121951520442963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 780.200000\n",
      "best mean reward 886.000000\n",
      "running time 2647.125823\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 780.2\n",
      "Train_BestReturn : 886.0\n",
      "TimeSinceStart : 2647.125823020935\n",
      "Training Loss : 0.5186306238174438\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 797.800000\n",
      "best mean reward 886.000000\n",
      "running time 2713.569813\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 797.8\n",
      "Train_BestReturn : 886.0\n",
      "TimeSinceStart : 2713.5698125362396\n",
      "Training Loss : 0.22375212609767914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 812.800000\n",
      "best mean reward 886.000000\n",
      "running time 2780.059125\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 812.8\n",
      "Train_BestReturn : 886.0\n",
      "TimeSinceStart : 2780.0591249465942\n",
      "Training Loss : 0.2546558082103729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 875.800000\n",
      "best mean reward 886.000000\n",
      "running time 2842.231654\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 875.8\n",
      "Train_BestReturn : 886.0\n",
      "TimeSinceStart : 2842.231653690338\n",
      "Training Loss : 0.23477426171302795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 898.500000\n",
      "best mean reward 898.500000\n",
      "running time 2903.281663\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 898.5\n",
      "Train_BestReturn : 898.5\n",
      "TimeSinceStart : 2903.2816631793976\n",
      "Training Loss : 0.4965842366218567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 845.800000\n",
      "best mean reward 898.500000\n",
      "running time 2964.237888\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 845.8\n",
      "Train_BestReturn : 898.5\n",
      "TimeSinceStart : 2964.237888097763\n",
      "Training Loss : 0.32478073239326477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 823.700000\n",
      "best mean reward 898.500000\n",
      "running time 3025.435090\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 823.7\n",
      "Train_BestReturn : 898.5\n",
      "TimeSinceStart : 3025.4350895881653\n",
      "Training Loss : 0.27175337076187134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 836.100000\n",
      "best mean reward 898.500000\n",
      "running time 3086.701864\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 836.1\n",
      "Train_BestReturn : 898.5\n",
      "TimeSinceStart : 3086.7018642425537\n",
      "Training Loss : 0.23049858212471008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 500001\n",
      "mean reward (100 episodes) 880.700000\n",
      "best mean reward 898.500000\n",
      "running time 3147.841480\n",
      "Train_EnvstepsSoFar : 500001\n",
      "Train_AverageReturn : 880.7\n",
      "Train_BestReturn : 898.5\n",
      "TimeSinceStart : 3147.8414804935455\n",
      "Training Loss : 0.8461501598358154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 510001\n",
      "mean reward (100 episodes) 889.600000\n",
      "best mean reward 898.500000\n",
      "running time 3209.122776\n",
      "Train_EnvstepsSoFar : 510001\n",
      "Train_AverageReturn : 889.6\n",
      "Train_BestReturn : 898.5\n",
      "TimeSinceStart : 3209.122776031494\n",
      "Training Loss : 0.610871434211731\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 520001\n",
      "mean reward (100 episodes) 961.700000\n",
      "best mean reward 961.700000\n",
      "running time 3270.193234\n",
      "Train_EnvstepsSoFar : 520001\n",
      "Train_AverageReturn : 961.7\n",
      "Train_BestReturn : 961.7\n",
      "TimeSinceStart : 3270.1932344436646\n",
      "Training Loss : 0.3159250020980835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 530001\n",
      "mean reward (100 episodes) 1037.400000\n",
      "best mean reward 1037.400000\n",
      "running time 3331.794113\n",
      "Train_EnvstepsSoFar : 530001\n",
      "Train_AverageReturn : 1037.4\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3331.7941131591797\n",
      "Training Loss : 0.5172668695449829\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 540001\n",
      "mean reward (100 episodes) 926.100000\n",
      "best mean reward 1037.400000\n",
      "running time 3393.600131\n",
      "Train_EnvstepsSoFar : 540001\n",
      "Train_AverageReturn : 926.1\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3393.6001307964325\n",
      "Training Loss : 0.25261032581329346\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 550001\n",
      "mean reward (100 episodes) 841.800000\n",
      "best mean reward 1037.400000\n",
      "running time 3454.959315\n",
      "Train_EnvstepsSoFar : 550001\n",
      "Train_AverageReturn : 841.8\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3454.9593148231506\n",
      "Training Loss : 0.48972591757774353\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 560001\n",
      "mean reward (100 episodes) 935.500000\n",
      "best mean reward 1037.400000\n",
      "running time 3516.771210\n",
      "Train_EnvstepsSoFar : 560001\n",
      "Train_AverageReturn : 935.5\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3516.7712099552155\n",
      "Training Loss : 0.20104870200157166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 570001\n",
      "mean reward (100 episodes) 975.900000\n",
      "best mean reward 1037.400000\n",
      "running time 3578.181174\n",
      "Train_EnvstepsSoFar : 570001\n",
      "Train_AverageReturn : 975.9\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3578.1811740398407\n",
      "Training Loss : 0.5953940153121948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 580001\n",
      "mean reward (100 episodes) 944.900000\n",
      "best mean reward 1037.400000\n",
      "running time 3639.596262\n",
      "Train_EnvstepsSoFar : 580001\n",
      "Train_AverageReturn : 944.9\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3639.5962615013123\n",
      "Training Loss : 0.5652867555618286\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 590001\n",
      "mean reward (100 episodes) 972.700000\n",
      "best mean reward 1037.400000\n",
      "running time 3701.755566\n",
      "Train_EnvstepsSoFar : 590001\n",
      "Train_AverageReturn : 972.7\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3701.755565881729\n",
      "Training Loss : 0.33015161752700806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 600001\n",
      "mean reward (100 episodes) 1008.400000\n",
      "best mean reward 1037.400000\n",
      "running time 3763.640827\n",
      "Train_EnvstepsSoFar : 600001\n",
      "Train_AverageReturn : 1008.4\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3763.640827178955\n",
      "Training Loss : 0.4856303930282593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 610001\n",
      "mean reward (100 episodes) 1001.900000\n",
      "best mean reward 1037.400000\n",
      "running time 3825.605443\n",
      "Train_EnvstepsSoFar : 610001\n",
      "Train_AverageReturn : 1001.9\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3825.6054434776306\n",
      "Training Loss : 0.47005629539489746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 620001\n",
      "mean reward (100 episodes) 1031.700000\n",
      "best mean reward 1037.400000\n",
      "running time 3887.726873\n",
      "Train_EnvstepsSoFar : 620001\n",
      "Train_AverageReturn : 1031.7\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3887.726873397827\n",
      "Training Loss : 0.2676779627799988\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 630001\n",
      "mean reward (100 episodes) 1019.000000\n",
      "best mean reward 1037.400000\n",
      "running time 3950.347713\n",
      "Train_EnvstepsSoFar : 630001\n",
      "Train_AverageReturn : 1019.0\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 3950.3477125167847\n",
      "Training Loss : 0.49972373247146606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 640001\n",
      "mean reward (100 episodes) 1004.300000\n",
      "best mean reward 1037.400000\n",
      "running time 4012.982042\n",
      "Train_EnvstepsSoFar : 640001\n",
      "Train_AverageReturn : 1004.3\n",
      "Train_BestReturn : 1037.4\n",
      "TimeSinceStart : 4012.9820415973663\n",
      "Training Loss : 0.4368225932121277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 650001\n",
      "mean reward (100 episodes) 1045.900000\n",
      "best mean reward 1045.900000\n",
      "running time 4075.531613\n",
      "Train_EnvstepsSoFar : 650001\n",
      "Train_AverageReturn : 1045.9\n",
      "Train_BestReturn : 1045.9\n",
      "TimeSinceStart : 4075.531613111496\n",
      "Training Loss : 0.1804787963628769\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 660001\n",
      "mean reward (100 episodes) 1011.000000\n",
      "best mean reward 1045.900000\n",
      "running time 4138.270155\n",
      "Train_EnvstepsSoFar : 660001\n",
      "Train_AverageReturn : 1011.0\n",
      "Train_BestReturn : 1045.9\n",
      "TimeSinceStart : 4138.2701551914215\n",
      "Training Loss : 0.41616731882095337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 670001\n",
      "mean reward (100 episodes) 1036.700000\n",
      "best mean reward 1045.900000\n",
      "running time 4201.234072\n",
      "Train_EnvstepsSoFar : 670001\n",
      "Train_AverageReturn : 1036.7\n",
      "Train_BestReturn : 1045.9\n",
      "TimeSinceStart : 4201.2340722084045\n",
      "Training Loss : 0.5173549652099609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 680001\n",
      "mean reward (100 episodes) 1048.400000\n",
      "best mean reward 1048.400000\n",
      "running time 4263.901286\n",
      "Train_EnvstepsSoFar : 680001\n",
      "Train_AverageReturn : 1048.4\n",
      "Train_BestReturn : 1048.4\n",
      "TimeSinceStart : 4263.9012858867645\n",
      "Training Loss : 0.5009990930557251\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 690001\n",
      "mean reward (100 episodes) 1038.500000\n",
      "best mean reward 1048.400000\n",
      "running time 4327.138684\n",
      "Train_EnvstepsSoFar : 690001\n",
      "Train_AverageReturn : 1038.5\n",
      "Train_BestReturn : 1048.4\n",
      "TimeSinceStart : 4327.13868355751\n",
      "Training Loss : 0.3003093898296356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 700001\n",
      "mean reward (100 episodes) 1111.600000\n",
      "best mean reward 1111.600000\n",
      "running time 4393.409939\n",
      "Train_EnvstepsSoFar : 700001\n",
      "Train_AverageReturn : 1111.6\n",
      "Train_BestReturn : 1111.6\n",
      "TimeSinceStart : 4393.409939289093\n",
      "Training Loss : 0.48773789405822754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 710001\n",
      "mean reward (100 episodes) 1164.600000\n",
      "best mean reward 1164.600000\n",
      "running time 4457.429924\n",
      "Train_EnvstepsSoFar : 710001\n",
      "Train_AverageReturn : 1164.6\n",
      "Train_BestReturn : 1164.6\n",
      "TimeSinceStart : 4457.4299240112305\n",
      "Training Loss : 0.4757578372955322\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 720001\n",
      "mean reward (100 episodes) 1213.800000\n",
      "best mean reward 1213.800000\n",
      "running time 4520.226999\n",
      "Train_EnvstepsSoFar : 720001\n",
      "Train_AverageReturn : 1213.8\n",
      "Train_BestReturn : 1213.8\n",
      "TimeSinceStart : 4520.226998806\n",
      "Training Loss : 0.44474080204963684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 730001\n",
      "mean reward (100 episodes) 1232.500000\n",
      "best mean reward 1232.500000\n",
      "running time 4583.044397\n",
      "Train_EnvstepsSoFar : 730001\n",
      "Train_AverageReturn : 1232.5\n",
      "Train_BestReturn : 1232.5\n",
      "TimeSinceStart : 4583.044397354126\n",
      "Training Loss : 1.0777740478515625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 740001\n",
      "mean reward (100 episodes) 1189.000000\n",
      "best mean reward 1232.500000\n",
      "running time 4646.189774\n",
      "Train_EnvstepsSoFar : 740001\n",
      "Train_AverageReturn : 1189.0\n",
      "Train_BestReturn : 1232.5\n",
      "TimeSinceStart : 4646.189773797989\n",
      "Training Loss : 0.3817300796508789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 750001\n",
      "mean reward (100 episodes) 1251.200000\n",
      "best mean reward 1251.200000\n",
      "running time 4709.448646\n",
      "Train_EnvstepsSoFar : 750001\n",
      "Train_AverageReturn : 1251.2\n",
      "Train_BestReturn : 1251.2\n",
      "TimeSinceStart : 4709.448645830154\n",
      "Training Loss : 0.45170721411705017\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 760001\n",
      "mean reward (100 episodes) 1292.800000\n",
      "best mean reward 1292.800000\n",
      "running time 4772.400369\n",
      "Train_EnvstepsSoFar : 760001\n",
      "Train_AverageReturn : 1292.8\n",
      "Train_BestReturn : 1292.8\n",
      "TimeSinceStart : 4772.4003694057465\n",
      "Training Loss : 0.4612363278865814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 770001\n",
      "mean reward (100 episodes) 1266.300000\n",
      "best mean reward 1292.800000\n",
      "running time 4835.553643\n",
      "Train_EnvstepsSoFar : 770001\n",
      "Train_AverageReturn : 1266.3\n",
      "Train_BestReturn : 1292.8\n",
      "TimeSinceStart : 4835.5536432266235\n",
      "Training Loss : 0.9547234773635864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 780001\n",
      "mean reward (100 episodes) 1274.300000\n",
      "best mean reward 1292.800000\n",
      "running time 4899.514816\n",
      "Train_EnvstepsSoFar : 780001\n",
      "Train_AverageReturn : 1274.3\n",
      "Train_BestReturn : 1292.8\n",
      "TimeSinceStart : 4899.51481628418\n",
      "Training Loss : 0.32383960485458374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 790001\n",
      "mean reward (100 episodes) 1336.200000\n",
      "best mean reward 1336.200000\n",
      "running time 4962.389760\n",
      "Train_EnvstepsSoFar : 790001\n",
      "Train_AverageReturn : 1336.2\n",
      "Train_BestReturn : 1336.2\n",
      "TimeSinceStart : 4962.389759778976\n",
      "Training Loss : 0.45433279871940613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 800001\n",
      "mean reward (100 episodes) 1360.300000\n",
      "best mean reward 1360.300000\n",
      "running time 5025.537374\n",
      "Train_EnvstepsSoFar : 800001\n",
      "Train_AverageReturn : 1360.3\n",
      "Train_BestReturn : 1360.3\n",
      "TimeSinceStart : 5025.537373781204\n",
      "Training Loss : 0.3778533637523651\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 810001\n",
      "mean reward (100 episodes) 1429.900000\n",
      "best mean reward 1429.900000\n",
      "running time 5088.824913\n",
      "Train_EnvstepsSoFar : 810001\n",
      "Train_AverageReturn : 1429.9\n",
      "Train_BestReturn : 1429.9\n",
      "TimeSinceStart : 5088.824913024902\n",
      "Training Loss : 0.5923051238059998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 820001\n",
      "mean reward (100 episodes) 1478.100000\n",
      "best mean reward 1478.100000\n",
      "running time 5152.300990\n",
      "Train_EnvstepsSoFar : 820001\n",
      "Train_AverageReturn : 1478.1\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5152.300990343094\n",
      "Training Loss : 0.2724340558052063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 830001\n",
      "mean reward (100 episodes) 1477.100000\n",
      "best mean reward 1478.100000\n",
      "running time 5215.578105\n",
      "Train_EnvstepsSoFar : 830001\n",
      "Train_AverageReturn : 1477.1\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5215.578104972839\n",
      "Training Loss : 0.4358264207839966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 840001\n",
      "mean reward (100 episodes) 1371.300000\n",
      "best mean reward 1478.100000\n",
      "running time 5278.893242\n",
      "Train_EnvstepsSoFar : 840001\n",
      "Train_AverageReturn : 1371.3\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5278.893241643906\n",
      "Training Loss : 0.5907429456710815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 850001\n",
      "mean reward (100 episodes) 1291.900000\n",
      "best mean reward 1478.100000\n",
      "running time 5342.570425\n",
      "Train_EnvstepsSoFar : 850001\n",
      "Train_AverageReturn : 1291.9\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5342.570424556732\n",
      "Training Loss : 0.5916708111763\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 860001\n",
      "mean reward (100 episodes) 1292.000000\n",
      "best mean reward 1478.100000\n",
      "running time 5410.503762\n",
      "Train_EnvstepsSoFar : 860001\n",
      "Train_AverageReturn : 1292.0\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5410.50376200676\n",
      "Training Loss : 0.4053579568862915\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 870001\n",
      "mean reward (100 episodes) 1382.400000\n",
      "best mean reward 1478.100000\n",
      "running time 5474.400044\n",
      "Train_EnvstepsSoFar : 870001\n",
      "Train_AverageReturn : 1382.4\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5474.400043725967\n",
      "Training Loss : 0.32892197370529175\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 880001\n",
      "mean reward (100 episodes) 1377.800000\n",
      "best mean reward 1478.100000\n",
      "running time 5538.036855\n",
      "Train_EnvstepsSoFar : 880001\n",
      "Train_AverageReturn : 1377.8\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5538.0368547439575\n",
      "Training Loss : 0.32357725501060486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 890001\n",
      "mean reward (100 episodes) 1330.800000\n",
      "best mean reward 1478.100000\n",
      "running time 5601.979563\n",
      "Train_EnvstepsSoFar : 890001\n",
      "Train_AverageReturn : 1330.8\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5601.979563474655\n",
      "Training Loss : 0.7979356646537781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 900001\n",
      "mean reward (100 episodes) 1357.000000\n",
      "best mean reward 1478.100000\n",
      "running time 5667.089363\n",
      "Train_EnvstepsSoFar : 900001\n",
      "Train_AverageReturn : 1357.0\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5667.089362859726\n",
      "Training Loss : 0.3087887763977051\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 910001\n",
      "mean reward (100 episodes) 1401.700000\n",
      "best mean reward 1478.100000\n",
      "running time 5734.974735\n",
      "Train_EnvstepsSoFar : 910001\n",
      "Train_AverageReturn : 1401.7\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5734.974735498428\n",
      "Training Loss : 0.8640296459197998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 920001\n",
      "mean reward (100 episodes) 1411.100000\n",
      "best mean reward 1478.100000\n",
      "running time 5799.170974\n",
      "Train_EnvstepsSoFar : 920001\n",
      "Train_AverageReturn : 1411.1\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5799.170973777771\n",
      "Training Loss : 0.4731898605823517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 930001\n",
      "mean reward (100 episodes) 1365.900000\n",
      "best mean reward 1478.100000\n",
      "running time 5863.265094\n",
      "Train_EnvstepsSoFar : 930001\n",
      "Train_AverageReturn : 1365.9\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5863.265093564987\n",
      "Training Loss : 0.5033196210861206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 940001\n",
      "mean reward (100 episodes) 1405.200000\n",
      "best mean reward 1478.100000\n",
      "running time 5926.915846\n",
      "Train_EnvstepsSoFar : 940001\n",
      "Train_AverageReturn : 1405.2\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5926.915845632553\n",
      "Training Loss : 1.1283057928085327\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 950001\n",
      "mean reward (100 episodes) 1453.600000\n",
      "best mean reward 1478.100000\n",
      "running time 5991.167115\n",
      "Train_EnvstepsSoFar : 950001\n",
      "Train_AverageReturn : 1453.6\n",
      "Train_BestReturn : 1478.1\n",
      "TimeSinceStart : 5991.16711473465\n",
      "Training Loss : 0.39748498797416687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 960001\n",
      "mean reward (100 episodes) 1515.400000\n",
      "best mean reward 1515.400000\n",
      "running time 6055.661707\n",
      "Train_EnvstepsSoFar : 960001\n",
      "Train_AverageReturn : 1515.4\n",
      "Train_BestReturn : 1515.4\n",
      "TimeSinceStart : 6055.66170668602\n",
      "Training Loss : 0.4005291163921356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 970001\n",
      "mean reward (100 episodes) 1522.200000\n",
      "best mean reward 1522.200000\n",
      "running time 6119.871987\n",
      "Train_EnvstepsSoFar : 970001\n",
      "Train_AverageReturn : 1522.2\n",
      "Train_BestReturn : 1522.2\n",
      "TimeSinceStart : 6119.8719873428345\n",
      "Training Loss : 1.4096475839614868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 980001\n",
      "mean reward (100 episodes) 1496.600000\n",
      "best mean reward 1522.200000\n",
      "running time 6185.104741\n",
      "Train_EnvstepsSoFar : 980001\n",
      "Train_AverageReturn : 1496.6\n",
      "Train_BestReturn : 1522.2\n",
      "TimeSinceStart : 6185.104741096497\n",
      "Training Loss : 0.46171754598617554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 990001\n",
      "mean reward (100 episodes) 1501.300000\n",
      "best mean reward 1522.200000\n",
      "running time 6249.893412\n",
      "Train_EnvstepsSoFar : 990001\n",
      "Train_AverageReturn : 1501.3\n",
      "Train_BestReturn : 1522.2\n",
      "TimeSinceStart : 6249.893411874771\n",
      "Training Loss : 0.4108395278453827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1000001\n",
      "mean reward (100 episodes) 1487.800000\n",
      "best mean reward 1522.200000\n",
      "running time 6314.943484\n",
      "Train_EnvstepsSoFar : 1000001\n",
      "Train_AverageReturn : 1487.8\n",
      "Train_BestReturn : 1522.2\n",
      "TimeSinceStart : 6314.943484306335\n",
      "Training Loss : 0.408855140209198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1010001\n",
      "mean reward (100 episodes) 1515.800000\n",
      "best mean reward 1522.200000\n",
      "running time 6379.650041\n",
      "Train_EnvstepsSoFar : 1010001\n",
      "Train_AverageReturn : 1515.8\n",
      "Train_BestReturn : 1522.2\n",
      "TimeSinceStart : 6379.650041341782\n",
      "Training Loss : 0.789309024810791\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1020001\n",
      "mean reward (100 episodes) 1499.200000\n",
      "best mean reward 1522.200000\n",
      "running time 6444.818224\n",
      "Train_EnvstepsSoFar : 1020001\n",
      "Train_AverageReturn : 1499.2\n",
      "Train_BestReturn : 1522.2\n",
      "TimeSinceStart : 6444.8182237148285\n",
      "Training Loss : 0.25524282455444336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1030001\n",
      "mean reward (100 episodes) 1550.400000\n",
      "best mean reward 1550.400000\n",
      "running time 6509.742229\n",
      "Train_EnvstepsSoFar : 1030001\n",
      "Train_AverageReturn : 1550.4\n",
      "Train_BestReturn : 1550.4\n",
      "TimeSinceStart : 6509.74222946167\n",
      "Training Loss : 1.3277122974395752\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1040001\n",
      "mean reward (100 episodes) 1543.600000\n",
      "best mean reward 1550.400000\n",
      "running time 6574.155863\n",
      "Train_EnvstepsSoFar : 1040001\n",
      "Train_AverageReturn : 1543.6\n",
      "Train_BestReturn : 1550.4\n",
      "TimeSinceStart : 6574.155863285065\n",
      "Training Loss : 0.3782629072666168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1050001\n",
      "mean reward (100 episodes) 1531.500000\n",
      "best mean reward 1550.400000\n",
      "running time 6640.004687\n",
      "Train_EnvstepsSoFar : 1050001\n",
      "Train_AverageReturn : 1531.5\n",
      "Train_BestReturn : 1550.4\n",
      "TimeSinceStart : 6640.004687070847\n",
      "Training Loss : 0.30173003673553467\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1060001\n",
      "mean reward (100 episodes) 1551.500000\n",
      "best mean reward 1551.500000\n",
      "running time 6705.205731\n",
      "Train_EnvstepsSoFar : 1060001\n",
      "Train_AverageReturn : 1551.5\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 6705.205731153488\n",
      "Training Loss : 0.18894678354263306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1070001\n",
      "mean reward (100 episodes) 1520.500000\n",
      "best mean reward 1551.500000\n",
      "running time 6772.666965\n",
      "Train_EnvstepsSoFar : 1070001\n",
      "Train_AverageReturn : 1520.5\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 6772.666965007782\n",
      "Training Loss : 0.4550398588180542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1080001\n",
      "mean reward (100 episodes) 1490.100000\n",
      "best mean reward 1551.500000\n",
      "running time 6837.261756\n",
      "Train_EnvstepsSoFar : 1080001\n",
      "Train_AverageReturn : 1490.1\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 6837.26175570488\n",
      "Training Loss : 0.47910141944885254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1090001\n",
      "mean reward (100 episodes) 1466.700000\n",
      "best mean reward 1551.500000\n",
      "running time 6901.229442\n",
      "Train_EnvstepsSoFar : 1090001\n",
      "Train_AverageReturn : 1466.7\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 6901.22944188118\n",
      "Training Loss : 0.4915071725845337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1100001\n",
      "mean reward (100 episodes) 1536.900000\n",
      "best mean reward 1551.500000\n",
      "running time 6965.343956\n",
      "Train_EnvstepsSoFar : 1100001\n",
      "Train_AverageReturn : 1536.9\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 6965.343955755234\n",
      "Training Loss : 0.6465030908584595\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1110001\n",
      "mean reward (100 episodes) 1520.800000\n",
      "best mean reward 1551.500000\n",
      "running time 7029.981437\n",
      "Train_EnvstepsSoFar : 1110001\n",
      "Train_AverageReturn : 1520.8\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 7029.98143696785\n",
      "Training Loss : 0.45520806312561035\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1120001\n",
      "mean reward (100 episodes) 1511.100000\n",
      "best mean reward 1551.500000\n",
      "running time 7094.343922\n",
      "Train_EnvstepsSoFar : 1120001\n",
      "Train_AverageReturn : 1511.1\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 7094.343922138214\n",
      "Training Loss : 0.7861148118972778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1130001\n",
      "mean reward (100 episodes) 1495.800000\n",
      "best mean reward 1551.500000\n",
      "running time 7158.823669\n",
      "Train_EnvstepsSoFar : 1130001\n",
      "Train_AverageReturn : 1495.8\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 7158.823668956757\n",
      "Training Loss : 0.3296096622943878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1140001\n",
      "mean reward (100 episodes) 1451.200000\n",
      "best mean reward 1551.500000\n",
      "running time 7223.246894\n",
      "Train_EnvstepsSoFar : 1140001\n",
      "Train_AverageReturn : 1451.2\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 7223.246894359589\n",
      "Training Loss : 0.466177761554718\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1150001\n",
      "mean reward (100 episodes) 1434.800000\n",
      "best mean reward 1551.500000\n",
      "running time 7287.148649\n",
      "Train_EnvstepsSoFar : 1150001\n",
      "Train_AverageReturn : 1434.8\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 7287.148649454117\n",
      "Training Loss : 0.8407440781593323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1160001\n",
      "mean reward (100 episodes) 1546.400000\n",
      "best mean reward 1551.500000\n",
      "running time 7351.006174\n",
      "Train_EnvstepsSoFar : 1160001\n",
      "Train_AverageReturn : 1546.4\n",
      "Train_BestReturn : 1551.5\n",
      "TimeSinceStart : 7351.006173849106\n",
      "Training Loss : 0.5947304368019104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1170001\n",
      "mean reward (100 episodes) 1635.500000\n",
      "best mean reward 1635.500000\n",
      "running time 7414.874405\n",
      "Train_EnvstepsSoFar : 1170001\n",
      "Train_AverageReturn : 1635.5\n",
      "Train_BestReturn : 1635.5\n",
      "TimeSinceStart : 7414.874405384064\n",
      "Training Loss : 0.28534996509552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1180001\n",
      "mean reward (100 episodes) 1671.600000\n",
      "best mean reward 1671.600000\n",
      "running time 7479.021363\n",
      "Train_EnvstepsSoFar : 1180001\n",
      "Train_AverageReturn : 1671.6\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7479.02136349678\n",
      "Training Loss : 0.8344037532806396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1190001\n",
      "mean reward (100 episodes) 1609.900000\n",
      "best mean reward 1671.600000\n",
      "running time 7542.863266\n",
      "Train_EnvstepsSoFar : 1190001\n",
      "Train_AverageReturn : 1609.9\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7542.8632662296295\n",
      "Training Loss : 0.4886188507080078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1200001\n",
      "mean reward (100 episodes) 1575.000000\n",
      "best mean reward 1671.600000\n",
      "running time 7606.843993\n",
      "Train_EnvstepsSoFar : 1200001\n",
      "Train_AverageReturn : 1575.0\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7606.843993425369\n",
      "Training Loss : 0.2729156017303467\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1210001\n",
      "mean reward (100 episodes) 1531.800000\n",
      "best mean reward 1671.600000\n",
      "running time 7670.650980\n",
      "Train_EnvstepsSoFar : 1210001\n",
      "Train_AverageReturn : 1531.8\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7670.650980234146\n",
      "Training Loss : 0.23062275350093842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1220001\n",
      "mean reward (100 episodes) 1538.100000\n",
      "best mean reward 1671.600000\n",
      "running time 7734.511444\n",
      "Train_EnvstepsSoFar : 1220001\n",
      "Train_AverageReturn : 1538.1\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7734.511444330215\n",
      "Training Loss : 0.40787816047668457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1230001\n",
      "mean reward (100 episodes) 1527.500000\n",
      "best mean reward 1671.600000\n",
      "running time 7798.709761\n",
      "Train_EnvstepsSoFar : 1230001\n",
      "Train_AverageReturn : 1527.5\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7798.709760904312\n",
      "Training Loss : 0.747515082359314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1240001\n",
      "mean reward (100 episodes) 1527.000000\n",
      "best mean reward 1671.600000\n",
      "running time 7862.542931\n",
      "Train_EnvstepsSoFar : 1240001\n",
      "Train_AverageReturn : 1527.0\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7862.542930603027\n",
      "Training Loss : 0.22180335223674774\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1250001\n",
      "mean reward (100 episodes) 1502.200000\n",
      "best mean reward 1671.600000\n",
      "running time 7926.553003\n",
      "Train_EnvstepsSoFar : 1250001\n",
      "Train_AverageReturn : 1502.2\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7926.5530025959015\n",
      "Training Loss : 0.4312514662742615\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1260001\n",
      "mean reward (100 episodes) 1540.900000\n",
      "best mean reward 1671.600000\n",
      "running time 7990.472145\n",
      "Train_EnvstepsSoFar : 1260001\n",
      "Train_AverageReturn : 1540.9\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 7990.472145318985\n",
      "Training Loss : 0.25648075342178345\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1270001\n",
      "mean reward (100 episodes) 1530.200000\n",
      "best mean reward 1671.600000\n",
      "running time 8054.121996\n",
      "Train_EnvstepsSoFar : 1270001\n",
      "Train_AverageReturn : 1530.2\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8054.121995687485\n",
      "Training Loss : 0.4801211953163147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1280001\n",
      "mean reward (100 episodes) 1609.900000\n",
      "best mean reward 1671.600000\n",
      "running time 8117.908140\n",
      "Train_EnvstepsSoFar : 1280001\n",
      "Train_AverageReturn : 1609.9\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8117.908140182495\n",
      "Training Loss : 0.33369243144989014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1290001\n",
      "mean reward (100 episodes) 1610.300000\n",
      "best mean reward 1671.600000\n",
      "running time 8181.636967\n",
      "Train_EnvstepsSoFar : 1290001\n",
      "Train_AverageReturn : 1610.3\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8181.636967182159\n",
      "Training Loss : 0.7642245888710022\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1300001\n",
      "mean reward (100 episodes) 1656.500000\n",
      "best mean reward 1671.600000\n",
      "running time 8245.547003\n",
      "Train_EnvstepsSoFar : 1300001\n",
      "Train_AverageReturn : 1656.5\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8245.547002792358\n",
      "Training Loss : 0.5383701324462891\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1310001\n",
      "mean reward (100 episodes) 1585.000000\n",
      "best mean reward 1671.600000\n",
      "running time 8308.544691\n",
      "Train_EnvstepsSoFar : 1310001\n",
      "Train_AverageReturn : 1585.0\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8308.544691085815\n",
      "Training Loss : 0.3720758855342865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1320001\n",
      "mean reward (100 episodes) 1554.800000\n",
      "best mean reward 1671.600000\n",
      "running time 8372.002013\n",
      "Train_EnvstepsSoFar : 1320001\n",
      "Train_AverageReturn : 1554.8\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8372.002012968063\n",
      "Training Loss : 0.7287106513977051\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1330001\n",
      "mean reward (100 episodes) 1504.300000\n",
      "best mean reward 1671.600000\n",
      "running time 8436.885608\n",
      "Train_EnvstepsSoFar : 1330001\n",
      "Train_AverageReturn : 1504.3\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8436.885607719421\n",
      "Training Loss : 0.885810136795044\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1340001\n",
      "mean reward (100 episodes) 1492.000000\n",
      "best mean reward 1671.600000\n",
      "running time 8500.244969\n",
      "Train_EnvstepsSoFar : 1340001\n",
      "Train_AverageReturn : 1492.0\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8500.244968891144\n",
      "Training Loss : 0.6955291032791138\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1350001\n",
      "mean reward (100 episodes) 1553.300000\n",
      "best mean reward 1671.600000\n",
      "running time 8563.661528\n",
      "Train_EnvstepsSoFar : 1350001\n",
      "Train_AverageReturn : 1553.3\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8563.661527872086\n",
      "Training Loss : 0.336167573928833\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1360001\n",
      "mean reward (100 episodes) 1563.200000\n",
      "best mean reward 1671.600000\n",
      "running time 8627.036994\n",
      "Train_EnvstepsSoFar : 1360001\n",
      "Train_AverageReturn : 1563.2\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8627.036993980408\n",
      "Training Loss : 0.6320502758026123\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1370001\n",
      "mean reward (100 episodes) 1606.700000\n",
      "best mean reward 1671.600000\n",
      "running time 8690.748338\n",
      "Train_EnvstepsSoFar : 1370001\n",
      "Train_AverageReturn : 1606.7\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8690.748338222504\n",
      "Training Loss : 0.5251213312149048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1380001\n",
      "mean reward (100 episodes) 1540.100000\n",
      "best mean reward 1671.600000\n",
      "running time 8754.507054\n",
      "Train_EnvstepsSoFar : 1380001\n",
      "Train_AverageReturn : 1540.1\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8754.507053852081\n",
      "Training Loss : 0.7890220880508423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1390001\n",
      "mean reward (100 episodes) 1535.000000\n",
      "best mean reward 1671.600000\n",
      "running time 8818.441940\n",
      "Train_EnvstepsSoFar : 1390001\n",
      "Train_AverageReturn : 1535.0\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8818.441939592361\n",
      "Training Loss : 0.5150278210639954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1400001\n",
      "mean reward (100 episodes) 1548.000000\n",
      "best mean reward 1671.600000\n",
      "running time 8882.360971\n",
      "Train_EnvstepsSoFar : 1400001\n",
      "Train_AverageReturn : 1548.0\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8882.36097073555\n",
      "Training Loss : 0.6275566220283508\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1410001\n",
      "mean reward (100 episodes) 1548.400000\n",
      "best mean reward 1671.600000\n",
      "running time 8945.908597\n",
      "Train_EnvstepsSoFar : 1410001\n",
      "Train_AverageReturn : 1548.4\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 8945.908596992493\n",
      "Training Loss : 0.23215579986572266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1420001\n",
      "mean reward (100 episodes) 1603.800000\n",
      "best mean reward 1671.600000\n",
      "running time 9009.159055\n",
      "Train_EnvstepsSoFar : 1420001\n",
      "Train_AverageReturn : 1603.8\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 9009.159055233002\n",
      "Training Loss : 0.7706702947616577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1430001\n",
      "mean reward (100 episodes) 1607.600000\n",
      "best mean reward 1671.600000\n",
      "running time 9072.379232\n",
      "Train_EnvstepsSoFar : 1430001\n",
      "Train_AverageReturn : 1607.6\n",
      "Train_BestReturn : 1671.6\n",
      "TimeSinceStart : 9072.379232168198\n",
      "Training Loss : 0.517997145652771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1440001\n",
      "mean reward (100 episodes) 1679.700000\n",
      "best mean reward 1679.700000\n",
      "running time 9136.096700\n",
      "Train_EnvstepsSoFar : 1440001\n",
      "Train_AverageReturn : 1679.7\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9136.096700429916\n",
      "Training Loss : 0.6457541584968567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1450001\n",
      "mean reward (100 episodes) 1642.800000\n",
      "best mean reward 1679.700000\n",
      "running time 9199.631284\n",
      "Train_EnvstepsSoFar : 1450001\n",
      "Train_AverageReturn : 1642.8\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9199.63128399849\n",
      "Training Loss : 0.6455537676811218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1460001\n",
      "mean reward (100 episodes) 1654.400000\n",
      "best mean reward 1679.700000\n",
      "running time 9263.246347\n",
      "Train_EnvstepsSoFar : 1460001\n",
      "Train_AverageReturn : 1654.4\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9263.24634718895\n",
      "Training Loss : 0.5883764624595642\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1470001\n",
      "mean reward (100 episodes) 1585.600000\n",
      "best mean reward 1679.700000\n",
      "running time 9326.706625\n",
      "Train_EnvstepsSoFar : 1470001\n",
      "Train_AverageReturn : 1585.6\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9326.706624746323\n",
      "Training Loss : 0.33458369970321655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1480001\n",
      "mean reward (100 episodes) 1549.300000\n",
      "best mean reward 1679.700000\n",
      "running time 9390.013018\n",
      "Train_EnvstepsSoFar : 1480001\n",
      "Train_AverageReturn : 1549.3\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9390.013017654419\n",
      "Training Loss : 0.6660740971565247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1490001\n",
      "mean reward (100 episodes) 1509.900000\n",
      "best mean reward 1679.700000\n",
      "running time 9453.543333\n",
      "Train_EnvstepsSoFar : 1490001\n",
      "Train_AverageReturn : 1509.9\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9453.543332576752\n",
      "Training Loss : 0.8055990934371948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1500001\n",
      "mean reward (100 episodes) 1554.700000\n",
      "best mean reward 1679.700000\n",
      "running time 9516.712932\n",
      "Train_EnvstepsSoFar : 1500001\n",
      "Train_AverageReturn : 1554.7\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9516.712932109833\n",
      "Training Loss : 0.20738732814788818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1510001\n",
      "mean reward (100 episodes) 1599.200000\n",
      "best mean reward 1679.700000\n",
      "running time 9579.739684\n",
      "Train_EnvstepsSoFar : 1510001\n",
      "Train_AverageReturn : 1599.2\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9579.739684343338\n",
      "Training Loss : 0.9205074906349182\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1520001\n",
      "mean reward (100 episodes) 1620.600000\n",
      "best mean reward 1679.700000\n",
      "running time 9642.821057\n",
      "Train_EnvstepsSoFar : 1520001\n",
      "Train_AverageReturn : 1620.6\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9642.821057319641\n",
      "Training Loss : 0.6319618225097656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1530001\n",
      "mean reward (100 episodes) 1588.500000\n",
      "best mean reward 1679.700000\n",
      "running time 9706.494299\n",
      "Train_EnvstepsSoFar : 1530001\n",
      "Train_AverageReturn : 1588.5\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9706.494298934937\n",
      "Training Loss : 0.4198523163795471\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1540001\n",
      "mean reward (100 episodes) 1602.600000\n",
      "best mean reward 1679.700000\n",
      "running time 9769.702349\n",
      "Train_EnvstepsSoFar : 1540001\n",
      "Train_AverageReturn : 1602.6\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9769.702348709106\n",
      "Training Loss : 0.9431056976318359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1550001\n",
      "mean reward (100 episodes) 1611.900000\n",
      "best mean reward 1679.700000\n",
      "running time 9833.059103\n",
      "Train_EnvstepsSoFar : 1550001\n",
      "Train_AverageReturn : 1611.9\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9833.059102535248\n",
      "Training Loss : 0.28829285502433777\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1560001\n",
      "mean reward (100 episodes) 1598.300000\n",
      "best mean reward 1679.700000\n",
      "running time 9895.902495\n",
      "Train_EnvstepsSoFar : 1560001\n",
      "Train_AverageReturn : 1598.3\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9895.90249466896\n",
      "Training Loss : 1.5419865846633911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1570001\n",
      "mean reward (100 episodes) 1573.000000\n",
      "best mean reward 1679.700000\n",
      "running time 9959.490710\n",
      "Train_EnvstepsSoFar : 1570001\n",
      "Train_AverageReturn : 1573.0\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 9959.490710020065\n",
      "Training Loss : 0.7351586818695068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1580001\n",
      "mean reward (100 episodes) 1576.700000\n",
      "best mean reward 1679.700000\n",
      "running time 10023.090311\n",
      "Train_EnvstepsSoFar : 1580001\n",
      "Train_AverageReturn : 1576.7\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10023.090310811996\n",
      "Training Loss : 0.40547454357147217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1590001\n",
      "mean reward (100 episodes) 1642.000000\n",
      "best mean reward 1679.700000\n",
      "running time 10086.196739\n",
      "Train_EnvstepsSoFar : 1590001\n",
      "Train_AverageReturn : 1642.0\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10086.196739196777\n",
      "Training Loss : 1.153016209602356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1600001\n",
      "mean reward (100 episodes) 1635.100000\n",
      "best mean reward 1679.700000\n",
      "running time 10148.874396\n",
      "Train_EnvstepsSoFar : 1600001\n",
      "Train_AverageReturn : 1635.1\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10148.87439584732\n",
      "Training Loss : 0.22362691164016724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1610001\n",
      "mean reward (100 episodes) 1622.700000\n",
      "best mean reward 1679.700000\n",
      "running time 10213.347807\n",
      "Train_EnvstepsSoFar : 1610001\n",
      "Train_AverageReturn : 1622.7\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10213.34780716896\n",
      "Training Loss : 1.1214139461517334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1620001\n",
      "mean reward (100 episodes) 1623.300000\n",
      "best mean reward 1679.700000\n",
      "running time 10276.720894\n",
      "Train_EnvstepsSoFar : 1620001\n",
      "Train_AverageReturn : 1623.3\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10276.720894098282\n",
      "Training Loss : 0.24146968126296997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1630001\n",
      "mean reward (100 episodes) 1571.000000\n",
      "best mean reward 1679.700000\n",
      "running time 10339.812087\n",
      "Train_EnvstepsSoFar : 1630001\n",
      "Train_AverageReturn : 1571.0\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10339.812087059021\n",
      "Training Loss : 0.5601244568824768\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1640001\n",
      "mean reward (100 episodes) 1603.900000\n",
      "best mean reward 1679.700000\n",
      "running time 10403.261531\n",
      "Train_EnvstepsSoFar : 1640001\n",
      "Train_AverageReturn : 1603.9\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10403.261531114578\n",
      "Training Loss : 0.33269405364990234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1650001\n",
      "mean reward (100 episodes) 1614.100000\n",
      "best mean reward 1679.700000\n",
      "running time 10466.477136\n",
      "Train_EnvstepsSoFar : 1650001\n",
      "Train_AverageReturn : 1614.1\n",
      "Train_BestReturn : 1679.7\n",
      "TimeSinceStart : 10466.477135658264\n",
      "Training Loss : 0.40714478492736816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1660001\n",
      "mean reward (100 episodes) 1683.900000\n",
      "best mean reward 1683.900000\n",
      "running time 10530.428730\n",
      "Train_EnvstepsSoFar : 1660001\n",
      "Train_AverageReturn : 1683.9\n",
      "Train_BestReturn : 1683.9\n",
      "TimeSinceStart : 10530.428729772568\n",
      "Training Loss : 0.5486307144165039\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1670001\n",
      "mean reward (100 episodes) 1696.800000\n",
      "best mean reward 1696.800000\n",
      "running time 10593.593691\n",
      "Train_EnvstepsSoFar : 1670001\n",
      "Train_AverageReturn : 1696.8\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10593.593690633774\n",
      "Training Loss : 0.31719958782196045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1680001\n",
      "mean reward (100 episodes) 1677.600000\n",
      "best mean reward 1696.800000\n",
      "running time 10657.225126\n",
      "Train_EnvstepsSoFar : 1680001\n",
      "Train_AverageReturn : 1677.6\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10657.225125551224\n",
      "Training Loss : 0.33385229110717773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1690001\n",
      "mean reward (100 episodes) 1621.400000\n",
      "best mean reward 1696.800000\n",
      "running time 10720.169076\n",
      "Train_EnvstepsSoFar : 1690001\n",
      "Train_AverageReturn : 1621.4\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10720.169075727463\n",
      "Training Loss : 1.250230312347412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1700001\n",
      "mean reward (100 episodes) 1609.300000\n",
      "best mean reward 1696.800000\n",
      "running time 10782.994112\n",
      "Train_EnvstepsSoFar : 1700001\n",
      "Train_AverageReturn : 1609.3\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10782.99411201477\n",
      "Training Loss : 0.4220876693725586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1710001\n",
      "mean reward (100 episodes) 1615.800000\n",
      "best mean reward 1696.800000\n",
      "running time 10845.902487\n",
      "Train_EnvstepsSoFar : 1710001\n",
      "Train_AverageReturn : 1615.8\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10845.902486801147\n",
      "Training Loss : 0.7818663716316223\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1720001\n",
      "mean reward (100 episodes) 1636.600000\n",
      "best mean reward 1696.800000\n",
      "running time 10909.034929\n",
      "Train_EnvstepsSoFar : 1720001\n",
      "Train_AverageReturn : 1636.6\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10909.034928560257\n",
      "Training Loss : 0.5590500831604004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1730001\n",
      "mean reward (100 episodes) 1556.400000\n",
      "best mean reward 1696.800000\n",
      "running time 10971.999636\n",
      "Train_EnvstepsSoFar : 1730001\n",
      "Train_AverageReturn : 1556.4\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 10971.999636173248\n",
      "Training Loss : 0.705204427242279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1740001\n",
      "mean reward (100 episodes) 1574.300000\n",
      "best mean reward 1696.800000\n",
      "running time 11035.075691\n",
      "Train_EnvstepsSoFar : 1740001\n",
      "Train_AverageReturn : 1574.3\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 11035.075690746307\n",
      "Training Loss : 0.41414669156074524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1750001\n",
      "mean reward (100 episodes) 1588.300000\n",
      "best mean reward 1696.800000\n",
      "running time 11098.140833\n",
      "Train_EnvstepsSoFar : 1750001\n",
      "Train_AverageReturn : 1588.3\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 11098.140833377838\n",
      "Training Loss : 1.168910264968872\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1760001\n",
      "mean reward (100 episodes) 1663.100000\n",
      "best mean reward 1696.800000\n",
      "running time 11161.406553\n",
      "Train_EnvstepsSoFar : 1760001\n",
      "Train_AverageReturn : 1663.1\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 11161.406552553177\n",
      "Training Loss : 0.5361224412918091\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1770001\n",
      "mean reward (100 episodes) 1690.300000\n",
      "best mean reward 1696.800000\n",
      "running time 11225.017282\n",
      "Train_EnvstepsSoFar : 1770001\n",
      "Train_AverageReturn : 1690.3\n",
      "Train_BestReturn : 1696.8\n",
      "TimeSinceStart : 11225.017281532288\n",
      "Training Loss : 0.7616640329360962\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1780001\n",
      "mean reward (100 episodes) 1719.300000\n",
      "best mean reward 1719.300000\n",
      "running time 11288.095726\n",
      "Train_EnvstepsSoFar : 1780001\n",
      "Train_AverageReturn : 1719.3\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11288.095726251602\n",
      "Training Loss : 0.22568011283874512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1790001\n",
      "mean reward (100 episodes) 1661.600000\n",
      "best mean reward 1719.300000\n",
      "running time 11351.425066\n",
      "Train_EnvstepsSoFar : 1790001\n",
      "Train_AverageReturn : 1661.6\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11351.425065994263\n",
      "Training Loss : 0.6549083590507507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1800001\n",
      "mean reward (100 episodes) 1662.200000\n",
      "best mean reward 1719.300000\n",
      "running time 11414.798418\n",
      "Train_EnvstepsSoFar : 1800001\n",
      "Train_AverageReturn : 1662.2\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11414.798418045044\n",
      "Training Loss : 0.40626633167266846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1810001\n",
      "mean reward (100 episodes) 1664.600000\n",
      "best mean reward 1719.300000\n",
      "running time 11477.782640\n",
      "Train_EnvstepsSoFar : 1810001\n",
      "Train_AverageReturn : 1664.6\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11477.782640457153\n",
      "Training Loss : 1.1076552867889404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1820001\n",
      "mean reward (100 episodes) 1710.700000\n",
      "best mean reward 1719.300000\n",
      "running time 11540.673507\n",
      "Train_EnvstepsSoFar : 1820001\n",
      "Train_AverageReturn : 1710.7\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11540.673506975174\n",
      "Training Loss : 0.19242163002490997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1830001\n",
      "mean reward (100 episodes) 1658.100000\n",
      "best mean reward 1719.300000\n",
      "running time 11603.550057\n",
      "Train_EnvstepsSoFar : 1830001\n",
      "Train_AverageReturn : 1658.1\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11603.550057172775\n",
      "Training Loss : 1.0777788162231445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1840001\n",
      "mean reward (100 episodes) 1585.100000\n",
      "best mean reward 1719.300000\n",
      "running time 11666.509692\n",
      "Train_EnvstepsSoFar : 1840001\n",
      "Train_AverageReturn : 1585.1\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11666.50969171524\n",
      "Training Loss : 0.2075434923171997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1850001\n",
      "mean reward (100 episodes) 1627.900000\n",
      "best mean reward 1719.300000\n",
      "running time 11730.046964\n",
      "Train_EnvstepsSoFar : 1850001\n",
      "Train_AverageReturn : 1627.9\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11730.046964168549\n",
      "Training Loss : 0.5305664539337158\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1860001\n",
      "mean reward (100 episodes) 1660.600000\n",
      "best mean reward 1719.300000\n",
      "running time 11793.587098\n",
      "Train_EnvstepsSoFar : 1860001\n",
      "Train_AverageReturn : 1660.6\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11793.587097883224\n",
      "Training Loss : 0.37194693088531494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1870001\n",
      "mean reward (100 episodes) 1661.700000\n",
      "best mean reward 1719.300000\n",
      "running time 11856.682260\n",
      "Train_EnvstepsSoFar : 1870001\n",
      "Train_AverageReturn : 1661.7\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11856.68225979805\n",
      "Training Loss : 0.19880801439285278\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1880001\n",
      "mean reward (100 episodes) 1603.100000\n",
      "best mean reward 1719.300000\n",
      "running time 11920.041109\n",
      "Train_EnvstepsSoFar : 1880001\n",
      "Train_AverageReturn : 1603.1\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11920.041108608246\n",
      "Training Loss : 0.88396155834198\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1890001\n",
      "mean reward (100 episodes) 1645.000000\n",
      "best mean reward 1719.300000\n",
      "running time 11983.266022\n",
      "Train_EnvstepsSoFar : 1890001\n",
      "Train_AverageReturn : 1645.0\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 11983.266021728516\n",
      "Training Loss : 0.27344170212745667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1900001\n",
      "mean reward (100 episodes) 1658.000000\n",
      "best mean reward 1719.300000\n",
      "running time 12047.799920\n",
      "Train_EnvstepsSoFar : 1900001\n",
      "Train_AverageReturn : 1658.0\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12047.799920082092\n",
      "Training Loss : 0.44746965169906616\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1910001\n",
      "mean reward (100 episodes) 1685.700000\n",
      "best mean reward 1719.300000\n",
      "running time 12111.058156\n",
      "Train_EnvstepsSoFar : 1910001\n",
      "Train_AverageReturn : 1685.7\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12111.058155536652\n",
      "Training Loss : 0.1963241696357727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1920001\n",
      "mean reward (100 episodes) 1677.900000\n",
      "best mean reward 1719.300000\n",
      "running time 12174.093101\n",
      "Train_EnvstepsSoFar : 1920001\n",
      "Train_AverageReturn : 1677.9\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12174.093101024628\n",
      "Training Loss : 0.3307351768016815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1930001\n",
      "mean reward (100 episodes) 1644.300000\n",
      "best mean reward 1719.300000\n",
      "running time 12237.303356\n",
      "Train_EnvstepsSoFar : 1930001\n",
      "Train_AverageReturn : 1644.3\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12237.303356409073\n",
      "Training Loss : 1.3852531909942627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1940001\n",
      "mean reward (100 episodes) 1629.100000\n",
      "best mean reward 1719.300000\n",
      "running time 12300.650845\n",
      "Train_EnvstepsSoFar : 1940001\n",
      "Train_AverageReturn : 1629.1\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12300.650844812393\n",
      "Training Loss : 0.6219481229782104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1950001\n",
      "mean reward (100 episodes) 1607.600000\n",
      "best mean reward 1719.300000\n",
      "running time 12364.329230\n",
      "Train_EnvstepsSoFar : 1950001\n",
      "Train_AverageReturn : 1607.6\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12364.329229831696\n",
      "Training Loss : 0.8682212829589844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1960001\n",
      "mean reward (100 episodes) 1596.900000\n",
      "best mean reward 1719.300000\n",
      "running time 12427.685579\n",
      "Train_EnvstepsSoFar : 1960001\n",
      "Train_AverageReturn : 1596.9\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12427.685579061508\n",
      "Training Loss : 0.2512536346912384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1970001\n",
      "mean reward (100 episodes) 1645.100000\n",
      "best mean reward 1719.300000\n",
      "running time 12490.815520\n",
      "Train_EnvstepsSoFar : 1970001\n",
      "Train_AverageReturn : 1645.1\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12490.815520048141\n",
      "Training Loss : 0.20358233153820038\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1980001\n",
      "mean reward (100 episodes) 1616.000000\n",
      "best mean reward 1719.300000\n",
      "running time 12554.229384\n",
      "Train_EnvstepsSoFar : 1980001\n",
      "Train_AverageReturn : 1616.0\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12554.229384422302\n",
      "Training Loss : 0.3409258723258972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1990001\n",
      "mean reward (100 episodes) 1639.500000\n",
      "best mean reward 1719.300000\n",
      "running time 12617.587261\n",
      "Train_EnvstepsSoFar : 1990001\n",
      "Train_AverageReturn : 1639.5\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12617.587260723114\n",
      "Training Loss : 0.2296046018600464\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 1999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2000001\n",
      "mean reward (100 episodes) 1628.600000\n",
      "best mean reward 1719.300000\n",
      "running time 12681.180634\n",
      "Train_EnvstepsSoFar : 2000001\n",
      "Train_AverageReturn : 1628.6\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12681.180633544922\n",
      "Training Loss : 0.19503608345985413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2010001\n",
      "mean reward (100 episodes) 1675.800000\n",
      "best mean reward 1719.300000\n",
      "running time 12744.631019\n",
      "Train_EnvstepsSoFar : 2010001\n",
      "Train_AverageReturn : 1675.8\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12744.63101863861\n",
      "Training Loss : 0.22027096152305603\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2020001\n",
      "mean reward (100 episodes) 1633.200000\n",
      "best mean reward 1719.300000\n",
      "running time 12807.750231\n",
      "Train_EnvstepsSoFar : 2020001\n",
      "Train_AverageReturn : 1633.2\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12807.750231027603\n",
      "Training Loss : 0.7031251788139343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2030001\n",
      "mean reward (100 episodes) 1622.500000\n",
      "best mean reward 1719.300000\n",
      "running time 12870.750924\n",
      "Train_EnvstepsSoFar : 2030001\n",
      "Train_AverageReturn : 1622.5\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12870.750924110413\n",
      "Training Loss : 0.2217794954776764\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2040001\n",
      "mean reward (100 episodes) 1565.900000\n",
      "best mean reward 1719.300000\n",
      "running time 12934.035608\n",
      "Train_EnvstepsSoFar : 2040001\n",
      "Train_AverageReturn : 1565.9\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12934.035608291626\n",
      "Training Loss : 0.2808036804199219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2050001\n",
      "mean reward (100 episodes) 1628.800000\n",
      "best mean reward 1719.300000\n",
      "running time 12997.420373\n",
      "Train_EnvstepsSoFar : 2050001\n",
      "Train_AverageReturn : 1628.8\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 12997.420372962952\n",
      "Training Loss : 0.8827310800552368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2060001\n",
      "mean reward (100 episodes) 1594.900000\n",
      "best mean reward 1719.300000\n",
      "running time 13060.894988\n",
      "Train_EnvstepsSoFar : 2060001\n",
      "Train_AverageReturn : 1594.9\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 13060.89498758316\n",
      "Training Loss : 0.4548910856246948\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2070001\n",
      "mean reward (100 episodes) 1687.300000\n",
      "best mean reward 1719.300000\n",
      "running time 13123.711330\n",
      "Train_EnvstepsSoFar : 2070001\n",
      "Train_AverageReturn : 1687.3\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 13123.7113301754\n",
      "Training Loss : 0.25815778970718384\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2080001\n",
      "mean reward (100 episodes) 1631.700000\n",
      "best mean reward 1719.300000\n",
      "running time 13186.606320\n",
      "Train_EnvstepsSoFar : 2080001\n",
      "Train_AverageReturn : 1631.7\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 13186.606320381165\n",
      "Training Loss : 0.8516567349433899\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2090001\n",
      "mean reward (100 episodes) 1686.700000\n",
      "best mean reward 1719.300000\n",
      "running time 13249.522118\n",
      "Train_EnvstepsSoFar : 2090001\n",
      "Train_AverageReturn : 1686.7\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 13249.522117853165\n",
      "Training Loss : 0.27299070358276367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2100001\n",
      "mean reward (100 episodes) 1674.400000\n",
      "best mean reward 1719.300000\n",
      "running time 13314.259847\n",
      "Train_EnvstepsSoFar : 2100001\n",
      "Train_AverageReturn : 1674.4\n",
      "Train_BestReturn : 1719.3\n",
      "TimeSinceStart : 13314.259846687317\n",
      "Training Loss : 0.17336060106754303\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2110001\n",
      "mean reward (100 episodes) 1719.900000\n",
      "best mean reward 1719.900000\n",
      "running time 13376.658398\n",
      "Train_EnvstepsSoFar : 2110001\n",
      "Train_AverageReturn : 1719.9\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13376.65839791298\n",
      "Training Loss : 0.2242608368396759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2120001\n",
      "mean reward (100 episodes) 1654.400000\n",
      "best mean reward 1719.900000\n",
      "running time 13439.187303\n",
      "Train_EnvstepsSoFar : 2120001\n",
      "Train_AverageReturn : 1654.4\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13439.187303066254\n",
      "Training Loss : 0.21325725317001343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2130001\n",
      "mean reward (100 episodes) 1629.800000\n",
      "best mean reward 1719.900000\n",
      "running time 13501.731665\n",
      "Train_EnvstepsSoFar : 2130001\n",
      "Train_AverageReturn : 1629.8\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13501.731665372849\n",
      "Training Loss : 0.3758140206336975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2140001\n",
      "mean reward (100 episodes) 1616.500000\n",
      "best mean reward 1719.900000\n",
      "running time 13564.141823\n",
      "Train_EnvstepsSoFar : 2140001\n",
      "Train_AverageReturn : 1616.5\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13564.141823291779\n",
      "Training Loss : 0.40146470069885254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2150001\n",
      "mean reward (100 episodes) 1659.900000\n",
      "best mean reward 1719.900000\n",
      "running time 13626.914442\n",
      "Train_EnvstepsSoFar : 2150001\n",
      "Train_AverageReturn : 1659.9\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13626.914442300797\n",
      "Training Loss : 0.20075193047523499\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2160001\n",
      "mean reward (100 episodes) 1699.100000\n",
      "best mean reward 1719.900000\n",
      "running time 13689.648853\n",
      "Train_EnvstepsSoFar : 2160001\n",
      "Train_AverageReturn : 1699.1\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13689.648853063583\n",
      "Training Loss : 0.42216259241104126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2170001\n",
      "mean reward (100 episodes) 1670.200000\n",
      "best mean reward 1719.900000\n",
      "running time 13752.264491\n",
      "Train_EnvstepsSoFar : 2170001\n",
      "Train_AverageReturn : 1670.2\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13752.264491081238\n",
      "Training Loss : 0.40108048915863037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2180001\n",
      "mean reward (100 episodes) 1683.400000\n",
      "best mean reward 1719.900000\n",
      "running time 13815.093336\n",
      "Train_EnvstepsSoFar : 2180001\n",
      "Train_AverageReturn : 1683.4\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13815.093336105347\n",
      "Training Loss : 1.2411233186721802\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2190001\n",
      "mean reward (100 episodes) 1702.200000\n",
      "best mean reward 1719.900000\n",
      "running time 13879.018156\n",
      "Train_EnvstepsSoFar : 2190001\n",
      "Train_AverageReturn : 1702.2\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13879.018155813217\n",
      "Training Loss : 0.7728489637374878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2200001\n",
      "mean reward (100 episodes) 1631.300000\n",
      "best mean reward 1719.900000\n",
      "running time 13941.597612\n",
      "Train_EnvstepsSoFar : 2200001\n",
      "Train_AverageReturn : 1631.3\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 13941.597611904144\n",
      "Training Loss : 0.3321264684200287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2210001\n",
      "mean reward (100 episodes) 1637.400000\n",
      "best mean reward 1719.900000\n",
      "running time 14004.234007\n",
      "Train_EnvstepsSoFar : 2210001\n",
      "Train_AverageReturn : 1637.4\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14004.234007120132\n",
      "Training Loss : 0.6392298340797424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2220001\n",
      "mean reward (100 episodes) 1599.400000\n",
      "best mean reward 1719.900000\n",
      "running time 14066.845168\n",
      "Train_EnvstepsSoFar : 2220001\n",
      "Train_AverageReturn : 1599.4\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14066.84516787529\n",
      "Training Loss : 0.5315085053443909\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2230001\n",
      "mean reward (100 episodes) 1672.500000\n",
      "best mean reward 1719.900000\n",
      "running time 14129.899193\n",
      "Train_EnvstepsSoFar : 2230001\n",
      "Train_AverageReturn : 1672.5\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14129.89919257164\n",
      "Training Loss : 0.4591372609138489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2240001\n",
      "mean reward (100 episodes) 1638.900000\n",
      "best mean reward 1719.900000\n",
      "running time 14192.615766\n",
      "Train_EnvstepsSoFar : 2240001\n",
      "Train_AverageReturn : 1638.9\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14192.61576628685\n",
      "Training Loss : 0.1269201636314392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2250001\n",
      "mean reward (100 episodes) 1601.500000\n",
      "best mean reward 1719.900000\n",
      "running time 14255.252532\n",
      "Train_EnvstepsSoFar : 2250001\n",
      "Train_AverageReturn : 1601.5\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14255.252532243729\n",
      "Training Loss : 0.3146691918373108\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2260001\n",
      "mean reward (100 episodes) 1625.500000\n",
      "best mean reward 1719.900000\n",
      "running time 14318.108469\n",
      "Train_EnvstepsSoFar : 2260001\n",
      "Train_AverageReturn : 1625.5\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14318.108469486237\n",
      "Training Loss : 0.503774881362915\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2270001\n",
      "mean reward (100 episodes) 1670.600000\n",
      "best mean reward 1719.900000\n",
      "running time 14380.698898\n",
      "Train_EnvstepsSoFar : 2270001\n",
      "Train_AverageReturn : 1670.6\n",
      "Train_BestReturn : 1719.9\n",
      "TimeSinceStart : 14380.698898077011\n",
      "Training Loss : 0.33442819118499756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2280001\n",
      "mean reward (100 episodes) 1740.100000\n",
      "best mean reward 1740.100000\n",
      "running time 14443.275580\n",
      "Train_EnvstepsSoFar : 2280001\n",
      "Train_AverageReturn : 1740.1\n",
      "Train_BestReturn : 1740.1\n",
      "TimeSinceStart : 14443.275579690933\n",
      "Training Loss : 0.27508726716041565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2290001\n",
      "mean reward (100 episodes) 1797.000000\n",
      "best mean reward 1797.000000\n",
      "running time 14506.029370\n",
      "Train_EnvstepsSoFar : 2290001\n",
      "Train_AverageReturn : 1797.0\n",
      "Train_BestReturn : 1797.0\n",
      "TimeSinceStart : 14506.029370069504\n",
      "Training Loss : 0.14163200557231903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2300001\n",
      "mean reward (100 episodes) 1805.600000\n",
      "best mean reward 1805.600000\n",
      "running time 14568.198234\n",
      "Train_EnvstepsSoFar : 2300001\n",
      "Train_AverageReturn : 1805.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14568.198233604431\n",
      "Training Loss : 0.2178678959608078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2310001\n",
      "mean reward (100 episodes) 1733.300000\n",
      "best mean reward 1805.600000\n",
      "running time 14630.505986\n",
      "Train_EnvstepsSoFar : 2310001\n",
      "Train_AverageReturn : 1733.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14630.505985736847\n",
      "Training Loss : 0.3443085849285126\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2320001\n",
      "mean reward (100 episodes) 1716.100000\n",
      "best mean reward 1805.600000\n",
      "running time 14693.458703\n",
      "Train_EnvstepsSoFar : 2320001\n",
      "Train_AverageReturn : 1716.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14693.458703279495\n",
      "Training Loss : 1.1627699136734009\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2330001\n",
      "mean reward (100 episodes) 1749.400000\n",
      "best mean reward 1805.600000\n",
      "running time 14756.188903\n",
      "Train_EnvstepsSoFar : 2330001\n",
      "Train_AverageReturn : 1749.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14756.188903093338\n",
      "Training Loss : 0.4880593419075012\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2340001\n",
      "mean reward (100 episodes) 1726.500000\n",
      "best mean reward 1805.600000\n",
      "running time 14819.377479\n",
      "Train_EnvstepsSoFar : 2340001\n",
      "Train_AverageReturn : 1726.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14819.377479076385\n",
      "Training Loss : 0.36151787638664246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2350001\n",
      "mean reward (100 episodes) 1690.700000\n",
      "best mean reward 1805.600000\n",
      "running time 14882.498640\n",
      "Train_EnvstepsSoFar : 2350001\n",
      "Train_AverageReturn : 1690.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14882.498640298843\n",
      "Training Loss : 0.11679603159427643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2360001\n",
      "mean reward (100 episodes) 1597.300000\n",
      "best mean reward 1805.600000\n",
      "running time 14945.224856\n",
      "Train_EnvstepsSoFar : 2360001\n",
      "Train_AverageReturn : 1597.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 14945.224856376648\n",
      "Training Loss : 0.719048261642456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2370001\n",
      "mean reward (100 episodes) 1577.300000\n",
      "best mean reward 1805.600000\n",
      "running time 15008.812711\n",
      "Train_EnvstepsSoFar : 2370001\n",
      "Train_AverageReturn : 1577.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15008.812711238861\n",
      "Training Loss : 0.34049734473228455\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2380001\n",
      "mean reward (100 episodes) 1508.300000\n",
      "best mean reward 1805.600000\n",
      "running time 15071.253513\n",
      "Train_EnvstepsSoFar : 2380001\n",
      "Train_AverageReturn : 1508.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15071.253512620926\n",
      "Training Loss : 0.19919386506080627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2390001\n",
      "mean reward (100 episodes) 1549.700000\n",
      "best mean reward 1805.600000\n",
      "running time 15134.016644\n",
      "Train_EnvstepsSoFar : 2390001\n",
      "Train_AverageReturn : 1549.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15134.016644477844\n",
      "Training Loss : 0.5799716114997864\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2400001\n",
      "mean reward (100 episodes) 1586.300000\n",
      "best mean reward 1805.600000\n",
      "running time 15197.073071\n",
      "Train_EnvstepsSoFar : 2400001\n",
      "Train_AverageReturn : 1586.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15197.07307100296\n",
      "Training Loss : 0.6360687017440796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2410001\n",
      "mean reward (100 episodes) 1544.800000\n",
      "best mean reward 1805.600000\n",
      "running time 15260.275009\n",
      "Train_EnvstepsSoFar : 2410001\n",
      "Train_AverageReturn : 1544.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15260.275008916855\n",
      "Training Loss : 0.28453242778778076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2420001\n",
      "mean reward (100 episodes) 1497.400000\n",
      "best mean reward 1805.600000\n",
      "running time 15324.012570\n",
      "Train_EnvstepsSoFar : 2420001\n",
      "Train_AverageReturn : 1497.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15324.012569665909\n",
      "Training Loss : 0.4763888120651245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2430001\n",
      "mean reward (100 episodes) 1524.800000\n",
      "best mean reward 1805.600000\n",
      "running time 15386.937677\n",
      "Train_EnvstepsSoFar : 2430001\n",
      "Train_AverageReturn : 1524.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15386.937677383423\n",
      "Training Loss : 1.0896186828613281\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2440001\n",
      "mean reward (100 episodes) 1606.400000\n",
      "best mean reward 1805.600000\n",
      "running time 15450.156514\n",
      "Train_EnvstepsSoFar : 2440001\n",
      "Train_AverageReturn : 1606.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15450.156513690948\n",
      "Training Loss : 0.31973981857299805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2450001\n",
      "mean reward (100 episodes) 1587.500000\n",
      "best mean reward 1805.600000\n",
      "running time 15513.249145\n",
      "Train_EnvstepsSoFar : 2450001\n",
      "Train_AverageReturn : 1587.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15513.249144792557\n",
      "Training Loss : 0.8995893001556396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2460001\n",
      "mean reward (100 episodes) 1612.400000\n",
      "best mean reward 1805.600000\n",
      "running time 15576.788227\n",
      "Train_EnvstepsSoFar : 2460001\n",
      "Train_AverageReturn : 1612.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15576.788227319717\n",
      "Training Loss : 0.176038920879364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2470001\n",
      "mean reward (100 episodes) 1663.000000\n",
      "best mean reward 1805.600000\n",
      "running time 15641.248271\n",
      "Train_EnvstepsSoFar : 2470001\n",
      "Train_AverageReturn : 1663.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15641.248270511627\n",
      "Training Loss : 0.30623960494995117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2480001\n",
      "mean reward (100 episodes) 1622.800000\n",
      "best mean reward 1805.600000\n",
      "running time 15704.319869\n",
      "Train_EnvstepsSoFar : 2480001\n",
      "Train_AverageReturn : 1622.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15704.319868803024\n",
      "Training Loss : 0.5017481446266174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2490001\n",
      "mean reward (100 episodes) 1606.900000\n",
      "best mean reward 1805.600000\n",
      "running time 15767.345992\n",
      "Train_EnvstepsSoFar : 2490001\n",
      "Train_AverageReturn : 1606.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15767.3459918499\n",
      "Training Loss : 0.1238444522023201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2500001\n",
      "mean reward (100 episodes) 1578.600000\n",
      "best mean reward 1805.600000\n",
      "running time 15830.189535\n",
      "Train_EnvstepsSoFar : 2500001\n",
      "Train_AverageReturn : 1578.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15830.189534902573\n",
      "Training Loss : 0.4456387162208557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2510001\n",
      "mean reward (100 episodes) 1594.900000\n",
      "best mean reward 1805.600000\n",
      "running time 15893.490987\n",
      "Train_EnvstepsSoFar : 2510001\n",
      "Train_AverageReturn : 1594.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15893.490986585617\n",
      "Training Loss : 1.192850112915039\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2520001\n",
      "mean reward (100 episodes) 1593.700000\n",
      "best mean reward 1805.600000\n",
      "running time 15956.619606\n",
      "Train_EnvstepsSoFar : 2520001\n",
      "Train_AverageReturn : 1593.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 15956.619606018066\n",
      "Training Loss : 0.15407702326774597\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2530001\n",
      "mean reward (100 episodes) 1619.700000\n",
      "best mean reward 1805.600000\n",
      "running time 16020.344608\n",
      "Train_EnvstepsSoFar : 2530001\n",
      "Train_AverageReturn : 1619.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16020.344608306885\n",
      "Training Loss : 0.20428313314914703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2540001\n",
      "mean reward (100 episodes) 1642.100000\n",
      "best mean reward 1805.600000\n",
      "running time 16083.415467\n",
      "Train_EnvstepsSoFar : 2540001\n",
      "Train_AverageReturn : 1642.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16083.41546678543\n",
      "Training Loss : 0.30342334508895874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2550001\n",
      "mean reward (100 episodes) 1637.900000\n",
      "best mean reward 1805.600000\n",
      "running time 16146.323892\n",
      "Train_EnvstepsSoFar : 2550001\n",
      "Train_AverageReturn : 1637.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16146.323891878128\n",
      "Training Loss : 0.47008955478668213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2560001\n",
      "mean reward (100 episodes) 1634.700000\n",
      "best mean reward 1805.600000\n",
      "running time 16209.466373\n",
      "Train_EnvstepsSoFar : 2560001\n",
      "Train_AverageReturn : 1634.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16209.466372966766\n",
      "Training Loss : 0.6319546699523926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2570001\n",
      "mean reward (100 episodes) 1632.400000\n",
      "best mean reward 1805.600000\n",
      "running time 16272.320744\n",
      "Train_EnvstepsSoFar : 2570001\n",
      "Train_AverageReturn : 1632.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16272.320744276047\n",
      "Training Loss : 0.2019701898097992\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2580001\n",
      "mean reward (100 episodes) 1615.800000\n",
      "best mean reward 1805.600000\n",
      "running time 16335.331094\n",
      "Train_EnvstepsSoFar : 2580001\n",
      "Train_AverageReturn : 1615.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16335.331094026566\n",
      "Training Loss : 1.2709636688232422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2590001\n",
      "mean reward (100 episodes) 1627.100000\n",
      "best mean reward 1805.600000\n",
      "running time 16398.735580\n",
      "Train_EnvstepsSoFar : 2590001\n",
      "Train_AverageReturn : 1627.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16398.7355799675\n",
      "Training Loss : 0.543563723564148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2600001\n",
      "mean reward (100 episodes) 1694.400000\n",
      "best mean reward 1805.600000\n",
      "running time 16461.689691\n",
      "Train_EnvstepsSoFar : 2600001\n",
      "Train_AverageReturn : 1694.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16461.689690589905\n",
      "Training Loss : 0.29862919449806213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2610001\n",
      "mean reward (100 episodes) 1692.500000\n",
      "best mean reward 1805.600000\n",
      "running time 16525.045699\n",
      "Train_EnvstepsSoFar : 2610001\n",
      "Train_AverageReturn : 1692.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16525.045699357986\n",
      "Training Loss : 0.20795077085494995\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2620001\n",
      "mean reward (100 episodes) 1566.500000\n",
      "best mean reward 1805.600000\n",
      "running time 16588.069607\n",
      "Train_EnvstepsSoFar : 2620001\n",
      "Train_AverageReturn : 1566.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16588.069606781006\n",
      "Training Loss : 0.608676552772522\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2630001\n",
      "mean reward (100 episodes) 1518.600000\n",
      "best mean reward 1805.600000\n",
      "running time 16651.039577\n",
      "Train_EnvstepsSoFar : 2630001\n",
      "Train_AverageReturn : 1518.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16651.039577007294\n",
      "Training Loss : 0.5079864263534546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2640001\n",
      "mean reward (100 episodes) 1487.200000\n",
      "best mean reward 1805.600000\n",
      "running time 16714.059237\n",
      "Train_EnvstepsSoFar : 2640001\n",
      "Train_AverageReturn : 1487.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16714.059237003326\n",
      "Training Loss : 1.0016385316848755\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2650001\n",
      "mean reward (100 episodes) 1489.200000\n",
      "best mean reward 1805.600000\n",
      "running time 16777.101550\n",
      "Train_EnvstepsSoFar : 2650001\n",
      "Train_AverageReturn : 1489.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16777.101550340652\n",
      "Training Loss : 0.7858936190605164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2660001\n",
      "mean reward (100 episodes) 1551.300000\n",
      "best mean reward 1805.600000\n",
      "running time 16840.513910\n",
      "Train_EnvstepsSoFar : 2660001\n",
      "Train_AverageReturn : 1551.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16840.513909578323\n",
      "Training Loss : 0.5419551730155945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2670001\n",
      "mean reward (100 episodes) 1578.300000\n",
      "best mean reward 1805.600000\n",
      "running time 16903.757298\n",
      "Train_EnvstepsSoFar : 2670001\n",
      "Train_AverageReturn : 1578.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16903.757298231125\n",
      "Training Loss : 0.64192795753479\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2680001\n",
      "mean reward (100 episodes) 1590.100000\n",
      "best mean reward 1805.600000\n",
      "running time 16966.588542\n",
      "Train_EnvstepsSoFar : 2680001\n",
      "Train_AverageReturn : 1590.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 16966.588542222977\n",
      "Training Loss : 0.2647184729576111\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2690001\n",
      "mean reward (100 episodes) 1621.200000\n",
      "best mean reward 1805.600000\n",
      "running time 17029.711665\n",
      "Train_EnvstepsSoFar : 2690001\n",
      "Train_AverageReturn : 1621.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17029.711665391922\n",
      "Training Loss : 0.1944025307893753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2700001\n",
      "mean reward (100 episodes) 1636.500000\n",
      "best mean reward 1805.600000\n",
      "running time 17092.793297\n",
      "Train_EnvstepsSoFar : 2700001\n",
      "Train_AverageReturn : 1636.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17092.793297052383\n",
      "Training Loss : 0.18162193894386292\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2710001\n",
      "mean reward (100 episodes) 1696.900000\n",
      "best mean reward 1805.600000\n",
      "running time 17156.173896\n",
      "Train_EnvstepsSoFar : 2710001\n",
      "Train_AverageReturn : 1696.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17156.173895597458\n",
      "Training Loss : 0.15335972607135773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2720001\n",
      "mean reward (100 episodes) 1683.500000\n",
      "best mean reward 1805.600000\n",
      "running time 17219.423517\n",
      "Train_EnvstepsSoFar : 2720001\n",
      "Train_AverageReturn : 1683.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17219.423516988754\n",
      "Training Loss : 0.2344394028186798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2730001\n",
      "mean reward (100 episodes) 1641.000000\n",
      "best mean reward 1805.600000\n",
      "running time 17282.616201\n",
      "Train_EnvstepsSoFar : 2730001\n",
      "Train_AverageReturn : 1641.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17282.616201400757\n",
      "Training Loss : 1.0214368104934692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2740001\n",
      "mean reward (100 episodes) 1602.600000\n",
      "best mean reward 1805.600000\n",
      "running time 17346.347355\n",
      "Train_EnvstepsSoFar : 2740001\n",
      "Train_AverageReturn : 1602.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17346.347354888916\n",
      "Training Loss : 0.20803241431713104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2750001\n",
      "mean reward (100 episodes) 1605.000000\n",
      "best mean reward 1805.600000\n",
      "running time 17411.033211\n",
      "Train_EnvstepsSoFar : 2750001\n",
      "Train_AverageReturn : 1605.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17411.033210754395\n",
      "Training Loss : 0.37917453050613403\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2760001\n",
      "mean reward (100 episodes) 1673.900000\n",
      "best mean reward 1805.600000\n",
      "running time 17474.604899\n",
      "Train_EnvstepsSoFar : 2760001\n",
      "Train_AverageReturn : 1673.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17474.604899168015\n",
      "Training Loss : 0.2895771861076355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2770001\n",
      "mean reward (100 episodes) 1669.600000\n",
      "best mean reward 1805.600000\n",
      "running time 17538.240847\n",
      "Train_EnvstepsSoFar : 2770001\n",
      "Train_AverageReturn : 1669.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17538.24084711075\n",
      "Training Loss : 0.37259557843208313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2780001\n",
      "mean reward (100 episodes) 1696.700000\n",
      "best mean reward 1805.600000\n",
      "running time 17601.778132\n",
      "Train_EnvstepsSoFar : 2780001\n",
      "Train_AverageReturn : 1696.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17601.778131723404\n",
      "Training Loss : 1.2002923488616943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2790001\n",
      "mean reward (100 episodes) 1718.800000\n",
      "best mean reward 1805.600000\n",
      "running time 17665.339655\n",
      "Train_EnvstepsSoFar : 2790001\n",
      "Train_AverageReturn : 1718.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17665.339655399323\n",
      "Training Loss : 0.6959693431854248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2800001\n",
      "mean reward (100 episodes) 1698.200000\n",
      "best mean reward 1805.600000\n",
      "running time 17729.335784\n",
      "Train_EnvstepsSoFar : 2800001\n",
      "Train_AverageReturn : 1698.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17729.335783720016\n",
      "Training Loss : 0.8010337352752686\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2810001\n",
      "mean reward (100 episodes) 1669.700000\n",
      "best mean reward 1805.600000\n",
      "running time 17792.884683\n",
      "Train_EnvstepsSoFar : 2810001\n",
      "Train_AverageReturn : 1669.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17792.884682893753\n",
      "Training Loss : 0.4442918300628662\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2820001\n",
      "mean reward (100 episodes) 1572.500000\n",
      "best mean reward 1805.600000\n",
      "running time 17856.376859\n",
      "Train_EnvstepsSoFar : 2820001\n",
      "Train_AverageReturn : 1572.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17856.37685894966\n",
      "Training Loss : 0.5212482810020447\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2830001\n",
      "mean reward (100 episodes) 1631.000000\n",
      "best mean reward 1805.600000\n",
      "running time 17920.113215\n",
      "Train_EnvstepsSoFar : 2830001\n",
      "Train_AverageReturn : 1631.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17920.113214731216\n",
      "Training Loss : 0.22674213349819183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2840001\n",
      "mean reward (100 episodes) 1687.700000\n",
      "best mean reward 1805.600000\n",
      "running time 17983.548101\n",
      "Train_EnvstepsSoFar : 2840001\n",
      "Train_AverageReturn : 1687.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 17983.548100948334\n",
      "Training Loss : 0.2053290605545044\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2850001\n",
      "mean reward (100 episodes) 1665.200000\n",
      "best mean reward 1805.600000\n",
      "running time 18047.026396\n",
      "Train_EnvstepsSoFar : 2850001\n",
      "Train_AverageReturn : 1665.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18047.02639579773\n",
      "Training Loss : 0.4155580401420593\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2860001\n",
      "mean reward (100 episodes) 1649.800000\n",
      "best mean reward 1805.600000\n",
      "running time 18111.029366\n",
      "Train_EnvstepsSoFar : 2860001\n",
      "Train_AverageReturn : 1649.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18111.029366254807\n",
      "Training Loss : 0.3972416818141937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2870001\n",
      "mean reward (100 episodes) 1676.800000\n",
      "best mean reward 1805.600000\n",
      "running time 18175.265062\n",
      "Train_EnvstepsSoFar : 2870001\n",
      "Train_AverageReturn : 1676.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18175.265061616898\n",
      "Training Loss : 0.17181220650672913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2880001\n",
      "mean reward (100 episodes) 1692.900000\n",
      "best mean reward 1805.600000\n",
      "running time 18239.154843\n",
      "Train_EnvstepsSoFar : 2880001\n",
      "Train_AverageReturn : 1692.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18239.154843330383\n",
      "Training Loss : 0.30738139152526855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2890001\n",
      "mean reward (100 episodes) 1644.700000\n",
      "best mean reward 1805.600000\n",
      "running time 18303.063593\n",
      "Train_EnvstepsSoFar : 2890001\n",
      "Train_AverageReturn : 1644.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18303.063592672348\n",
      "Training Loss : 0.39643430709838867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2900001\n",
      "mean reward (100 episodes) 1578.600000\n",
      "best mean reward 1805.600000\n",
      "running time 18366.985570\n",
      "Train_EnvstepsSoFar : 2900001\n",
      "Train_AverageReturn : 1578.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18366.98556995392\n",
      "Training Loss : 0.17763760685920715\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2910001\n",
      "mean reward (100 episodes) 1670.200000\n",
      "best mean reward 1805.600000\n",
      "running time 18430.950606\n",
      "Train_EnvstepsSoFar : 2910001\n",
      "Train_AverageReturn : 1670.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18430.950605630875\n",
      "Training Loss : 1.4099980592727661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2920001\n",
      "mean reward (100 episodes) 1681.100000\n",
      "best mean reward 1805.600000\n",
      "running time 18494.583217\n",
      "Train_EnvstepsSoFar : 2920001\n",
      "Train_AverageReturn : 1681.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18494.583217144012\n",
      "Training Loss : 1.5623666048049927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2930001\n",
      "mean reward (100 episodes) 1671.700000\n",
      "best mean reward 1805.600000\n",
      "running time 18558.393307\n",
      "Train_EnvstepsSoFar : 2930001\n",
      "Train_AverageReturn : 1671.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18558.393306970596\n",
      "Training Loss : 0.6000996232032776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2940001\n",
      "mean reward (100 episodes) 1594.000000\n",
      "best mean reward 1805.600000\n",
      "running time 18623.141636\n",
      "Train_EnvstepsSoFar : 2940001\n",
      "Train_AverageReturn : 1594.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18623.141636371613\n",
      "Training Loss : 0.5661933422088623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2950001\n",
      "mean reward (100 episodes) 1679.500000\n",
      "best mean reward 1805.600000\n",
      "running time 18687.374992\n",
      "Train_EnvstepsSoFar : 2950001\n",
      "Train_AverageReturn : 1679.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18687.374992132187\n",
      "Training Loss : 0.22202038764953613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2960001\n",
      "mean reward (100 episodes) 1652.400000\n",
      "best mean reward 1805.600000\n",
      "running time 18750.955842\n",
      "Train_EnvstepsSoFar : 2960001\n",
      "Train_AverageReturn : 1652.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18750.955842018127\n",
      "Training Loss : 0.6261875033378601\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2970001\n",
      "mean reward (100 episodes) 1665.200000\n",
      "best mean reward 1805.600000\n",
      "running time 18814.625268\n",
      "Train_EnvstepsSoFar : 2970001\n",
      "Train_AverageReturn : 1665.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18814.625267505646\n",
      "Training Loss : 0.21091701090335846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2980001\n",
      "mean reward (100 episodes) 1661.300000\n",
      "best mean reward 1805.600000\n",
      "running time 18878.343799\n",
      "Train_EnvstepsSoFar : 2980001\n",
      "Train_AverageReturn : 1661.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18878.343799114227\n",
      "Training Loss : 0.10806373506784439\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 2990001\n",
      "mean reward (100 episodes) 1635.300000\n",
      "best mean reward 1805.600000\n",
      "running time 18941.967499\n",
      "Train_EnvstepsSoFar : 2990001\n",
      "Train_AverageReturn : 1635.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 18941.967499494553\n",
      "Training Loss : 2.4541921615600586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3000001\n",
      "mean reward (100 episodes) 1642.200000\n",
      "best mean reward 1805.600000\n",
      "running time 19005.978505\n",
      "Train_EnvstepsSoFar : 3000001\n",
      "Train_AverageReturn : 1642.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19005.978504657745\n",
      "Training Loss : 0.06205903738737106\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3010001\n",
      "mean reward (100 episodes) 1624.100000\n",
      "best mean reward 1805.600000\n",
      "running time 19069.835661\n",
      "Train_EnvstepsSoFar : 3010001\n",
      "Train_AverageReturn : 1624.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19069.83566069603\n",
      "Training Loss : 0.23849572241306305\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3020001\n",
      "mean reward (100 episodes) 1654.600000\n",
      "best mean reward 1805.600000\n",
      "running time 19134.514575\n",
      "Train_EnvstepsSoFar : 3020001\n",
      "Train_AverageReturn : 1654.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19134.514575242996\n",
      "Training Loss : 0.6380858421325684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3030001\n",
      "mean reward (100 episodes) 1558.100000\n",
      "best mean reward 1805.600000\n",
      "running time 19198.521613\n",
      "Train_EnvstepsSoFar : 3030001\n",
      "Train_AverageReturn : 1558.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19198.52161335945\n",
      "Training Loss : 0.24203763902187347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3040001\n",
      "mean reward (100 episodes) 1546.000000\n",
      "best mean reward 1805.600000\n",
      "running time 19262.661806\n",
      "Train_EnvstepsSoFar : 3040001\n",
      "Train_AverageReturn : 1546.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19262.661806344986\n",
      "Training Loss : 1.4452052116394043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3050001\n",
      "mean reward (100 episodes) 1529.500000\n",
      "best mean reward 1805.600000\n",
      "running time 19326.631133\n",
      "Train_EnvstepsSoFar : 3050001\n",
      "Train_AverageReturn : 1529.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19326.63113284111\n",
      "Training Loss : 0.6438732147216797\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3060001\n",
      "mean reward (100 episodes) 1614.700000\n",
      "best mean reward 1805.600000\n",
      "running time 19390.390805\n",
      "Train_EnvstepsSoFar : 3060001\n",
      "Train_AverageReturn : 1614.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19390.39080452919\n",
      "Training Loss : 0.2304072380065918\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3070001\n",
      "mean reward (100 episodes) 1692.000000\n",
      "best mean reward 1805.600000\n",
      "running time 19454.326482\n",
      "Train_EnvstepsSoFar : 3070001\n",
      "Train_AverageReturn : 1692.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19454.32648205757\n",
      "Training Loss : 0.21144048869609833\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3080001\n",
      "mean reward (100 episodes) 1704.400000\n",
      "best mean reward 1805.600000\n",
      "running time 19518.192648\n",
      "Train_EnvstepsSoFar : 3080001\n",
      "Train_AverageReturn : 1704.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19518.192648410797\n",
      "Training Loss : 0.20694540441036224\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3090001\n",
      "mean reward (100 episodes) 1661.100000\n",
      "best mean reward 1805.600000\n",
      "running time 19582.191123\n",
      "Train_EnvstepsSoFar : 3090001\n",
      "Train_AverageReturn : 1661.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19582.19112253189\n",
      "Training Loss : 0.13799166679382324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3100001\n",
      "mean reward (100 episodes) 1651.100000\n",
      "best mean reward 1805.600000\n",
      "running time 19646.528419\n",
      "Train_EnvstepsSoFar : 3100001\n",
      "Train_AverageReturn : 1651.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19646.528418779373\n",
      "Training Loss : 0.21627560257911682\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3110001\n",
      "mean reward (100 episodes) 1669.200000\n",
      "best mean reward 1805.600000\n",
      "running time 19709.828982\n",
      "Train_EnvstepsSoFar : 3110001\n",
      "Train_AverageReturn : 1669.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19709.828981637955\n",
      "Training Loss : 0.2917327880859375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3120001\n",
      "mean reward (100 episodes) 1677.500000\n",
      "best mean reward 1805.600000\n",
      "running time 19773.093962\n",
      "Train_EnvstepsSoFar : 3120001\n",
      "Train_AverageReturn : 1677.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19773.093962430954\n",
      "Training Loss : 1.124671459197998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3130001\n",
      "mean reward (100 episodes) 1609.500000\n",
      "best mean reward 1805.600000\n",
      "running time 19836.473830\n",
      "Train_EnvstepsSoFar : 3130001\n",
      "Train_AverageReturn : 1609.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19836.473829507828\n",
      "Training Loss : 0.12476562708616257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3140001\n",
      "mean reward (100 episodes) 1591.200000\n",
      "best mean reward 1805.600000\n",
      "running time 19900.332648\n",
      "Train_EnvstepsSoFar : 3140001\n",
      "Train_AverageReturn : 1591.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19900.332648038864\n",
      "Training Loss : 0.6442975997924805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3150001\n",
      "mean reward (100 episodes) 1611.200000\n",
      "best mean reward 1805.600000\n",
      "running time 19963.713269\n",
      "Train_EnvstepsSoFar : 3150001\n",
      "Train_AverageReturn : 1611.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 19963.713268518448\n",
      "Training Loss : 0.5100488066673279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3160001\n",
      "mean reward (100 episodes) 1636.400000\n",
      "best mean reward 1805.600000\n",
      "running time 20027.491791\n",
      "Train_EnvstepsSoFar : 3160001\n",
      "Train_AverageReturn : 1636.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20027.49179124832\n",
      "Training Loss : 0.19251547753810883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3170001\n",
      "mean reward (100 episodes) 1684.900000\n",
      "best mean reward 1805.600000\n",
      "running time 20090.716552\n",
      "Train_EnvstepsSoFar : 3170001\n",
      "Train_AverageReturn : 1684.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20090.7165517807\n",
      "Training Loss : 0.3688037693500519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3180001\n",
      "mean reward (100 episodes) 1700.000000\n",
      "best mean reward 1805.600000\n",
      "running time 20154.330018\n",
      "Train_EnvstepsSoFar : 3180001\n",
      "Train_AverageReturn : 1700.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20154.33001756668\n",
      "Training Loss : 0.39614859223365784\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3190001\n",
      "mean reward (100 episodes) 1630.700000\n",
      "best mean reward 1805.600000\n",
      "running time 20217.637437\n",
      "Train_EnvstepsSoFar : 3190001\n",
      "Train_AverageReturn : 1630.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20217.63743686676\n",
      "Training Loss : 1.407705307006836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3200001\n",
      "mean reward (100 episodes) 1587.800000\n",
      "best mean reward 1805.600000\n",
      "running time 20280.968341\n",
      "Train_EnvstepsSoFar : 3200001\n",
      "Train_AverageReturn : 1587.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20280.96834087372\n",
      "Training Loss : 0.7657337784767151\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3210001\n",
      "mean reward (100 episodes) 1660.200000\n",
      "best mean reward 1805.600000\n",
      "running time 20344.439050\n",
      "Train_EnvstepsSoFar : 3210001\n",
      "Train_AverageReturn : 1660.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20344.439049959183\n",
      "Training Loss : 0.4858050048351288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3220001\n",
      "mean reward (100 episodes) 1649.700000\n",
      "best mean reward 1805.600000\n",
      "running time 20408.182577\n",
      "Train_EnvstepsSoFar : 3220001\n",
      "Train_AverageReturn : 1649.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20408.18257713318\n",
      "Training Loss : 0.26274538040161133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3230001\n",
      "mean reward (100 episodes) 1623.100000\n",
      "best mean reward 1805.600000\n",
      "running time 20472.218302\n",
      "Train_EnvstepsSoFar : 3230001\n",
      "Train_AverageReturn : 1623.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20472.21830201149\n",
      "Training Loss : 0.42729490995407104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3240001\n",
      "mean reward (100 episodes) 1592.000000\n",
      "best mean reward 1805.600000\n",
      "running time 20536.207582\n",
      "Train_EnvstepsSoFar : 3240001\n",
      "Train_AverageReturn : 1592.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20536.2075817585\n",
      "Training Loss : 0.445586234331131\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3250001\n",
      "mean reward (100 episodes) 1625.500000\n",
      "best mean reward 1805.600000\n",
      "running time 20599.444027\n",
      "Train_EnvstepsSoFar : 3250001\n",
      "Train_AverageReturn : 1625.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20599.44402742386\n",
      "Training Loss : 0.5675768852233887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3260001\n",
      "mean reward (100 episodes) 1648.100000\n",
      "best mean reward 1805.600000\n",
      "running time 20663.710169\n",
      "Train_EnvstepsSoFar : 3260001\n",
      "Train_AverageReturn : 1648.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20663.710169315338\n",
      "Training Loss : 0.21945026516914368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3270001\n",
      "mean reward (100 episodes) 1688.000000\n",
      "best mean reward 1805.600000\n",
      "running time 20727.294198\n",
      "Train_EnvstepsSoFar : 3270001\n",
      "Train_AverageReturn : 1688.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20727.294198274612\n",
      "Training Loss : 0.20009249448776245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3280001\n",
      "mean reward (100 episodes) 1679.000000\n",
      "best mean reward 1805.600000\n",
      "running time 20790.806289\n",
      "Train_EnvstepsSoFar : 3280001\n",
      "Train_AverageReturn : 1679.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20790.806288957596\n",
      "Training Loss : 0.2202615886926651\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3290001\n",
      "mean reward (100 episodes) 1671.200000\n",
      "best mean reward 1805.600000\n",
      "running time 20854.618479\n",
      "Train_EnvstepsSoFar : 3290001\n",
      "Train_AverageReturn : 1671.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20854.618478536606\n",
      "Training Loss : 0.2117902785539627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3300001\n",
      "mean reward (100 episodes) 1659.900000\n",
      "best mean reward 1805.600000\n",
      "running time 20918.864038\n",
      "Train_EnvstepsSoFar : 3300001\n",
      "Train_AverageReturn : 1659.9\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20918.864037513733\n",
      "Training Loss : 0.4232466220855713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3310001\n",
      "mean reward (100 episodes) 1637.800000\n",
      "best mean reward 1805.600000\n",
      "running time 20982.086645\n",
      "Train_EnvstepsSoFar : 3310001\n",
      "Train_AverageReturn : 1637.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 20982.086644649506\n",
      "Training Loss : 0.2270406037569046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3320001\n",
      "mean reward (100 episodes) 1677.600000\n",
      "best mean reward 1805.600000\n",
      "running time 21046.612722\n",
      "Train_EnvstepsSoFar : 3320001\n",
      "Train_AverageReturn : 1677.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21046.612721681595\n",
      "Training Loss : 0.2927546501159668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3330001\n",
      "mean reward (100 episodes) 1666.400000\n",
      "best mean reward 1805.600000\n",
      "running time 21110.037231\n",
      "Train_EnvstepsSoFar : 3330001\n",
      "Train_AverageReturn : 1666.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21110.037231206894\n",
      "Training Loss : 0.92820143699646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3340001\n",
      "mean reward (100 episodes) 1658.700000\n",
      "best mean reward 1805.600000\n",
      "running time 21173.690922\n",
      "Train_EnvstepsSoFar : 3340001\n",
      "Train_AverageReturn : 1658.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21173.690922498703\n",
      "Training Loss : 1.7999966144561768\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3350001\n",
      "mean reward (100 episodes) 1693.600000\n",
      "best mean reward 1805.600000\n",
      "running time 21236.944201\n",
      "Train_EnvstepsSoFar : 3350001\n",
      "Train_AverageReturn : 1693.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21236.94420146942\n",
      "Training Loss : 0.6289632320404053\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3360001\n",
      "mean reward (100 episodes) 1717.800000\n",
      "best mean reward 1805.600000\n",
      "running time 21300.330523\n",
      "Train_EnvstepsSoFar : 3360001\n",
      "Train_AverageReturn : 1717.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21300.33052301407\n",
      "Training Loss : 0.632803738117218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3370001\n",
      "mean reward (100 episodes) 1732.500000\n",
      "best mean reward 1805.600000\n",
      "running time 21363.838482\n",
      "Train_EnvstepsSoFar : 3370001\n",
      "Train_AverageReturn : 1732.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21363.838481664658\n",
      "Training Loss : 0.1479751020669937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3380001\n",
      "mean reward (100 episodes) 1664.700000\n",
      "best mean reward 1805.600000\n",
      "running time 21427.613794\n",
      "Train_EnvstepsSoFar : 3380001\n",
      "Train_AverageReturn : 1664.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21427.613793611526\n",
      "Training Loss : 0.13504847884178162\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3390001\n",
      "mean reward (100 episodes) 1715.100000\n",
      "best mean reward 1805.600000\n",
      "running time 21491.058311\n",
      "Train_EnvstepsSoFar : 3390001\n",
      "Train_AverageReturn : 1715.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21491.058311223984\n",
      "Training Loss : 0.7900168895721436\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3400001\n",
      "mean reward (100 episodes) 1693.400000\n",
      "best mean reward 1805.600000\n",
      "running time 21554.260041\n",
      "Train_EnvstepsSoFar : 3400001\n",
      "Train_AverageReturn : 1693.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21554.260041236877\n",
      "Training Loss : 0.7745786905288696\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3410001\n",
      "mean reward (100 episodes) 1708.200000\n",
      "best mean reward 1805.600000\n",
      "running time 21617.806128\n",
      "Train_EnvstepsSoFar : 3410001\n",
      "Train_AverageReturn : 1708.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21617.806128025055\n",
      "Training Loss : 0.2603405714035034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3420001\n",
      "mean reward (100 episodes) 1618.100000\n",
      "best mean reward 1805.600000\n",
      "running time 21681.075548\n",
      "Train_EnvstepsSoFar : 3420001\n",
      "Train_AverageReturn : 1618.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21681.07554769516\n",
      "Training Loss : 0.2191501259803772\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3430001\n",
      "mean reward (100 episodes) 1632.700000\n",
      "best mean reward 1805.600000\n",
      "running time 21744.449278\n",
      "Train_EnvstepsSoFar : 3430001\n",
      "Train_AverageReturn : 1632.7\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21744.449278354645\n",
      "Training Loss : 0.21243581175804138\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3440001\n",
      "mean reward (100 episodes) 1633.600000\n",
      "best mean reward 1805.600000\n",
      "running time 21807.752540\n",
      "Train_EnvstepsSoFar : 3440001\n",
      "Train_AverageReturn : 1633.6\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21807.75254011154\n",
      "Training Loss : 0.7394183278083801\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3450001\n",
      "mean reward (100 episodes) 1683.100000\n",
      "best mean reward 1805.600000\n",
      "running time 21871.527183\n",
      "Train_EnvstepsSoFar : 3450001\n",
      "Train_AverageReturn : 1683.1\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21871.52718257904\n",
      "Training Loss : 0.22330805659294128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3460001\n",
      "mean reward (100 episodes) 1677.800000\n",
      "best mean reward 1805.600000\n",
      "running time 21935.573496\n",
      "Train_EnvstepsSoFar : 3460001\n",
      "Train_AverageReturn : 1677.8\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21935.573496341705\n",
      "Training Loss : 0.3815883696079254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3470001\n",
      "mean reward (100 episodes) 1649.000000\n",
      "best mean reward 1805.600000\n",
      "running time 21999.033562\n",
      "Train_EnvstepsSoFar : 3470001\n",
      "Train_AverageReturn : 1649.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 21999.03356194496\n",
      "Training Loss : 0.14192792773246765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3480001\n",
      "mean reward (100 episodes) 1640.500000\n",
      "best mean reward 1805.600000\n",
      "running time 22062.634667\n",
      "Train_EnvstepsSoFar : 3480001\n",
      "Train_AverageReturn : 1640.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22062.63466691971\n",
      "Training Loss : 0.23607325553894043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3490001\n",
      "mean reward (100 episodes) 1658.500000\n",
      "best mean reward 1805.600000\n",
      "running time 22125.981117\n",
      "Train_EnvstepsSoFar : 3490001\n",
      "Train_AverageReturn : 1658.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22125.981116771698\n",
      "Training Loss : 0.4754258990287781\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3500001\n",
      "mean reward (100 episodes) 1745.000000\n",
      "best mean reward 1805.600000\n",
      "running time 22190.262250\n",
      "Train_EnvstepsSoFar : 3500001\n",
      "Train_AverageReturn : 1745.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22190.262250185013\n",
      "Training Loss : 0.9409292936325073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3510001\n",
      "mean reward (100 episodes) 1745.500000\n",
      "best mean reward 1805.600000\n",
      "running time 22254.272717\n",
      "Train_EnvstepsSoFar : 3510001\n",
      "Train_AverageReturn : 1745.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22254.272716760635\n",
      "Training Loss : 0.2890494465827942\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3520001\n",
      "mean reward (100 episodes) 1730.500000\n",
      "best mean reward 1805.600000\n",
      "running time 22317.878481\n",
      "Train_EnvstepsSoFar : 3520001\n",
      "Train_AverageReturn : 1730.5\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22317.878481149673\n",
      "Training Loss : 0.16383987665176392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3530001\n",
      "mean reward (100 episodes) 1720.300000\n",
      "best mean reward 1805.600000\n",
      "running time 22381.532092\n",
      "Train_EnvstepsSoFar : 3530001\n",
      "Train_AverageReturn : 1720.3\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22381.532091856003\n",
      "Training Loss : 0.2253471165895462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3540001\n",
      "mean reward (100 episodes) 1641.400000\n",
      "best mean reward 1805.600000\n",
      "running time 22445.153577\n",
      "Train_EnvstepsSoFar : 3540001\n",
      "Train_AverageReturn : 1641.4\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22445.153576612473\n",
      "Training Loss : 0.5297871232032776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3550001\n",
      "mean reward (100 episodes) 1668.200000\n",
      "best mean reward 1805.600000\n",
      "running time 22508.178993\n",
      "Train_EnvstepsSoFar : 3550001\n",
      "Train_AverageReturn : 1668.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22508.17899298668\n",
      "Training Loss : 0.8296219110488892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3560001\n",
      "mean reward (100 episodes) 1778.000000\n",
      "best mean reward 1805.600000\n",
      "running time 22571.553611\n",
      "Train_EnvstepsSoFar : 3560001\n",
      "Train_AverageReturn : 1778.0\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22571.55361056328\n",
      "Training Loss : 1.5870558023452759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3570001\n",
      "mean reward (100 episodes) 1731.200000\n",
      "best mean reward 1805.600000\n",
      "running time 22634.921869\n",
      "Train_EnvstepsSoFar : 3570001\n",
      "Train_AverageReturn : 1731.2\n",
      "Train_BestReturn : 1805.6\n",
      "TimeSinceStart : 22634.921869277954\n",
      "Training Loss : 0.1126759946346283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3580001\n",
      "mean reward (100 episodes) 1852.200000\n",
      "best mean reward 1852.200000\n",
      "running time 22699.099078\n",
      "Train_EnvstepsSoFar : 3580001\n",
      "Train_AverageReturn : 1852.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 22699.099078178406\n",
      "Training Loss : 0.2681540846824646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3590001\n",
      "mean reward (100 episodes) 1782.300000\n",
      "best mean reward 1852.200000\n",
      "running time 22762.153639\n",
      "Train_EnvstepsSoFar : 3590001\n",
      "Train_AverageReturn : 1782.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 22762.15363931656\n",
      "Training Loss : 1.074074149131775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3600001\n",
      "mean reward (100 episodes) 1777.800000\n",
      "best mean reward 1852.200000\n",
      "running time 22825.343994\n",
      "Train_EnvstepsSoFar : 3600001\n",
      "Train_AverageReturn : 1777.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 22825.343993663788\n",
      "Training Loss : 0.285935640335083\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3610001\n",
      "mean reward (100 episodes) 1648.500000\n",
      "best mean reward 1852.200000\n",
      "running time 22888.404993\n",
      "Train_EnvstepsSoFar : 3610001\n",
      "Train_AverageReturn : 1648.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 22888.40499305725\n",
      "Training Loss : 1.8316757678985596\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3620001\n",
      "mean reward (100 episodes) 1610.600000\n",
      "best mean reward 1852.200000\n",
      "running time 22951.583097\n",
      "Train_EnvstepsSoFar : 3620001\n",
      "Train_AverageReturn : 1610.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 22951.58309698105\n",
      "Training Loss : 1.3349183797836304\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3630001\n",
      "mean reward (100 episodes) 1638.400000\n",
      "best mean reward 1852.200000\n",
      "running time 23014.773315\n",
      "Train_EnvstepsSoFar : 3630001\n",
      "Train_AverageReturn : 1638.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23014.77331471443\n",
      "Training Loss : 0.9552742838859558\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3640001\n",
      "mean reward (100 episodes) 1638.600000\n",
      "best mean reward 1852.200000\n",
      "running time 23077.388927\n",
      "Train_EnvstepsSoFar : 3640001\n",
      "Train_AverageReturn : 1638.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23077.38892674446\n",
      "Training Loss : 0.35382795333862305\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3650001\n",
      "mean reward (100 episodes) 1674.600000\n",
      "best mean reward 1852.200000\n",
      "running time 23140.339376\n",
      "Train_EnvstepsSoFar : 3650001\n",
      "Train_AverageReturn : 1674.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23140.339376449585\n",
      "Training Loss : 0.1785680055618286\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3660001\n",
      "mean reward (100 episodes) 1681.100000\n",
      "best mean reward 1852.200000\n",
      "running time 23203.475539\n",
      "Train_EnvstepsSoFar : 3660001\n",
      "Train_AverageReturn : 1681.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23203.47553873062\n",
      "Training Loss : 0.21514496207237244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3670001\n",
      "mean reward (100 episodes) 1728.700000\n",
      "best mean reward 1852.200000\n",
      "running time 23266.734620\n",
      "Train_EnvstepsSoFar : 3670001\n",
      "Train_AverageReturn : 1728.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23266.73461985588\n",
      "Training Loss : 0.14575667679309845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3680001\n",
      "mean reward (100 episodes) 1751.900000\n",
      "best mean reward 1852.200000\n",
      "running time 23329.496841\n",
      "Train_EnvstepsSoFar : 3680001\n",
      "Train_AverageReturn : 1751.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23329.496841430664\n",
      "Training Loss : 1.4352699518203735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3690001\n",
      "mean reward (100 episodes) 1766.000000\n",
      "best mean reward 1852.200000\n",
      "running time 23392.075671\n",
      "Train_EnvstepsSoFar : 3690001\n",
      "Train_AverageReturn : 1766.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23392.075670957565\n",
      "Training Loss : 0.9809019565582275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3700001\n",
      "mean reward (100 episodes) 1695.100000\n",
      "best mean reward 1852.200000\n",
      "running time 23455.173619\n",
      "Train_EnvstepsSoFar : 3700001\n",
      "Train_AverageReturn : 1695.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23455.173618793488\n",
      "Training Loss : 0.22052106261253357\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3710001\n",
      "mean reward (100 episodes) 1699.200000\n",
      "best mean reward 1852.200000\n",
      "running time 23518.111412\n",
      "Train_EnvstepsSoFar : 3710001\n",
      "Train_AverageReturn : 1699.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23518.11141228676\n",
      "Training Loss : 0.40057915449142456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3720001\n",
      "mean reward (100 episodes) 1718.900000\n",
      "best mean reward 1852.200000\n",
      "running time 23581.062771\n",
      "Train_EnvstepsSoFar : 3720001\n",
      "Train_AverageReturn : 1718.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23581.062770843506\n",
      "Training Loss : 0.9704287052154541\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3730001\n",
      "mean reward (100 episodes) 1744.600000\n",
      "best mean reward 1852.200000\n",
      "running time 23643.890940\n",
      "Train_EnvstepsSoFar : 3730001\n",
      "Train_AverageReturn : 1744.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23643.89094042778\n",
      "Training Loss : 0.23931334912776947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3740001\n",
      "mean reward (100 episodes) 1761.400000\n",
      "best mean reward 1852.200000\n",
      "running time 23706.544523\n",
      "Train_EnvstepsSoFar : 3740001\n",
      "Train_AverageReturn : 1761.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23706.544523000717\n",
      "Training Loss : 1.4712642431259155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3750001\n",
      "mean reward (100 episodes) 1772.100000\n",
      "best mean reward 1852.200000\n",
      "running time 23769.239398\n",
      "Train_EnvstepsSoFar : 3750001\n",
      "Train_AverageReturn : 1772.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23769.239398241043\n",
      "Training Loss : 0.19784048199653625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3760001\n",
      "mean reward (100 episodes) 1786.400000\n",
      "best mean reward 1852.200000\n",
      "running time 23832.024731\n",
      "Train_EnvstepsSoFar : 3760001\n",
      "Train_AverageReturn : 1786.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23832.024730682373\n",
      "Training Loss : 0.42568057775497437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3770001\n",
      "mean reward (100 episodes) 1742.200000\n",
      "best mean reward 1852.200000\n",
      "running time 23894.892528\n",
      "Train_EnvstepsSoFar : 3770001\n",
      "Train_AverageReturn : 1742.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23894.89252758026\n",
      "Training Loss : 1.6465506553649902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3780001\n",
      "mean reward (100 episodes) 1762.000000\n",
      "best mean reward 1852.200000\n",
      "running time 23957.893464\n",
      "Train_EnvstepsSoFar : 3780001\n",
      "Train_AverageReturn : 1762.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 23957.89346432686\n",
      "Training Loss : 0.6195053458213806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3790001\n",
      "mean reward (100 episodes) 1709.800000\n",
      "best mean reward 1852.200000\n",
      "running time 24020.876423\n",
      "Train_EnvstepsSoFar : 3790001\n",
      "Train_AverageReturn : 1709.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24020.876423358917\n",
      "Training Loss : 0.24823355674743652\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3800001\n",
      "mean reward (100 episodes) 1697.200000\n",
      "best mean reward 1852.200000\n",
      "running time 24083.945065\n",
      "Train_EnvstepsSoFar : 3800001\n",
      "Train_AverageReturn : 1697.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24083.945065021515\n",
      "Training Loss : 0.1877698302268982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3810001\n",
      "mean reward (100 episodes) 1681.300000\n",
      "best mean reward 1852.200000\n",
      "running time 24146.671265\n",
      "Train_EnvstepsSoFar : 3810001\n",
      "Train_AverageReturn : 1681.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24146.671265363693\n",
      "Training Loss : 0.2529091238975525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3820001\n",
      "mean reward (100 episodes) 1751.000000\n",
      "best mean reward 1852.200000\n",
      "running time 24209.376785\n",
      "Train_EnvstepsSoFar : 3820001\n",
      "Train_AverageReturn : 1751.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24209.376784801483\n",
      "Training Loss : 0.31383806467056274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3830001\n",
      "mean reward (100 episodes) 1733.900000\n",
      "best mean reward 1852.200000\n",
      "running time 24272.225024\n",
      "Train_EnvstepsSoFar : 3830001\n",
      "Train_AverageReturn : 1733.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24272.225024223328\n",
      "Training Loss : 0.28013724088668823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3840001\n",
      "mean reward (100 episodes) 1801.300000\n",
      "best mean reward 1852.200000\n",
      "running time 24335.393392\n",
      "Train_EnvstepsSoFar : 3840001\n",
      "Train_AverageReturn : 1801.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24335.393391609192\n",
      "Training Loss : 0.26776939630508423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3850001\n",
      "mean reward (100 episodes) 1802.200000\n",
      "best mean reward 1852.200000\n",
      "running time 24398.551675\n",
      "Train_EnvstepsSoFar : 3850001\n",
      "Train_AverageReturn : 1802.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24398.55167531967\n",
      "Training Loss : 0.2570187449455261\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3860001\n",
      "mean reward (100 episodes) 1802.800000\n",
      "best mean reward 1852.200000\n",
      "running time 24462.122225\n",
      "Train_EnvstepsSoFar : 3860001\n",
      "Train_AverageReturn : 1802.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24462.122225284576\n",
      "Training Loss : 1.0075807571411133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3870001\n",
      "mean reward (100 episodes) 1740.200000\n",
      "best mean reward 1852.200000\n",
      "running time 24524.950377\n",
      "Train_EnvstepsSoFar : 3870001\n",
      "Train_AverageReturn : 1740.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24524.950377225876\n",
      "Training Loss : 0.5456469655036926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3880001\n",
      "mean reward (100 episodes) 1738.600000\n",
      "best mean reward 1852.200000\n",
      "running time 24587.778081\n",
      "Train_EnvstepsSoFar : 3880001\n",
      "Train_AverageReturn : 1738.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24587.778081417084\n",
      "Training Loss : 0.23888300359249115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3890001\n",
      "mean reward (100 episodes) 1706.700000\n",
      "best mean reward 1852.200000\n",
      "running time 24650.700968\n",
      "Train_EnvstepsSoFar : 3890001\n",
      "Train_AverageReturn : 1706.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24650.700968265533\n",
      "Training Loss : 0.1302611529827118\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3900001\n",
      "mean reward (100 episodes) 1793.400000\n",
      "best mean reward 1852.200000\n",
      "running time 24713.381814\n",
      "Train_EnvstepsSoFar : 3900001\n",
      "Train_AverageReturn : 1793.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24713.38181400299\n",
      "Training Loss : 1.1472479104995728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3910001\n",
      "mean reward (100 episodes) 1747.000000\n",
      "best mean reward 1852.200000\n",
      "running time 24776.213156\n",
      "Train_EnvstepsSoFar : 3910001\n",
      "Train_AverageReturn : 1747.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24776.21315574646\n",
      "Training Loss : 0.1744561344385147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3920001\n",
      "mean reward (100 episodes) 1729.600000\n",
      "best mean reward 1852.200000\n",
      "running time 24838.879937\n",
      "Train_EnvstepsSoFar : 3920001\n",
      "Train_AverageReturn : 1729.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24838.879937171936\n",
      "Training Loss : 0.5659075379371643\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3930001\n",
      "mean reward (100 episodes) 1664.800000\n",
      "best mean reward 1852.200000\n",
      "running time 24901.629025\n",
      "Train_EnvstepsSoFar : 3930001\n",
      "Train_AverageReturn : 1664.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24901.62902545929\n",
      "Training Loss : 1.4739376306533813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3940001\n",
      "mean reward (100 episodes) 1654.600000\n",
      "best mean reward 1852.200000\n",
      "running time 24964.576249\n",
      "Train_EnvstepsSoFar : 3940001\n",
      "Train_AverageReturn : 1654.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 24964.5762488842\n",
      "Training Loss : 0.789802610874176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3950001\n",
      "mean reward (100 episodes) 1730.900000\n",
      "best mean reward 1852.200000\n",
      "running time 25027.115490\n",
      "Train_EnvstepsSoFar : 3950001\n",
      "Train_AverageReturn : 1730.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25027.115490198135\n",
      "Training Loss : 0.21238981187343597\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3960001\n",
      "mean reward (100 episodes) 1763.500000\n",
      "best mean reward 1852.200000\n",
      "running time 25090.075933\n",
      "Train_EnvstepsSoFar : 3960001\n",
      "Train_AverageReturn : 1763.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25090.075932741165\n",
      "Training Loss : 0.2330058068037033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3970001\n",
      "mean reward (100 episodes) 1764.100000\n",
      "best mean reward 1852.200000\n",
      "running time 25152.421415\n",
      "Train_EnvstepsSoFar : 3970001\n",
      "Train_AverageReturn : 1764.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25152.42141509056\n",
      "Training Loss : 0.7251020669937134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3980001\n",
      "mean reward (100 episodes) 1714.300000\n",
      "best mean reward 1852.200000\n",
      "running time 25215.354857\n",
      "Train_EnvstepsSoFar : 3980001\n",
      "Train_AverageReturn : 1714.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25215.354856729507\n",
      "Training Loss : 0.17317330837249756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 3990001\n",
      "mean reward (100 episodes) 1706.200000\n",
      "best mean reward 1852.200000\n",
      "running time 25278.313317\n",
      "Train_EnvstepsSoFar : 3990001\n",
      "Train_AverageReturn : 1706.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25278.313316822052\n",
      "Training Loss : 0.8456947803497314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4000001\n",
      "mean reward (100 episodes) 1777.100000\n",
      "best mean reward 1852.200000\n",
      "running time 25341.094362\n",
      "Train_EnvstepsSoFar : 4000001\n",
      "Train_AverageReturn : 1777.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25341.094361543655\n",
      "Training Loss : 1.3626022338867188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4010001\n",
      "mean reward (100 episodes) 1793.400000\n",
      "best mean reward 1852.200000\n",
      "running time 25404.018949\n",
      "Train_EnvstepsSoFar : 4010001\n",
      "Train_AverageReturn : 1793.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25404.018948554993\n",
      "Training Loss : 2.3740906715393066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4020001\n",
      "mean reward (100 episodes) 1762.700000\n",
      "best mean reward 1852.200000\n",
      "running time 25466.895499\n",
      "Train_EnvstepsSoFar : 4020001\n",
      "Train_AverageReturn : 1762.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25466.895498991013\n",
      "Training Loss : 0.19237568974494934\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4030001\n",
      "mean reward (100 episodes) 1736.400000\n",
      "best mean reward 1852.200000\n",
      "running time 25529.786935\n",
      "Train_EnvstepsSoFar : 4030001\n",
      "Train_AverageReturn : 1736.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25529.786935329437\n",
      "Training Loss : 0.8548008799552917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4040001\n",
      "mean reward (100 episodes) 1733.400000\n",
      "best mean reward 1852.200000\n",
      "running time 25592.855983\n",
      "Train_EnvstepsSoFar : 4040001\n",
      "Train_AverageReturn : 1733.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25592.855983495712\n",
      "Training Loss : 0.41513529419898987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4050001\n",
      "mean reward (100 episodes) 1769.600000\n",
      "best mean reward 1852.200000\n",
      "running time 25656.000809\n",
      "Train_EnvstepsSoFar : 4050001\n",
      "Train_AverageReturn : 1769.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25656.00080895424\n",
      "Training Loss : 0.1588783860206604\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4060001\n",
      "mean reward (100 episodes) 1752.900000\n",
      "best mean reward 1852.200000\n",
      "running time 25718.770299\n",
      "Train_EnvstepsSoFar : 4060001\n",
      "Train_AverageReturn : 1752.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25718.770298719406\n",
      "Training Loss : 0.29583001136779785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4070001\n",
      "mean reward (100 episodes) 1678.000000\n",
      "best mean reward 1852.200000\n",
      "running time 25782.078332\n",
      "Train_EnvstepsSoFar : 4070001\n",
      "Train_AverageReturn : 1678.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25782.078331947327\n",
      "Training Loss : 0.8305040597915649\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4080001\n",
      "mean reward (100 episodes) 1594.900000\n",
      "best mean reward 1852.200000\n",
      "running time 25845.320103\n",
      "Train_EnvstepsSoFar : 4080001\n",
      "Train_AverageReturn : 1594.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25845.32010269165\n",
      "Training Loss : 0.11322193592786789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4090001\n",
      "mean reward (100 episodes) 1570.200000\n",
      "best mean reward 1852.200000\n",
      "running time 25908.047016\n",
      "Train_EnvstepsSoFar : 4090001\n",
      "Train_AverageReturn : 1570.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25908.047016382217\n",
      "Training Loss : 1.1666359901428223\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4100001\n",
      "mean reward (100 episodes) 1597.000000\n",
      "best mean reward 1852.200000\n",
      "running time 25970.556997\n",
      "Train_EnvstepsSoFar : 4100001\n",
      "Train_AverageReturn : 1597.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 25970.556997060776\n",
      "Training Loss : 0.1229289174079895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4110001\n",
      "mean reward (100 episodes) 1656.300000\n",
      "best mean reward 1852.200000\n",
      "running time 26033.238414\n",
      "Train_EnvstepsSoFar : 4110001\n",
      "Train_AverageReturn : 1656.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26033.23841404915\n",
      "Training Loss : 0.25286269187927246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4120001\n",
      "mean reward (100 episodes) 1636.600000\n",
      "best mean reward 1852.200000\n",
      "running time 26096.007825\n",
      "Train_EnvstepsSoFar : 4120001\n",
      "Train_AverageReturn : 1636.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26096.007824897766\n",
      "Training Loss : 0.20313581824302673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4130001\n",
      "mean reward (100 episodes) 1652.700000\n",
      "best mean reward 1852.200000\n",
      "running time 26159.059734\n",
      "Train_EnvstepsSoFar : 4130001\n",
      "Train_AverageReturn : 1652.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26159.059733629227\n",
      "Training Loss : 0.4864859879016876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4140001\n",
      "mean reward (100 episodes) 1693.900000\n",
      "best mean reward 1852.200000\n",
      "running time 26223.297518\n",
      "Train_EnvstepsSoFar : 4140001\n",
      "Train_AverageReturn : 1693.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26223.297518253326\n",
      "Training Loss : 0.8290553092956543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4150001\n",
      "mean reward (100 episodes) 1716.600000\n",
      "best mean reward 1852.200000\n",
      "running time 26286.524535\n",
      "Train_EnvstepsSoFar : 4150001\n",
      "Train_AverageReturn : 1716.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26286.5245347023\n",
      "Training Loss : 1.8108117580413818\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4160001\n",
      "mean reward (100 episodes) 1726.800000\n",
      "best mean reward 1852.200000\n",
      "running time 26349.585871\n",
      "Train_EnvstepsSoFar : 4160001\n",
      "Train_AverageReturn : 1726.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26349.58587050438\n",
      "Training Loss : 0.28857648372650146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4170001\n",
      "mean reward (100 episodes) 1720.500000\n",
      "best mean reward 1852.200000\n",
      "running time 26412.663551\n",
      "Train_EnvstepsSoFar : 4170001\n",
      "Train_AverageReturn : 1720.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26412.66355085373\n",
      "Training Loss : 1.048574447631836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4180001\n",
      "mean reward (100 episodes) 1678.300000\n",
      "best mean reward 1852.200000\n",
      "running time 26476.214700\n",
      "Train_EnvstepsSoFar : 4180001\n",
      "Train_AverageReturn : 1678.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26476.21469950676\n",
      "Training Loss : 0.25130271911621094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4190001\n",
      "mean reward (100 episodes) 1678.600000\n",
      "best mean reward 1852.200000\n",
      "running time 26539.663481\n",
      "Train_EnvstepsSoFar : 4190001\n",
      "Train_AverageReturn : 1678.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26539.663480997086\n",
      "Training Loss : 1.4949064254760742\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4200001\n",
      "mean reward (100 episodes) 1640.700000\n",
      "best mean reward 1852.200000\n",
      "running time 26602.385832\n",
      "Train_EnvstepsSoFar : 4200001\n",
      "Train_AverageReturn : 1640.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26602.385831832886\n",
      "Training Loss : 0.39229482412338257\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4210001\n",
      "mean reward (100 episodes) 1623.100000\n",
      "best mean reward 1852.200000\n",
      "running time 26665.411641\n",
      "Train_EnvstepsSoFar : 4210001\n",
      "Train_AverageReturn : 1623.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26665.411640644073\n",
      "Training Loss : 0.18888810276985168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4220001\n",
      "mean reward (100 episodes) 1659.100000\n",
      "best mean reward 1852.200000\n",
      "running time 26728.951128\n",
      "Train_EnvstepsSoFar : 4220001\n",
      "Train_AverageReturn : 1659.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26728.95112800598\n",
      "Training Loss : 0.9435836672782898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4230001\n",
      "mean reward (100 episodes) 1718.000000\n",
      "best mean reward 1852.200000\n",
      "running time 26792.364986\n",
      "Train_EnvstepsSoFar : 4230001\n",
      "Train_AverageReturn : 1718.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26792.36498594284\n",
      "Training Loss : 0.13995935022830963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4240001\n",
      "mean reward (100 episodes) 1691.100000\n",
      "best mean reward 1852.200000\n",
      "running time 26855.791340\n",
      "Train_EnvstepsSoFar : 4240001\n",
      "Train_AverageReturn : 1691.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26855.791340351105\n",
      "Training Loss : 0.25896507501602173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4250001\n",
      "mean reward (100 episodes) 1682.000000\n",
      "best mean reward 1852.200000\n",
      "running time 26918.995516\n",
      "Train_EnvstepsSoFar : 4250001\n",
      "Train_AverageReturn : 1682.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26918.9955163002\n",
      "Training Loss : 3.115720748901367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4260001\n",
      "mean reward (100 episodes) 1643.600000\n",
      "best mean reward 1852.200000\n",
      "running time 26982.045973\n",
      "Train_EnvstepsSoFar : 4260001\n",
      "Train_AverageReturn : 1643.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 26982.045973062515\n",
      "Training Loss : 0.82624751329422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4270001\n",
      "mean reward (100 episodes) 1657.300000\n",
      "best mean reward 1852.200000\n",
      "running time 27045.033909\n",
      "Train_EnvstepsSoFar : 4270001\n",
      "Train_AverageReturn : 1657.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27045.033909082413\n",
      "Training Loss : 0.5052453279495239\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4280001\n",
      "mean reward (100 episodes) 1625.800000\n",
      "best mean reward 1852.200000\n",
      "running time 27108.212415\n",
      "Train_EnvstepsSoFar : 4280001\n",
      "Train_AverageReturn : 1625.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27108.212414741516\n",
      "Training Loss : 0.884777843952179\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4290001\n",
      "mean reward (100 episodes) 1729.700000\n",
      "best mean reward 1852.200000\n",
      "running time 27171.426207\n",
      "Train_EnvstepsSoFar : 4290001\n",
      "Train_AverageReturn : 1729.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27171.426207304\n",
      "Training Loss : 0.24110594391822815\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4300001\n",
      "mean reward (100 episodes) 1778.000000\n",
      "best mean reward 1852.200000\n",
      "running time 27234.639506\n",
      "Train_EnvstepsSoFar : 4300001\n",
      "Train_AverageReturn : 1778.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27234.63950586319\n",
      "Training Loss : 0.7542345523834229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4310001\n",
      "mean reward (100 episodes) 1714.100000\n",
      "best mean reward 1852.200000\n",
      "running time 27297.794580\n",
      "Train_EnvstepsSoFar : 4310001\n",
      "Train_AverageReturn : 1714.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27297.794579982758\n",
      "Training Loss : 0.16663885116577148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4320001\n",
      "mean reward (100 episodes) 1646.000000\n",
      "best mean reward 1852.200000\n",
      "running time 27360.941611\n",
      "Train_EnvstepsSoFar : 4320001\n",
      "Train_AverageReturn : 1646.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27360.941610574722\n",
      "Training Loss : 0.457605242729187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4330001\n",
      "mean reward (100 episodes) 1585.300000\n",
      "best mean reward 1852.200000\n",
      "running time 27424.379810\n",
      "Train_EnvstepsSoFar : 4330001\n",
      "Train_AverageReturn : 1585.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27424.379809617996\n",
      "Training Loss : 0.28575077652931213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4340001\n",
      "mean reward (100 episodes) 1642.400000\n",
      "best mean reward 1852.200000\n",
      "running time 27487.946299\n",
      "Train_EnvstepsSoFar : 4340001\n",
      "Train_AverageReturn : 1642.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27487.94629907608\n",
      "Training Loss : 0.10939059406518936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4350001\n",
      "mean reward (100 episodes) 1614.200000\n",
      "best mean reward 1852.200000\n",
      "running time 27551.042968\n",
      "Train_EnvstepsSoFar : 4350001\n",
      "Train_AverageReturn : 1614.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27551.042967557907\n",
      "Training Loss : 0.6065507531166077\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4360001\n",
      "mean reward (100 episodes) 1620.400000\n",
      "best mean reward 1852.200000\n",
      "running time 27614.014861\n",
      "Train_EnvstepsSoFar : 4360001\n",
      "Train_AverageReturn : 1620.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27614.014861106873\n",
      "Training Loss : 0.19472379982471466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4370001\n",
      "mean reward (100 episodes) 1579.200000\n",
      "best mean reward 1852.200000\n",
      "running time 27677.158007\n",
      "Train_EnvstepsSoFar : 4370001\n",
      "Train_AverageReturn : 1579.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27677.158007383347\n",
      "Training Loss : 0.17154482007026672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4380001\n",
      "mean reward (100 episodes) 1595.800000\n",
      "best mean reward 1852.200000\n",
      "running time 27740.320661\n",
      "Train_EnvstepsSoFar : 4380001\n",
      "Train_AverageReturn : 1595.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27740.320661067963\n",
      "Training Loss : 0.188032329082489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4390001\n",
      "mean reward (100 episodes) 1592.200000\n",
      "best mean reward 1852.200000\n",
      "running time 27803.286463\n",
      "Train_EnvstepsSoFar : 4390001\n",
      "Train_AverageReturn : 1592.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27803.286462545395\n",
      "Training Loss : 0.2249399572610855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4400001\n",
      "mean reward (100 episodes) 1533.600000\n",
      "best mean reward 1852.200000\n",
      "running time 27866.845397\n",
      "Train_EnvstepsSoFar : 4400001\n",
      "Train_AverageReturn : 1533.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27866.84539747238\n",
      "Training Loss : 0.9901573657989502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4410001\n",
      "mean reward (100 episodes) 1514.100000\n",
      "best mean reward 1852.200000\n",
      "running time 27930.384369\n",
      "Train_EnvstepsSoFar : 4410001\n",
      "Train_AverageReturn : 1514.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27930.384368896484\n",
      "Training Loss : 0.8333871960639954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4420001\n",
      "mean reward (100 episodes) 1489.800000\n",
      "best mean reward 1852.200000\n",
      "running time 27994.626536\n",
      "Train_EnvstepsSoFar : 4420001\n",
      "Train_AverageReturn : 1489.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 27994.626536369324\n",
      "Training Loss : 0.636265754699707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4430001\n",
      "mean reward (100 episodes) 1623.200000\n",
      "best mean reward 1852.200000\n",
      "running time 28057.850585\n",
      "Train_EnvstepsSoFar : 4430001\n",
      "Train_AverageReturn : 1623.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28057.850584983826\n",
      "Training Loss : 0.2481507956981659\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4440001\n",
      "mean reward (100 episodes) 1654.100000\n",
      "best mean reward 1852.200000\n",
      "running time 28120.840427\n",
      "Train_EnvstepsSoFar : 4440001\n",
      "Train_AverageReturn : 1654.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28120.84042739868\n",
      "Training Loss : 0.7935019731521606\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4450001\n",
      "mean reward (100 episodes) 1670.100000\n",
      "best mean reward 1852.200000\n",
      "running time 28183.898309\n",
      "Train_EnvstepsSoFar : 4450001\n",
      "Train_AverageReturn : 1670.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28183.898308753967\n",
      "Training Loss : 0.8484398722648621\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4460001\n",
      "mean reward (100 episodes) 1584.000000\n",
      "best mean reward 1852.200000\n",
      "running time 28246.965353\n",
      "Train_EnvstepsSoFar : 4460001\n",
      "Train_AverageReturn : 1584.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28246.965353250504\n",
      "Training Loss : 0.23820790648460388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4470001\n",
      "mean reward (100 episodes) 1592.800000\n",
      "best mean reward 1852.200000\n",
      "running time 28309.954460\n",
      "Train_EnvstepsSoFar : 4470001\n",
      "Train_AverageReturn : 1592.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28309.954460144043\n",
      "Training Loss : 0.3284231424331665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4480001\n",
      "mean reward (100 episodes) 1564.100000\n",
      "best mean reward 1852.200000\n",
      "running time 28373.235919\n",
      "Train_EnvstepsSoFar : 4480001\n",
      "Train_AverageReturn : 1564.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28373.23591899872\n",
      "Training Loss : 1.4187053442001343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4490001\n",
      "mean reward (100 episodes) 1523.300000\n",
      "best mean reward 1852.200000\n",
      "running time 28436.192144\n",
      "Train_EnvstepsSoFar : 4490001\n",
      "Train_AverageReturn : 1523.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28436.192144155502\n",
      "Training Loss : 0.7289033532142639\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4500001\n",
      "mean reward (100 episodes) 1607.700000\n",
      "best mean reward 1852.200000\n",
      "running time 28499.418501\n",
      "Train_EnvstepsSoFar : 4500001\n",
      "Train_AverageReturn : 1607.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28499.41850066185\n",
      "Training Loss : 0.23946306109428406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4510001\n",
      "mean reward (100 episodes) 1633.200000\n",
      "best mean reward 1852.200000\n",
      "running time 28562.651090\n",
      "Train_EnvstepsSoFar : 4510001\n",
      "Train_AverageReturn : 1633.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28562.651089906693\n",
      "Training Loss : 0.5004866719245911\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4520001\n",
      "mean reward (100 episodes) 1705.300000\n",
      "best mean reward 1852.200000\n",
      "running time 28625.929003\n",
      "Train_EnvstepsSoFar : 4520001\n",
      "Train_AverageReturn : 1705.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28625.929003238678\n",
      "Training Loss : 1.5124620199203491\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4530001\n",
      "mean reward (100 episodes) 1650.600000\n",
      "best mean reward 1852.200000\n",
      "running time 28689.036387\n",
      "Train_EnvstepsSoFar : 4530001\n",
      "Train_AverageReturn : 1650.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28689.036387205124\n",
      "Training Loss : 0.3117639124393463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4540001\n",
      "mean reward (100 episodes) 1644.300000\n",
      "best mean reward 1852.200000\n",
      "running time 28751.938912\n",
      "Train_EnvstepsSoFar : 4540001\n",
      "Train_AverageReturn : 1644.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28751.938912153244\n",
      "Training Loss : 0.24622467160224915\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4550001\n",
      "mean reward (100 episodes) 1615.300000\n",
      "best mean reward 1852.200000\n",
      "running time 28815.478479\n",
      "Train_EnvstepsSoFar : 4550001\n",
      "Train_AverageReturn : 1615.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28815.47847867012\n",
      "Training Loss : 0.2385673075914383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4560001\n",
      "mean reward (100 episodes) 1669.400000\n",
      "best mean reward 1852.200000\n",
      "running time 28878.517893\n",
      "Train_EnvstepsSoFar : 4560001\n",
      "Train_AverageReturn : 1669.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28878.517892837524\n",
      "Training Loss : 0.5934420824050903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4570001\n",
      "mean reward (100 episodes) 1680.300000\n",
      "best mean reward 1852.200000\n",
      "running time 28941.480974\n",
      "Train_EnvstepsSoFar : 4570001\n",
      "Train_AverageReturn : 1680.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 28941.48097395897\n",
      "Training Loss : 0.6871713995933533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4580001\n",
      "mean reward (100 episodes) 1611.200000\n",
      "best mean reward 1852.200000\n",
      "running time 29004.505268\n",
      "Train_EnvstepsSoFar : 4580001\n",
      "Train_AverageReturn : 1611.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29004.505267620087\n",
      "Training Loss : 0.936674177646637\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4590001\n",
      "mean reward (100 episodes) 1587.000000\n",
      "best mean reward 1852.200000\n",
      "running time 29067.599079\n",
      "Train_EnvstepsSoFar : 4590001\n",
      "Train_AverageReturn : 1587.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29067.59907889366\n",
      "Training Loss : 1.581536054611206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4600001\n",
      "mean reward (100 episodes) 1582.300000\n",
      "best mean reward 1852.200000\n",
      "running time 29130.680490\n",
      "Train_EnvstepsSoFar : 4600001\n",
      "Train_AverageReturn : 1582.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29130.680490493774\n",
      "Training Loss : 1.4843168258666992\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4610001\n",
      "mean reward (100 episodes) 1576.500000\n",
      "best mean reward 1852.200000\n",
      "running time 29194.035440\n",
      "Train_EnvstepsSoFar : 4610001\n",
      "Train_AverageReturn : 1576.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29194.03543996811\n",
      "Training Loss : 0.21625742316246033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4620001\n",
      "mean reward (100 episodes) 1561.700000\n",
      "best mean reward 1852.200000\n",
      "running time 29256.741085\n",
      "Train_EnvstepsSoFar : 4620001\n",
      "Train_AverageReturn : 1561.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29256.74108481407\n",
      "Training Loss : 0.2688228487968445\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4630001\n",
      "mean reward (100 episodes) 1564.700000\n",
      "best mean reward 1852.200000\n",
      "running time 29319.776957\n",
      "Train_EnvstepsSoFar : 4630001\n",
      "Train_AverageReturn : 1564.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29319.776956558228\n",
      "Training Loss : 0.19767093658447266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4640001\n",
      "mean reward (100 episodes) 1598.300000\n",
      "best mean reward 1852.200000\n",
      "running time 29383.382255\n",
      "Train_EnvstepsSoFar : 4640001\n",
      "Train_AverageReturn : 1598.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29383.382255077362\n",
      "Training Loss : 0.3949059247970581\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4650001\n",
      "mean reward (100 episodes) 1633.600000\n",
      "best mean reward 1852.200000\n",
      "running time 29446.536233\n",
      "Train_EnvstepsSoFar : 4650001\n",
      "Train_AverageReturn : 1633.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29446.536232709885\n",
      "Training Loss : 0.22356435656547546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4660001\n",
      "mean reward (100 episodes) 1688.500000\n",
      "best mean reward 1852.200000\n",
      "running time 29509.477679\n",
      "Train_EnvstepsSoFar : 4660001\n",
      "Train_AverageReturn : 1688.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29509.477679014206\n",
      "Training Loss : 0.13969632983207703\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4670001\n",
      "mean reward (100 episodes) 1732.600000\n",
      "best mean reward 1852.200000\n",
      "running time 29572.470911\n",
      "Train_EnvstepsSoFar : 4670001\n",
      "Train_AverageReturn : 1732.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29572.47091126442\n",
      "Training Loss : 0.16512252390384674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4680001\n",
      "mean reward (100 episodes) 1790.100000\n",
      "best mean reward 1852.200000\n",
      "running time 29635.350214\n",
      "Train_EnvstepsSoFar : 4680001\n",
      "Train_AverageReturn : 1790.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29635.350214242935\n",
      "Training Loss : 1.638196587562561\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4690001\n",
      "mean reward (100 episodes) 1756.400000\n",
      "best mean reward 1852.200000\n",
      "running time 29699.196362\n",
      "Train_EnvstepsSoFar : 4690001\n",
      "Train_AverageReturn : 1756.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29699.196361541748\n",
      "Training Loss : 0.8407500982284546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4700001\n",
      "mean reward (100 episodes) 1674.200000\n",
      "best mean reward 1852.200000\n",
      "running time 29761.893757\n",
      "Train_EnvstepsSoFar : 4700001\n",
      "Train_AverageReturn : 1674.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29761.893756628036\n",
      "Training Loss : 2.9340476989746094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4710001\n",
      "mean reward (100 episodes) 1658.600000\n",
      "best mean reward 1852.200000\n",
      "running time 29824.907093\n",
      "Train_EnvstepsSoFar : 4710001\n",
      "Train_AverageReturn : 1658.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29824.907092809677\n",
      "Training Loss : 0.16913555562496185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4720001\n",
      "mean reward (100 episodes) 1738.700000\n",
      "best mean reward 1852.200000\n",
      "running time 29887.329176\n",
      "Train_EnvstepsSoFar : 4720001\n",
      "Train_AverageReturn : 1738.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29887.329175949097\n",
      "Training Loss : 0.08827728778123856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4730001\n",
      "mean reward (100 episodes) 1794.400000\n",
      "best mean reward 1852.200000\n",
      "running time 29949.722597\n",
      "Train_EnvstepsSoFar : 4730001\n",
      "Train_AverageReturn : 1794.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 29949.722597122192\n",
      "Training Loss : 0.357083797454834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4740001\n",
      "mean reward (100 episodes) 1758.100000\n",
      "best mean reward 1852.200000\n",
      "running time 30012.642745\n",
      "Train_EnvstepsSoFar : 4740001\n",
      "Train_AverageReturn : 1758.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30012.642745494843\n",
      "Training Loss : 0.18960466980934143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4750001\n",
      "mean reward (100 episodes) 1649.300000\n",
      "best mean reward 1852.200000\n",
      "running time 30075.156206\n",
      "Train_EnvstepsSoFar : 4750001\n",
      "Train_AverageReturn : 1649.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30075.15620613098\n",
      "Training Loss : 0.2349415123462677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4760001\n",
      "mean reward (100 episodes) 1576.600000\n",
      "best mean reward 1852.200000\n",
      "running time 30138.042722\n",
      "Train_EnvstepsSoFar : 4760001\n",
      "Train_AverageReturn : 1576.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30138.042721748352\n",
      "Training Loss : 0.7582347393035889\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4770001\n",
      "mean reward (100 episodes) 1573.300000\n",
      "best mean reward 1852.200000\n",
      "running time 30200.835595\n",
      "Train_EnvstepsSoFar : 4770001\n",
      "Train_AverageReturn : 1573.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30200.83559536934\n",
      "Training Loss : 0.20991960167884827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4780001\n",
      "mean reward (100 episodes) 1608.000000\n",
      "best mean reward 1852.200000\n",
      "running time 30263.717494\n",
      "Train_EnvstepsSoFar : 4780001\n",
      "Train_AverageReturn : 1608.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30263.717494010925\n",
      "Training Loss : 1.1460771560668945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4790001\n",
      "mean reward (100 episodes) 1701.900000\n",
      "best mean reward 1852.200000\n",
      "running time 30327.057509\n",
      "Train_EnvstepsSoFar : 4790001\n",
      "Train_AverageReturn : 1701.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30327.057509422302\n",
      "Training Loss : 2.0406203269958496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4800001\n",
      "mean reward (100 episodes) 1737.900000\n",
      "best mean reward 1852.200000\n",
      "running time 30389.767255\n",
      "Train_EnvstepsSoFar : 4800001\n",
      "Train_AverageReturn : 1737.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30389.767254829407\n",
      "Training Loss : 0.9016354084014893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4810001\n",
      "mean reward (100 episodes) 1700.900000\n",
      "best mean reward 1852.200000\n",
      "running time 30452.379720\n",
      "Train_EnvstepsSoFar : 4810001\n",
      "Train_AverageReturn : 1700.9\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30452.379720449448\n",
      "Training Loss : 0.18015432357788086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4820001\n",
      "mean reward (100 episodes) 1613.600000\n",
      "best mean reward 1852.200000\n",
      "running time 30514.978507\n",
      "Train_EnvstepsSoFar : 4820001\n",
      "Train_AverageReturn : 1613.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30514.978506803513\n",
      "Training Loss : 0.10394604504108429\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4830001\n",
      "mean reward (100 episodes) 1619.800000\n",
      "best mean reward 1852.200000\n",
      "running time 30577.789078\n",
      "Train_EnvstepsSoFar : 4830001\n",
      "Train_AverageReturn : 1619.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30577.78907752037\n",
      "Training Loss : 1.9311329126358032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4840001\n",
      "mean reward (100 episodes) 1597.200000\n",
      "best mean reward 1852.200000\n",
      "running time 30641.070532\n",
      "Train_EnvstepsSoFar : 4840001\n",
      "Train_AverageReturn : 1597.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30641.07053232193\n",
      "Training Loss : 0.16136811673641205\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4850001\n",
      "mean reward (100 episodes) 1583.400000\n",
      "best mean reward 1852.200000\n",
      "running time 30703.943964\n",
      "Train_EnvstepsSoFar : 4850001\n",
      "Train_AverageReturn : 1583.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30703.94396352768\n",
      "Training Loss : 0.21085232496261597\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4860001\n",
      "mean reward (100 episodes) 1615.700000\n",
      "best mean reward 1852.200000\n",
      "running time 30766.777981\n",
      "Train_EnvstepsSoFar : 4860001\n",
      "Train_AverageReturn : 1615.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30766.777980804443\n",
      "Training Loss : 0.2512665390968323\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4870001\n",
      "mean reward (100 episodes) 1624.200000\n",
      "best mean reward 1852.200000\n",
      "running time 30829.423316\n",
      "Train_EnvstepsSoFar : 4870001\n",
      "Train_AverageReturn : 1624.2\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30829.423315763474\n",
      "Training Loss : 0.4782752990722656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4880001\n",
      "mean reward (100 episodes) 1691.500000\n",
      "best mean reward 1852.200000\n",
      "running time 30892.014514\n",
      "Train_EnvstepsSoFar : 4880001\n",
      "Train_AverageReturn : 1691.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30892.014513731003\n",
      "Training Loss : 0.2827509045600891\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4890001\n",
      "mean reward (100 episodes) 1664.300000\n",
      "best mean reward 1852.200000\n",
      "running time 30954.675502\n",
      "Train_EnvstepsSoFar : 4890001\n",
      "Train_AverageReturn : 1664.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 30954.675502300262\n",
      "Training Loss : 0.29409340023994446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4900001\n",
      "mean reward (100 episodes) 1698.600000\n",
      "best mean reward 1852.200000\n",
      "running time 31017.568720\n",
      "Train_EnvstepsSoFar : 4900001\n",
      "Train_AverageReturn : 1698.6\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31017.56872010231\n",
      "Training Loss : 0.41112643480300903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4910001\n",
      "mean reward (100 episodes) 1632.000000\n",
      "best mean reward 1852.200000\n",
      "running time 31080.305296\n",
      "Train_EnvstepsSoFar : 4910001\n",
      "Train_AverageReturn : 1632.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31080.305295705795\n",
      "Training Loss : 0.27807557582855225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4920001\n",
      "mean reward (100 episodes) 1684.400000\n",
      "best mean reward 1852.200000\n",
      "running time 31143.252614\n",
      "Train_EnvstepsSoFar : 4920001\n",
      "Train_AverageReturn : 1684.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31143.25261449814\n",
      "Training Loss : 0.3420027196407318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4930001\n",
      "mean reward (100 episodes) 1719.800000\n",
      "best mean reward 1852.200000\n",
      "running time 31206.327896\n",
      "Train_EnvstepsSoFar : 4930001\n",
      "Train_AverageReturn : 1719.8\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31206.327896118164\n",
      "Training Loss : 0.16960492730140686\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4940001\n",
      "mean reward (100 episodes) 1735.500000\n",
      "best mean reward 1852.200000\n",
      "running time 31269.193734\n",
      "Train_EnvstepsSoFar : 4940001\n",
      "Train_AverageReturn : 1735.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31269.193734407425\n",
      "Training Loss : 0.633975625038147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4950001\n",
      "mean reward (100 episodes) 1657.700000\n",
      "best mean reward 1852.200000\n",
      "running time 31332.050939\n",
      "Train_EnvstepsSoFar : 4950001\n",
      "Train_AverageReturn : 1657.7\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31332.050938606262\n",
      "Training Loss : 1.5091400146484375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4960001\n",
      "mean reward (100 episodes) 1666.500000\n",
      "best mean reward 1852.200000\n",
      "running time 31395.080832\n",
      "Train_EnvstepsSoFar : 4960001\n",
      "Train_AverageReturn : 1666.5\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31395.08083176613\n",
      "Training Loss : 0.5634756684303284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4970001\n",
      "mean reward (100 episodes) 1717.000000\n",
      "best mean reward 1852.200000\n",
      "running time 31458.514191\n",
      "Train_EnvstepsSoFar : 4970001\n",
      "Train_AverageReturn : 1717.0\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31458.514190912247\n",
      "Training Loss : 1.4240473508834839\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4980001\n",
      "mean reward (100 episodes) 1729.100000\n",
      "best mean reward 1852.200000\n",
      "running time 31522.549227\n",
      "Train_EnvstepsSoFar : 4980001\n",
      "Train_AverageReturn : 1729.1\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31522.549226522446\n",
      "Training Loss : 0.2490885853767395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 4990001\n",
      "mean reward (100 episodes) 1699.300000\n",
      "best mean reward 1852.200000\n",
      "running time 31585.769080\n",
      "Train_EnvstepsSoFar : 4990001\n",
      "Train_AverageReturn : 1699.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31585.76908016205\n",
      "Training Loss : 1.004144310951233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5000001\n",
      "mean reward (100 episodes) 1766.400000\n",
      "best mean reward 1852.200000\n",
      "running time 31649.090308\n",
      "Train_EnvstepsSoFar : 5000001\n",
      "Train_AverageReturn : 1766.4\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31649.090307712555\n",
      "Training Loss : 0.47869688272476196\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5010001\n",
      "mean reward (100 episodes) 1825.300000\n",
      "best mean reward 1852.200000\n",
      "running time 31712.011805\n",
      "Train_EnvstepsSoFar : 5010001\n",
      "Train_AverageReturn : 1825.3\n",
      "Train_BestReturn : 1852.2\n",
      "TimeSinceStart : 31712.011805057526\n",
      "Training Loss : 0.2697451710700989\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5020001\n",
      "mean reward (100 episodes) 1856.000000\n",
      "best mean reward 1856.000000\n",
      "running time 31775.235012\n",
      "Train_EnvstepsSoFar : 5020001\n",
      "Train_AverageReturn : 1856.0\n",
      "Train_BestReturn : 1856.0\n",
      "TimeSinceStart : 31775.235011816025\n",
      "Training Loss : 0.27007174491882324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5030001\n",
      "mean reward (100 episodes) 1790.700000\n",
      "best mean reward 1856.000000\n",
      "running time 31838.326286\n",
      "Train_EnvstepsSoFar : 5030001\n",
      "Train_AverageReturn : 1790.7\n",
      "Train_BestReturn : 1856.0\n",
      "TimeSinceStart : 31838.326286315918\n",
      "Training Loss : 0.265226274728775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5040001\n",
      "mean reward (100 episodes) 1839.400000\n",
      "best mean reward 1856.000000\n",
      "running time 31901.744306\n",
      "Train_EnvstepsSoFar : 5040001\n",
      "Train_AverageReturn : 1839.4\n",
      "Train_BestReturn : 1856.0\n",
      "TimeSinceStart : 31901.744306325912\n",
      "Training Loss : 0.2848893404006958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5050001\n",
      "mean reward (100 episodes) 1862.600000\n",
      "best mean reward 1862.600000\n",
      "running time 31964.875062\n",
      "Train_EnvstepsSoFar : 5050001\n",
      "Train_AverageReturn : 1862.6\n",
      "Train_BestReturn : 1862.6\n",
      "TimeSinceStart : 31964.87506222725\n",
      "Training Loss : 1.4702503681182861\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5060001\n",
      "mean reward (100 episodes) 1882.300000\n",
      "best mean reward 1882.300000\n",
      "running time 32027.771376\n",
      "Train_EnvstepsSoFar : 5060001\n",
      "Train_AverageReturn : 1882.3\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32027.771375894547\n",
      "Training Loss : 0.2712039053440094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5070001\n",
      "mean reward (100 episodes) 1801.600000\n",
      "best mean reward 1882.300000\n",
      "running time 32090.865700\n",
      "Train_EnvstepsSoFar : 5070001\n",
      "Train_AverageReturn : 1801.6\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32090.865700483322\n",
      "Training Loss : 0.608500599861145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5080001\n",
      "mean reward (100 episodes) 1764.600000\n",
      "best mean reward 1882.300000\n",
      "running time 32154.230690\n",
      "Train_EnvstepsSoFar : 5080001\n",
      "Train_AverageReturn : 1764.6\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32154.23069024086\n",
      "Training Loss : 0.45246803760528564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5090001\n",
      "mean reward (100 episodes) 1804.100000\n",
      "best mean reward 1882.300000\n",
      "running time 32217.621672\n",
      "Train_EnvstepsSoFar : 5090001\n",
      "Train_AverageReturn : 1804.1\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32217.621671676636\n",
      "Training Loss : 0.15761995315551758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5100001\n",
      "mean reward (100 episodes) 1845.000000\n",
      "best mean reward 1882.300000\n",
      "running time 32280.038254\n",
      "Train_EnvstepsSoFar : 5100001\n",
      "Train_AverageReturn : 1845.0\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32280.0382540226\n",
      "Training Loss : 0.39323535561561584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5110001\n",
      "mean reward (100 episodes) 1781.300000\n",
      "best mean reward 1882.300000\n",
      "running time 32342.869540\n",
      "Train_EnvstepsSoFar : 5110001\n",
      "Train_AverageReturn : 1781.3\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32342.8695397377\n",
      "Training Loss : 0.5568885207176208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5120001\n",
      "mean reward (100 episodes) 1778.700000\n",
      "best mean reward 1882.300000\n",
      "running time 32406.157345\n",
      "Train_EnvstepsSoFar : 5120001\n",
      "Train_AverageReturn : 1778.7\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32406.157345056534\n",
      "Training Loss : 0.7238482236862183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5130001\n",
      "mean reward (100 episodes) 1791.600000\n",
      "best mean reward 1882.300000\n",
      "running time 32469.221109\n",
      "Train_EnvstepsSoFar : 5130001\n",
      "Train_AverageReturn : 1791.6\n",
      "Train_BestReturn : 1882.3\n",
      "TimeSinceStart : 32469.22110939026\n",
      "Training Loss : 1.1800224781036377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5140001\n",
      "mean reward (100 episodes) 1886.900000\n",
      "best mean reward 1886.900000\n",
      "running time 32532.190619\n",
      "Train_EnvstepsSoFar : 5140001\n",
      "Train_AverageReturn : 1886.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32532.190618515015\n",
      "Training Loss : 0.24081969261169434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5150001\n",
      "mean reward (100 episodes) 1858.800000\n",
      "best mean reward 1886.900000\n",
      "running time 32595.021432\n",
      "Train_EnvstepsSoFar : 5150001\n",
      "Train_AverageReturn : 1858.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32595.021431684494\n",
      "Training Loss : 0.4919016659259796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5160001\n",
      "mean reward (100 episodes) 1730.300000\n",
      "best mean reward 1886.900000\n",
      "running time 32658.037864\n",
      "Train_EnvstepsSoFar : 5160001\n",
      "Train_AverageReturn : 1730.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32658.037863969803\n",
      "Training Loss : 0.19935794174671173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5170001\n",
      "mean reward (100 episodes) 1645.300000\n",
      "best mean reward 1886.900000\n",
      "running time 32721.098953\n",
      "Train_EnvstepsSoFar : 5170001\n",
      "Train_AverageReturn : 1645.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32721.09895324707\n",
      "Training Loss : 0.3566458821296692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5180001\n",
      "mean reward (100 episodes) 1717.000000\n",
      "best mean reward 1886.900000\n",
      "running time 32784.183889\n",
      "Train_EnvstepsSoFar : 5180001\n",
      "Train_AverageReturn : 1717.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32784.18388867378\n",
      "Training Loss : 1.1975027322769165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5190001\n",
      "mean reward (100 episodes) 1773.500000\n",
      "best mean reward 1886.900000\n",
      "running time 32847.339063\n",
      "Train_EnvstepsSoFar : 5190001\n",
      "Train_AverageReturn : 1773.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32847.33906316757\n",
      "Training Loss : 0.3234540820121765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5200001\n",
      "mean reward (100 episodes) 1755.100000\n",
      "best mean reward 1886.900000\n",
      "running time 32910.695928\n",
      "Train_EnvstepsSoFar : 5200001\n",
      "Train_AverageReturn : 1755.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32910.69592785835\n",
      "Training Loss : 1.5201280117034912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5210001\n",
      "mean reward (100 episodes) 1778.300000\n",
      "best mean reward 1886.900000\n",
      "running time 32973.866000\n",
      "Train_EnvstepsSoFar : 5210001\n",
      "Train_AverageReturn : 1778.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 32973.866000175476\n",
      "Training Loss : 0.392244815826416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5220001\n",
      "mean reward (100 episodes) 1740.400000\n",
      "best mean reward 1886.900000\n",
      "running time 33037.973889\n",
      "Train_EnvstepsSoFar : 5220001\n",
      "Train_AverageReturn : 1740.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33037.97388911247\n",
      "Training Loss : 0.8203622102737427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5230001\n",
      "mean reward (100 episodes) 1698.900000\n",
      "best mean reward 1886.900000\n",
      "running time 33101.400297\n",
      "Train_EnvstepsSoFar : 5230001\n",
      "Train_AverageReturn : 1698.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33101.4002969265\n",
      "Training Loss : 0.5820448398590088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5240001\n",
      "mean reward (100 episodes) 1696.400000\n",
      "best mean reward 1886.900000\n",
      "running time 33164.596143\n",
      "Train_EnvstepsSoFar : 5240001\n",
      "Train_AverageReturn : 1696.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33164.596143484116\n",
      "Training Loss : 2.2069344520568848\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5250001\n",
      "mean reward (100 episodes) 1672.600000\n",
      "best mean reward 1886.900000\n",
      "running time 33227.738343\n",
      "Train_EnvstepsSoFar : 5250001\n",
      "Train_AverageReturn : 1672.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33227.73834276199\n",
      "Training Loss : 0.09809349477291107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5260001\n",
      "mean reward (100 episodes) 1664.500000\n",
      "best mean reward 1886.900000\n",
      "running time 33291.434980\n",
      "Train_EnvstepsSoFar : 5260001\n",
      "Train_AverageReturn : 1664.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33291.43498015404\n",
      "Training Loss : 0.15031090378761292\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5270001\n",
      "mean reward (100 episodes) 1731.800000\n",
      "best mean reward 1886.900000\n",
      "running time 33354.673114\n",
      "Train_EnvstepsSoFar : 5270001\n",
      "Train_AverageReturn : 1731.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33354.673114299774\n",
      "Training Loss : 1.8693369626998901\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5280001\n",
      "mean reward (100 episodes) 1789.200000\n",
      "best mean reward 1886.900000\n",
      "running time 33417.772813\n",
      "Train_EnvstepsSoFar : 5280001\n",
      "Train_AverageReturn : 1789.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33417.77281332016\n",
      "Training Loss : 0.4343172311782837\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5290001\n",
      "mean reward (100 episodes) 1863.400000\n",
      "best mean reward 1886.900000\n",
      "running time 33480.373146\n",
      "Train_EnvstepsSoFar : 5290001\n",
      "Train_AverageReturn : 1863.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33480.37314629555\n",
      "Training Loss : 0.396793007850647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5300001\n",
      "mean reward (100 episodes) 1817.300000\n",
      "best mean reward 1886.900000\n",
      "running time 33543.421576\n",
      "Train_EnvstepsSoFar : 5300001\n",
      "Train_AverageReturn : 1817.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33543.42157578468\n",
      "Training Loss : 0.22690865397453308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5310001\n",
      "mean reward (100 episodes) 1768.800000\n",
      "best mean reward 1886.900000\n",
      "running time 33606.240214\n",
      "Train_EnvstepsSoFar : 5310001\n",
      "Train_AverageReturn : 1768.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33606.24021434784\n",
      "Training Loss : 0.5759159922599792\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5320001\n",
      "mean reward (100 episodes) 1716.000000\n",
      "best mean reward 1886.900000\n",
      "running time 33669.002853\n",
      "Train_EnvstepsSoFar : 5320001\n",
      "Train_AverageReturn : 1716.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33669.002853393555\n",
      "Training Loss : 0.8163223266601562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5330001\n",
      "mean reward (100 episodes) 1791.700000\n",
      "best mean reward 1886.900000\n",
      "running time 33732.081883\n",
      "Train_EnvstepsSoFar : 5330001\n",
      "Train_AverageReturn : 1791.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33732.08188343048\n",
      "Training Loss : 1.4113013744354248\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5340001\n",
      "mean reward (100 episodes) 1795.700000\n",
      "best mean reward 1886.900000\n",
      "running time 33794.935181\n",
      "Train_EnvstepsSoFar : 5340001\n",
      "Train_AverageReturn : 1795.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33794.93518090248\n",
      "Training Loss : 1.382420539855957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5350001\n",
      "mean reward (100 episodes) 1786.800000\n",
      "best mean reward 1886.900000\n",
      "running time 33857.646310\n",
      "Train_EnvstepsSoFar : 5350001\n",
      "Train_AverageReturn : 1786.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33857.64630961418\n",
      "Training Loss : 0.5429571270942688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5360001\n",
      "mean reward (100 episodes) 1784.700000\n",
      "best mean reward 1886.900000\n",
      "running time 33920.660045\n",
      "Train_EnvstepsSoFar : 5360001\n",
      "Train_AverageReturn : 1784.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33920.660044670105\n",
      "Training Loss : 0.7529456615447998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5370001\n",
      "mean reward (100 episodes) 1778.500000\n",
      "best mean reward 1886.900000\n",
      "running time 33983.444536\n",
      "Train_EnvstepsSoFar : 5370001\n",
      "Train_AverageReturn : 1778.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 33983.44453573227\n",
      "Training Loss : 1.4545994997024536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5380001\n",
      "mean reward (100 episodes) 1739.900000\n",
      "best mean reward 1886.900000\n",
      "running time 34046.443555\n",
      "Train_EnvstepsSoFar : 5380001\n",
      "Train_AverageReturn : 1739.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34046.44355535507\n",
      "Training Loss : 0.28745412826538086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5390001\n",
      "mean reward (100 episodes) 1806.700000\n",
      "best mean reward 1886.900000\n",
      "running time 34109.386497\n",
      "Train_EnvstepsSoFar : 5390001\n",
      "Train_AverageReturn : 1806.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34109.38649702072\n",
      "Training Loss : 0.9789230823516846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5400001\n",
      "mean reward (100 episodes) 1859.800000\n",
      "best mean reward 1886.900000\n",
      "running time 34172.365505\n",
      "Train_EnvstepsSoFar : 5400001\n",
      "Train_AverageReturn : 1859.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34172.36550474167\n",
      "Training Loss : 4.5303955078125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5410001\n",
      "mean reward (100 episodes) 1860.000000\n",
      "best mean reward 1886.900000\n",
      "running time 34235.367678\n",
      "Train_EnvstepsSoFar : 5410001\n",
      "Train_AverageReturn : 1860.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34235.367678403854\n",
      "Training Loss : 0.9765469431877136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5420001\n",
      "mean reward (100 episodes) 1782.800000\n",
      "best mean reward 1886.900000\n",
      "running time 34298.458488\n",
      "Train_EnvstepsSoFar : 5420001\n",
      "Train_AverageReturn : 1782.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34298.458488464355\n",
      "Training Loss : 0.16659529507160187\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5430001\n",
      "mean reward (100 episodes) 1747.400000\n",
      "best mean reward 1886.900000\n",
      "running time 34361.353238\n",
      "Train_EnvstepsSoFar : 5430001\n",
      "Train_AverageReturn : 1747.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34361.353238105774\n",
      "Training Loss : 0.26167160272598267\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5440001\n",
      "mean reward (100 episodes) 1799.100000\n",
      "best mean reward 1886.900000\n",
      "running time 34424.157331\n",
      "Train_EnvstepsSoFar : 5440001\n",
      "Train_AverageReturn : 1799.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34424.15733075142\n",
      "Training Loss : 0.1533224880695343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5450001\n",
      "mean reward (100 episodes) 1739.500000\n",
      "best mean reward 1886.900000\n",
      "running time 34487.156807\n",
      "Train_EnvstepsSoFar : 5450001\n",
      "Train_AverageReturn : 1739.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34487.15680718422\n",
      "Training Loss : 1.517011046409607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5460001\n",
      "mean reward (100 episodes) 1679.900000\n",
      "best mean reward 1886.900000\n",
      "running time 34550.329501\n",
      "Train_EnvstepsSoFar : 5460001\n",
      "Train_AverageReturn : 1679.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34550.32950115204\n",
      "Training Loss : 0.2544720768928528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5470001\n",
      "mean reward (100 episodes) 1632.700000\n",
      "best mean reward 1886.900000\n",
      "running time 34613.549072\n",
      "Train_EnvstepsSoFar : 5470001\n",
      "Train_AverageReturn : 1632.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34613.54907178879\n",
      "Training Loss : 0.36610567569732666\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5480001\n",
      "mean reward (100 episodes) 1628.500000\n",
      "best mean reward 1886.900000\n",
      "running time 34676.550949\n",
      "Train_EnvstepsSoFar : 5480001\n",
      "Train_AverageReturn : 1628.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34676.55094861984\n",
      "Training Loss : 0.8792797923088074\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5490001\n",
      "mean reward (100 episodes) 1625.400000\n",
      "best mean reward 1886.900000\n",
      "running time 34739.432237\n",
      "Train_EnvstepsSoFar : 5490001\n",
      "Train_AverageReturn : 1625.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34739.432237148285\n",
      "Training Loss : 0.6710829138755798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5500001\n",
      "mean reward (100 episodes) 1694.400000\n",
      "best mean reward 1886.900000\n",
      "running time 34802.410924\n",
      "Train_EnvstepsSoFar : 5500001\n",
      "Train_AverageReturn : 1694.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34802.41092419624\n",
      "Training Loss : 1.4873137474060059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5510001\n",
      "mean reward (100 episodes) 1712.400000\n",
      "best mean reward 1886.900000\n",
      "running time 34865.565719\n",
      "Train_EnvstepsSoFar : 5510001\n",
      "Train_AverageReturn : 1712.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34865.56571888924\n",
      "Training Loss : 0.4807561933994293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5520001\n",
      "mean reward (100 episodes) 1756.000000\n",
      "best mean reward 1886.900000\n",
      "running time 34928.271904\n",
      "Train_EnvstepsSoFar : 5520001\n",
      "Train_AverageReturn : 1756.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34928.27190351486\n",
      "Training Loss : 0.2734375596046448\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5530001\n",
      "mean reward (100 episodes) 1759.900000\n",
      "best mean reward 1886.900000\n",
      "running time 34991.163694\n",
      "Train_EnvstepsSoFar : 5530001\n",
      "Train_AverageReturn : 1759.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 34991.163694143295\n",
      "Training Loss : 2.2376813888549805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5540001\n",
      "mean reward (100 episodes) 1773.700000\n",
      "best mean reward 1886.900000\n",
      "running time 35054.891630\n",
      "Train_EnvstepsSoFar : 5540001\n",
      "Train_AverageReturn : 1773.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35054.89163041115\n",
      "Training Loss : 2.1013131141662598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5550001\n",
      "mean reward (100 episodes) 1707.200000\n",
      "best mean reward 1886.900000\n",
      "running time 35117.961293\n",
      "Train_EnvstepsSoFar : 5550001\n",
      "Train_AverageReturn : 1707.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35117.96129274368\n",
      "Training Loss : 0.6849844455718994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5560001\n",
      "mean reward (100 episodes) 1669.700000\n",
      "best mean reward 1886.900000\n",
      "running time 35181.262870\n",
      "Train_EnvstepsSoFar : 5560001\n",
      "Train_AverageReturn : 1669.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35181.26287031174\n",
      "Training Loss : 1.2642898559570312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5570001\n",
      "mean reward (100 episodes) 1725.800000\n",
      "best mean reward 1886.900000\n",
      "running time 35245.061248\n",
      "Train_EnvstepsSoFar : 5570001\n",
      "Train_AverageReturn : 1725.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35245.06124830246\n",
      "Training Loss : 1.4751571416854858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5580001\n",
      "mean reward (100 episodes) 1765.800000\n",
      "best mean reward 1886.900000\n",
      "running time 35307.710397\n",
      "Train_EnvstepsSoFar : 5580001\n",
      "Train_AverageReturn : 1765.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35307.71039700508\n",
      "Training Loss : 1.1553843021392822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5590001\n",
      "mean reward (100 episodes) 1799.600000\n",
      "best mean reward 1886.900000\n",
      "running time 35371.097475\n",
      "Train_EnvstepsSoFar : 5590001\n",
      "Train_AverageReturn : 1799.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35371.0974752903\n",
      "Training Loss : 1.0784218311309814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5600001\n",
      "mean reward (100 episodes) 1776.000000\n",
      "best mean reward 1886.900000\n",
      "running time 35434.284133\n",
      "Train_EnvstepsSoFar : 5600001\n",
      "Train_AverageReturn : 1776.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35434.28413295746\n",
      "Training Loss : 0.173658087849617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5610001\n",
      "mean reward (100 episodes) 1751.200000\n",
      "best mean reward 1886.900000\n",
      "running time 35498.562373\n",
      "Train_EnvstepsSoFar : 5610001\n",
      "Train_AverageReturn : 1751.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35498.56237268448\n",
      "Training Loss : 0.9457545280456543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5620001\n",
      "mean reward (100 episodes) 1708.400000\n",
      "best mean reward 1886.900000\n",
      "running time 35561.798289\n",
      "Train_EnvstepsSoFar : 5620001\n",
      "Train_AverageReturn : 1708.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35561.79828929901\n",
      "Training Loss : 0.26486027240753174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5630001\n",
      "mean reward (100 episodes) 1697.600000\n",
      "best mean reward 1886.900000\n",
      "running time 35624.719644\n",
      "Train_EnvstepsSoFar : 5630001\n",
      "Train_AverageReturn : 1697.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35624.71964383125\n",
      "Training Loss : 1.2081501483917236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5640001\n",
      "mean reward (100 episodes) 1715.700000\n",
      "best mean reward 1886.900000\n",
      "running time 35687.611060\n",
      "Train_EnvstepsSoFar : 5640001\n",
      "Train_AverageReturn : 1715.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35687.61106014252\n",
      "Training Loss : 1.4803199768066406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5650001\n",
      "mean reward (100 episodes) 1791.700000\n",
      "best mean reward 1886.900000\n",
      "running time 35750.782818\n",
      "Train_EnvstepsSoFar : 5650001\n",
      "Train_AverageReturn : 1791.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35750.782818078995\n",
      "Training Loss : 0.16801001131534576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5660001\n",
      "mean reward (100 episodes) 1767.100000\n",
      "best mean reward 1886.900000\n",
      "running time 35814.333513\n",
      "Train_EnvstepsSoFar : 5660001\n",
      "Train_AverageReturn : 1767.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35814.33351254463\n",
      "Training Loss : 0.6997351050376892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5670001\n",
      "mean reward (100 episodes) 1760.400000\n",
      "best mean reward 1886.900000\n",
      "running time 35877.318861\n",
      "Train_EnvstepsSoFar : 5670001\n",
      "Train_AverageReturn : 1760.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35877.31886148453\n",
      "Training Loss : 0.14772723615169525\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5680001\n",
      "mean reward (100 episodes) 1704.900000\n",
      "best mean reward 1886.900000\n",
      "running time 35940.713134\n",
      "Train_EnvstepsSoFar : 5680001\n",
      "Train_AverageReturn : 1704.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 35940.71313381195\n",
      "Training Loss : 0.6000896692276001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5690001\n",
      "mean reward (100 episodes) 1756.000000\n",
      "best mean reward 1886.900000\n",
      "running time 36003.862973\n",
      "Train_EnvstepsSoFar : 5690001\n",
      "Train_AverageReturn : 1756.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36003.862973451614\n",
      "Training Loss : 0.2385396659374237\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5700001\n",
      "mean reward (100 episodes) 1738.200000\n",
      "best mean reward 1886.900000\n",
      "running time 36067.331503\n",
      "Train_EnvstepsSoFar : 5700001\n",
      "Train_AverageReturn : 1738.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36067.331503391266\n",
      "Training Loss : 1.1750794649124146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5710001\n",
      "mean reward (100 episodes) 1717.900000\n",
      "best mean reward 1886.900000\n",
      "running time 36130.782130\n",
      "Train_EnvstepsSoFar : 5710001\n",
      "Train_AverageReturn : 1717.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36130.782130241394\n",
      "Training Loss : 0.21355897188186646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5720001\n",
      "mean reward (100 episodes) 1670.700000\n",
      "best mean reward 1886.900000\n",
      "running time 36193.816644\n",
      "Train_EnvstepsSoFar : 5720001\n",
      "Train_AverageReturn : 1670.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36193.81664443016\n",
      "Training Loss : 0.21187834441661835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5730001\n",
      "mean reward (100 episodes) 1687.700000\n",
      "best mean reward 1886.900000\n",
      "running time 36256.788280\n",
      "Train_EnvstepsSoFar : 5730001\n",
      "Train_AverageReturn : 1687.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36256.78828024864\n",
      "Training Loss : 0.21371331810951233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5740001\n",
      "mean reward (100 episodes) 1693.600000\n",
      "best mean reward 1886.900000\n",
      "running time 36320.395154\n",
      "Train_EnvstepsSoFar : 5740001\n",
      "Train_AverageReturn : 1693.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36320.39515399933\n",
      "Training Loss : 0.5914115905761719\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5750001\n",
      "mean reward (100 episodes) 1690.600000\n",
      "best mean reward 1886.900000\n",
      "running time 36383.519281\n",
      "Train_EnvstepsSoFar : 5750001\n",
      "Train_AverageReturn : 1690.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36383.51928067207\n",
      "Training Loss : 0.27384305000305176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5760001\n",
      "mean reward (100 episodes) 1672.100000\n",
      "best mean reward 1886.900000\n",
      "running time 36446.859628\n",
      "Train_EnvstepsSoFar : 5760001\n",
      "Train_AverageReturn : 1672.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36446.85962820053\n",
      "Training Loss : 2.0418736934661865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5770001\n",
      "mean reward (100 episodes) 1684.700000\n",
      "best mean reward 1886.900000\n",
      "running time 36509.984937\n",
      "Train_EnvstepsSoFar : 5770001\n",
      "Train_AverageReturn : 1684.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36509.98493671417\n",
      "Training Loss : 0.9009168744087219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5780001\n",
      "mean reward (100 episodes) 1671.300000\n",
      "best mean reward 1886.900000\n",
      "running time 36573.237909\n",
      "Train_EnvstepsSoFar : 5780001\n",
      "Train_AverageReturn : 1671.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36573.2379090786\n",
      "Training Loss : 0.30326008796691895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5790001\n",
      "mean reward (100 episodes) 1640.700000\n",
      "best mean reward 1886.900000\n",
      "running time 36637.274345\n",
      "Train_EnvstepsSoFar : 5790001\n",
      "Train_AverageReturn : 1640.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36637.27434492111\n",
      "Training Loss : 0.23821452260017395\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5800001\n",
      "mean reward (100 episodes) 1610.000000\n",
      "best mean reward 1886.900000\n",
      "running time 36700.563491\n",
      "Train_EnvstepsSoFar : 5800001\n",
      "Train_AverageReturn : 1610.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36700.56349134445\n",
      "Training Loss : 0.23200780153274536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5810001\n",
      "mean reward (100 episodes) 1682.300000\n",
      "best mean reward 1886.900000\n",
      "running time 36763.439253\n",
      "Train_EnvstepsSoFar : 5810001\n",
      "Train_AverageReturn : 1682.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36763.439252614975\n",
      "Training Loss : 0.6354221701622009\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5820001\n",
      "mean reward (100 episodes) 1750.200000\n",
      "best mean reward 1886.900000\n",
      "running time 36826.649031\n",
      "Train_EnvstepsSoFar : 5820001\n",
      "Train_AverageReturn : 1750.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36826.64903116226\n",
      "Training Loss : 0.14798606932163239\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5830001\n",
      "mean reward (100 episodes) 1691.800000\n",
      "best mean reward 1886.900000\n",
      "running time 36890.785812\n",
      "Train_EnvstepsSoFar : 5830001\n",
      "Train_AverageReturn : 1691.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36890.785811662674\n",
      "Training Loss : 1.396720290184021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5840001\n",
      "mean reward (100 episodes) 1667.300000\n",
      "best mean reward 1886.900000\n",
      "running time 36953.741390\n",
      "Train_EnvstepsSoFar : 5840001\n",
      "Train_AverageReturn : 1667.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 36953.741389513016\n",
      "Training Loss : 0.9298774003982544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5850001\n",
      "mean reward (100 episodes) 1598.700000\n",
      "best mean reward 1886.900000\n",
      "running time 37017.552699\n",
      "Train_EnvstepsSoFar : 5850001\n",
      "Train_AverageReturn : 1598.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37017.55269932747\n",
      "Training Loss : 0.21472027897834778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5860001\n",
      "mean reward (100 episodes) 1614.300000\n",
      "best mean reward 1886.900000\n",
      "running time 37080.686277\n",
      "Train_EnvstepsSoFar : 5860001\n",
      "Train_AverageReturn : 1614.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37080.68627667427\n",
      "Training Loss : 0.17204107344150543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5870001\n",
      "mean reward (100 episodes) 1626.100000\n",
      "best mean reward 1886.900000\n",
      "running time 37143.959370\n",
      "Train_EnvstepsSoFar : 5870001\n",
      "Train_AverageReturn : 1626.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37143.95937037468\n",
      "Training Loss : 0.7204034328460693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5880001\n",
      "mean reward (100 episodes) 1676.300000\n",
      "best mean reward 1886.900000\n",
      "running time 37207.418967\n",
      "Train_EnvstepsSoFar : 5880001\n",
      "Train_AverageReturn : 1676.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37207.41896724701\n",
      "Training Loss : 0.2845020890235901\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5890001\n",
      "mean reward (100 episodes) 1685.100000\n",
      "best mean reward 1886.900000\n",
      "running time 37270.084384\n",
      "Train_EnvstepsSoFar : 5890001\n",
      "Train_AverageReturn : 1685.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37270.08438420296\n",
      "Training Loss : 2.693192958831787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5900001\n",
      "mean reward (100 episodes) 1638.400000\n",
      "best mean reward 1886.900000\n",
      "running time 37333.291454\n",
      "Train_EnvstepsSoFar : 5900001\n",
      "Train_AverageReturn : 1638.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37333.29145407677\n",
      "Training Loss : 0.1853407323360443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5910001\n",
      "mean reward (100 episodes) 1626.200000\n",
      "best mean reward 1886.900000\n",
      "running time 37396.678944\n",
      "Train_EnvstepsSoFar : 5910001\n",
      "Train_AverageReturn : 1626.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37396.67894387245\n",
      "Training Loss : 0.1612963080406189\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5920001\n",
      "mean reward (100 episodes) 1622.900000\n",
      "best mean reward 1886.900000\n",
      "running time 37459.760548\n",
      "Train_EnvstepsSoFar : 5920001\n",
      "Train_AverageReturn : 1622.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37459.760548353195\n",
      "Training Loss : 0.8408149480819702\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5930001\n",
      "mean reward (100 episodes) 1654.300000\n",
      "best mean reward 1886.900000\n",
      "running time 37523.036747\n",
      "Train_EnvstepsSoFar : 5930001\n",
      "Train_AverageReturn : 1654.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37523.03674721718\n",
      "Training Loss : 0.2222631424665451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5940001\n",
      "mean reward (100 episodes) 1657.500000\n",
      "best mean reward 1886.900000\n",
      "running time 37585.933499\n",
      "Train_EnvstepsSoFar : 5940001\n",
      "Train_AverageReturn : 1657.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37585.93349933624\n",
      "Training Loss : 0.6292542219161987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5950001\n",
      "mean reward (100 episodes) 1700.300000\n",
      "best mean reward 1886.900000\n",
      "running time 37649.162278\n",
      "Train_EnvstepsSoFar : 5950001\n",
      "Train_AverageReturn : 1700.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37649.162278175354\n",
      "Training Loss : 0.20443269610404968\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5960001\n",
      "mean reward (100 episodes) 1783.400000\n",
      "best mean reward 1886.900000\n",
      "running time 37712.721890\n",
      "Train_EnvstepsSoFar : 5960001\n",
      "Train_AverageReturn : 1783.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37712.72188973427\n",
      "Training Loss : 0.4004574418067932\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5970001\n",
      "mean reward (100 episodes) 1868.300000\n",
      "best mean reward 1886.900000\n",
      "running time 37775.587968\n",
      "Train_EnvstepsSoFar : 5970001\n",
      "Train_AverageReturn : 1868.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37775.5879676342\n",
      "Training Loss : 0.32666072249412537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5980001\n",
      "mean reward (100 episodes) 1746.900000\n",
      "best mean reward 1886.900000\n",
      "running time 37838.656700\n",
      "Train_EnvstepsSoFar : 5980001\n",
      "Train_AverageReturn : 1746.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37838.65670013428\n",
      "Training Loss : 0.8676637411117554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 5990001\n",
      "mean reward (100 episodes) 1747.200000\n",
      "best mean reward 1886.900000\n",
      "running time 37901.724336\n",
      "Train_EnvstepsSoFar : 5990001\n",
      "Train_AverageReturn : 1747.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37901.72433614731\n",
      "Training Loss : 1.4304630756378174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6000001\n",
      "mean reward (100 episodes) 1737.400000\n",
      "best mean reward 1886.900000\n",
      "running time 37964.836270\n",
      "Train_EnvstepsSoFar : 6000001\n",
      "Train_AverageReturn : 1737.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 37964.83627009392\n",
      "Training Loss : 1.4859867095947266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6010001\n",
      "mean reward (100 episodes) 1682.900000\n",
      "best mean reward 1886.900000\n",
      "running time 38027.569515\n",
      "Train_EnvstepsSoFar : 6010001\n",
      "Train_AverageReturn : 1682.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38027.56951546669\n",
      "Training Loss : 1.1286669969558716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6020001\n",
      "mean reward (100 episodes) 1703.300000\n",
      "best mean reward 1886.900000\n",
      "running time 38090.756072\n",
      "Train_EnvstepsSoFar : 6020001\n",
      "Train_AverageReturn : 1703.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38090.756071805954\n",
      "Training Loss : 0.1532249003648758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6030001\n",
      "mean reward (100 episodes) 1654.000000\n",
      "best mean reward 1886.900000\n",
      "running time 38153.787755\n",
      "Train_EnvstepsSoFar : 6030001\n",
      "Train_AverageReturn : 1654.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38153.78775525093\n",
      "Training Loss : 0.7400398850440979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6040001\n",
      "mean reward (100 episodes) 1648.400000\n",
      "best mean reward 1886.900000\n",
      "running time 38216.529365\n",
      "Train_EnvstepsSoFar : 6040001\n",
      "Train_AverageReturn : 1648.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38216.52936458588\n",
      "Training Loss : 0.83057701587677\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6050001\n",
      "mean reward (100 episodes) 1695.800000\n",
      "best mean reward 1886.900000\n",
      "running time 38279.642112\n",
      "Train_EnvstepsSoFar : 6050001\n",
      "Train_AverageReturn : 1695.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38279.64211177826\n",
      "Training Loss : 2.0075674057006836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6060001\n",
      "mean reward (100 episodes) 1682.900000\n",
      "best mean reward 1886.900000\n",
      "running time 38343.011020\n",
      "Train_EnvstepsSoFar : 6060001\n",
      "Train_AverageReturn : 1682.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38343.011019706726\n",
      "Training Loss : 0.7056200504302979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6070001\n",
      "mean reward (100 episodes) 1709.700000\n",
      "best mean reward 1886.900000\n",
      "running time 38405.864985\n",
      "Train_EnvstepsSoFar : 6070001\n",
      "Train_AverageReturn : 1709.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38405.864985227585\n",
      "Training Loss : 0.2280511111021042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6080001\n",
      "mean reward (100 episodes) 1651.300000\n",
      "best mean reward 1886.900000\n",
      "running time 38468.922865\n",
      "Train_EnvstepsSoFar : 6080001\n",
      "Train_AverageReturn : 1651.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38468.92286515236\n",
      "Training Loss : 0.8984344601631165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6090001\n",
      "mean reward (100 episodes) 1681.400000\n",
      "best mean reward 1886.900000\n",
      "running time 38531.755015\n",
      "Train_EnvstepsSoFar : 6090001\n",
      "Train_AverageReturn : 1681.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38531.75501489639\n",
      "Training Loss : 2.903916835784912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6100001\n",
      "mean reward (100 episodes) 1682.400000\n",
      "best mean reward 1886.900000\n",
      "running time 38594.682054\n",
      "Train_EnvstepsSoFar : 6100001\n",
      "Train_AverageReturn : 1682.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38594.6820538044\n",
      "Training Loss : 0.19540464878082275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6110001\n",
      "mean reward (100 episodes) 1746.600000\n",
      "best mean reward 1886.900000\n",
      "running time 38657.809973\n",
      "Train_EnvstepsSoFar : 6110001\n",
      "Train_AverageReturn : 1746.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38657.80997347832\n",
      "Training Loss : 0.7009596824645996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6120001\n",
      "mean reward (100 episodes) 1717.800000\n",
      "best mean reward 1886.900000\n",
      "running time 38722.002818\n",
      "Train_EnvstepsSoFar : 6120001\n",
      "Train_AverageReturn : 1717.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38722.002818107605\n",
      "Training Loss : 0.1292065680027008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6130001\n",
      "mean reward (100 episodes) 1748.800000\n",
      "best mean reward 1886.900000\n",
      "running time 38784.868094\n",
      "Train_EnvstepsSoFar : 6130001\n",
      "Train_AverageReturn : 1748.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38784.868094444275\n",
      "Training Loss : 0.19757726788520813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6140001\n",
      "mean reward (100 episodes) 1788.800000\n",
      "best mean reward 1886.900000\n",
      "running time 38848.400848\n",
      "Train_EnvstepsSoFar : 6140001\n",
      "Train_AverageReturn : 1788.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38848.400847911835\n",
      "Training Loss : 0.1535862386226654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6150001\n",
      "mean reward (100 episodes) 1817.100000\n",
      "best mean reward 1886.900000\n",
      "running time 38911.097594\n",
      "Train_EnvstepsSoFar : 6150001\n",
      "Train_AverageReturn : 1817.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38911.09759426117\n",
      "Training Loss : 0.5355736017227173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6160001\n",
      "mean reward (100 episodes) 1770.900000\n",
      "best mean reward 1886.900000\n",
      "running time 38973.832141\n",
      "Train_EnvstepsSoFar : 6160001\n",
      "Train_AverageReturn : 1770.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 38973.83214139938\n",
      "Training Loss : 0.5076150298118591\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6170001\n",
      "mean reward (100 episodes) 1735.200000\n",
      "best mean reward 1886.900000\n",
      "running time 39036.901334\n",
      "Train_EnvstepsSoFar : 6170001\n",
      "Train_AverageReturn : 1735.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39036.901334285736\n",
      "Training Loss : 0.32778626680374146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6180001\n",
      "mean reward (100 episodes) 1647.300000\n",
      "best mean reward 1886.900000\n",
      "running time 39099.759466\n",
      "Train_EnvstepsSoFar : 6180001\n",
      "Train_AverageReturn : 1647.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39099.759466171265\n",
      "Training Loss : 0.2202650010585785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6190001\n",
      "mean reward (100 episodes) 1642.800000\n",
      "best mean reward 1886.900000\n",
      "running time 39163.049259\n",
      "Train_EnvstepsSoFar : 6190001\n",
      "Train_AverageReturn : 1642.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39163.04925894737\n",
      "Training Loss : 0.7776851654052734\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6200001\n",
      "mean reward (100 episodes) 1593.300000\n",
      "best mean reward 1886.900000\n",
      "running time 39225.779130\n",
      "Train_EnvstepsSoFar : 6200001\n",
      "Train_AverageReturn : 1593.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39225.779129743576\n",
      "Training Loss : 0.1856432557106018\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6210001\n",
      "mean reward (100 episodes) 1567.700000\n",
      "best mean reward 1886.900000\n",
      "running time 39289.131617\n",
      "Train_EnvstepsSoFar : 6210001\n",
      "Train_AverageReturn : 1567.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39289.131617069244\n",
      "Training Loss : 0.5153499841690063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6220001\n",
      "mean reward (100 episodes) 1560.400000\n",
      "best mean reward 1886.900000\n",
      "running time 39352.146875\n",
      "Train_EnvstepsSoFar : 6220001\n",
      "Train_AverageReturn : 1560.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39352.14687538147\n",
      "Training Loss : 0.24048754572868347\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6230001\n",
      "mean reward (100 episodes) 1577.000000\n",
      "best mean reward 1886.900000\n",
      "running time 39415.217601\n",
      "Train_EnvstepsSoFar : 6230001\n",
      "Train_AverageReturn : 1577.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39415.21760082245\n",
      "Training Loss : 0.22237414121627808\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6240001\n",
      "mean reward (100 episodes) 1612.500000\n",
      "best mean reward 1886.900000\n",
      "running time 39478.093320\n",
      "Train_EnvstepsSoFar : 6240001\n",
      "Train_AverageReturn : 1612.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39478.0933201313\n",
      "Training Loss : 0.2591094970703125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6250001\n",
      "mean reward (100 episodes) 1681.600000\n",
      "best mean reward 1886.900000\n",
      "running time 39540.946447\n",
      "Train_EnvstepsSoFar : 6250001\n",
      "Train_AverageReturn : 1681.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39540.94644665718\n",
      "Training Loss : 1.52827787399292\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6260001\n",
      "mean reward (100 episodes) 1666.900000\n",
      "best mean reward 1886.900000\n",
      "running time 39603.684317\n",
      "Train_EnvstepsSoFar : 6260001\n",
      "Train_AverageReturn : 1666.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39603.68431735039\n",
      "Training Loss : 1.391401767730713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6270001\n",
      "mean reward (100 episodes) 1653.400000\n",
      "best mean reward 1886.900000\n",
      "running time 39666.468967\n",
      "Train_EnvstepsSoFar : 6270001\n",
      "Train_AverageReturn : 1653.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39666.468967199326\n",
      "Training Loss : 0.13984766602516174\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6280001\n",
      "mean reward (100 episodes) 1636.300000\n",
      "best mean reward 1886.900000\n",
      "running time 39729.374959\n",
      "Train_EnvstepsSoFar : 6280001\n",
      "Train_AverageReturn : 1636.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39729.37495851517\n",
      "Training Loss : 0.16979080438613892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6290001\n",
      "mean reward (100 episodes) 1610.900000\n",
      "best mean reward 1886.900000\n",
      "running time 39792.222630\n",
      "Train_EnvstepsSoFar : 6290001\n",
      "Train_AverageReturn : 1610.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39792.222630023956\n",
      "Training Loss : 1.5657641887664795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6300001\n",
      "mean reward (100 episodes) 1757.000000\n",
      "best mean reward 1886.900000\n",
      "running time 39854.821644\n",
      "Train_EnvstepsSoFar : 6300001\n",
      "Train_AverageReturn : 1757.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39854.821644067764\n",
      "Training Loss : 0.25739365816116333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6310001\n",
      "mean reward (100 episodes) 1766.400000\n",
      "best mean reward 1886.900000\n",
      "running time 39917.943174\n",
      "Train_EnvstepsSoFar : 6310001\n",
      "Train_AverageReturn : 1766.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39917.94317436218\n",
      "Training Loss : 0.30822718143463135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6320001\n",
      "mean reward (100 episodes) 1807.300000\n",
      "best mean reward 1886.900000\n",
      "running time 39980.728447\n",
      "Train_EnvstepsSoFar : 6320001\n",
      "Train_AverageReturn : 1807.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 39980.72844743729\n",
      "Training Loss : 0.389349102973938\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6330001\n",
      "mean reward (100 episodes) 1657.600000\n",
      "best mean reward 1886.900000\n",
      "running time 40043.568605\n",
      "Train_EnvstepsSoFar : 6330001\n",
      "Train_AverageReturn : 1657.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40043.56860494614\n",
      "Training Loss : 1.243727445602417\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6340001\n",
      "mean reward (100 episodes) 1705.000000\n",
      "best mean reward 1886.900000\n",
      "running time 40106.274466\n",
      "Train_EnvstepsSoFar : 6340001\n",
      "Train_AverageReturn : 1705.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40106.27446603775\n",
      "Training Loss : 1.5582301616668701\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6350001\n",
      "mean reward (100 episodes) 1715.500000\n",
      "best mean reward 1886.900000\n",
      "running time 40169.112468\n",
      "Train_EnvstepsSoFar : 6350001\n",
      "Train_AverageReturn : 1715.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40169.11246752739\n",
      "Training Loss : 0.7922191619873047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6360001\n",
      "mean reward (100 episodes) 1772.100000\n",
      "best mean reward 1886.900000\n",
      "running time 40232.897214\n",
      "Train_EnvstepsSoFar : 6360001\n",
      "Train_AverageReturn : 1772.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40232.89721441269\n",
      "Training Loss : 0.21522238850593567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6370001\n",
      "mean reward (100 episodes) 1708.900000\n",
      "best mean reward 1886.900000\n",
      "running time 40295.807943\n",
      "Train_EnvstepsSoFar : 6370001\n",
      "Train_AverageReturn : 1708.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40295.807943344116\n",
      "Training Loss : 0.2638733386993408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6380001\n",
      "mean reward (100 episodes) 1745.100000\n",
      "best mean reward 1886.900000\n",
      "running time 40358.801408\n",
      "Train_EnvstepsSoFar : 6380001\n",
      "Train_AverageReturn : 1745.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40358.801407814026\n",
      "Training Loss : 0.1474892944097519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6390001\n",
      "mean reward (100 episodes) 1670.200000\n",
      "best mean reward 1886.900000\n",
      "running time 40421.753190\n",
      "Train_EnvstepsSoFar : 6390001\n",
      "Train_AverageReturn : 1670.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40421.75318956375\n",
      "Training Loss : 1.8809994459152222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6400001\n",
      "mean reward (100 episodes) 1663.400000\n",
      "best mean reward 1886.900000\n",
      "running time 40484.703736\n",
      "Train_EnvstepsSoFar : 6400001\n",
      "Train_AverageReturn : 1663.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40484.70373606682\n",
      "Training Loss : 0.9677699208259583\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6410001\n",
      "mean reward (100 episodes) 1599.700000\n",
      "best mean reward 1886.900000\n",
      "running time 40548.471565\n",
      "Train_EnvstepsSoFar : 6410001\n",
      "Train_AverageReturn : 1599.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40548.471564769745\n",
      "Training Loss : 0.3634123206138611\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6420001\n",
      "mean reward (100 episodes) 1657.200000\n",
      "best mean reward 1886.900000\n",
      "running time 40611.266638\n",
      "Train_EnvstepsSoFar : 6420001\n",
      "Train_AverageReturn : 1657.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40611.266637802124\n",
      "Training Loss : 0.8266408443450928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6430001\n",
      "mean reward (100 episodes) 1616.200000\n",
      "best mean reward 1886.900000\n",
      "running time 40673.954998\n",
      "Train_EnvstepsSoFar : 6430001\n",
      "Train_AverageReturn : 1616.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40673.95499801636\n",
      "Training Loss : 0.587598979473114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6440001\n",
      "mean reward (100 episodes) 1658.100000\n",
      "best mean reward 1886.900000\n",
      "running time 40736.680542\n",
      "Train_EnvstepsSoFar : 6440001\n",
      "Train_AverageReturn : 1658.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40736.68054175377\n",
      "Training Loss : 0.25510460138320923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6450001\n",
      "mean reward (100 episodes) 1621.300000\n",
      "best mean reward 1886.900000\n",
      "running time 40799.241400\n",
      "Train_EnvstepsSoFar : 6450001\n",
      "Train_AverageReturn : 1621.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40799.24140024185\n",
      "Training Loss : 0.14123839139938354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6460001\n",
      "mean reward (100 episodes) 1651.300000\n",
      "best mean reward 1886.900000\n",
      "running time 40862.629075\n",
      "Train_EnvstepsSoFar : 6460001\n",
      "Train_AverageReturn : 1651.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40862.62907457352\n",
      "Training Loss : 1.1492340564727783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6470001\n",
      "mean reward (100 episodes) 1633.800000\n",
      "best mean reward 1886.900000\n",
      "running time 40925.536009\n",
      "Train_EnvstepsSoFar : 6470001\n",
      "Train_AverageReturn : 1633.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40925.536009311676\n",
      "Training Loss : 0.624031662940979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6480001\n",
      "mean reward (100 episodes) 1634.500000\n",
      "best mean reward 1886.900000\n",
      "running time 40988.455306\n",
      "Train_EnvstepsSoFar : 6480001\n",
      "Train_AverageReturn : 1634.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 40988.45530629158\n",
      "Training Loss : 0.15483592450618744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6490001\n",
      "mean reward (100 episodes) 1652.900000\n",
      "best mean reward 1886.900000\n",
      "running time 41051.685614\n",
      "Train_EnvstepsSoFar : 6490001\n",
      "Train_AverageReturn : 1652.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41051.6856136322\n",
      "Training Loss : 0.1647256463766098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6500001\n",
      "mean reward (100 episodes) 1630.000000\n",
      "best mean reward 1886.900000\n",
      "running time 41114.380998\n",
      "Train_EnvstepsSoFar : 6500001\n",
      "Train_AverageReturn : 1630.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41114.38099813461\n",
      "Training Loss : 0.1639816015958786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6510001\n",
      "mean reward (100 episodes) 1625.200000\n",
      "best mean reward 1886.900000\n",
      "running time 41177.358169\n",
      "Train_EnvstepsSoFar : 6510001\n",
      "Train_AverageReturn : 1625.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41177.358169317245\n",
      "Training Loss : 0.14875447750091553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6520001\n",
      "mean reward (100 episodes) 1641.800000\n",
      "best mean reward 1886.900000\n",
      "running time 41240.369798\n",
      "Train_EnvstepsSoFar : 6520001\n",
      "Train_AverageReturn : 1641.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41240.36979794502\n",
      "Training Loss : 0.2732292413711548\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6530001\n",
      "mean reward (100 episodes) 1648.000000\n",
      "best mean reward 1886.900000\n",
      "running time 41303.167942\n",
      "Train_EnvstepsSoFar : 6530001\n",
      "Train_AverageReturn : 1648.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41303.1679418087\n",
      "Training Loss : 0.48614802956581116\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6540001\n",
      "mean reward (100 episodes) 1620.900000\n",
      "best mean reward 1886.900000\n",
      "running time 41366.273301\n",
      "Train_EnvstepsSoFar : 6540001\n",
      "Train_AverageReturn : 1620.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41366.27330112457\n",
      "Training Loss : 0.17280925810337067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6550001\n",
      "mean reward (100 episodes) 1626.000000\n",
      "best mean reward 1886.900000\n",
      "running time 41429.173499\n",
      "Train_EnvstepsSoFar : 6550001\n",
      "Train_AverageReturn : 1626.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41429.173498630524\n",
      "Training Loss : 1.4232394695281982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6560001\n",
      "mean reward (100 episodes) 1668.100000\n",
      "best mean reward 1886.900000\n",
      "running time 41491.905286\n",
      "Train_EnvstepsSoFar : 6560001\n",
      "Train_AverageReturn : 1668.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41491.90528559685\n",
      "Training Loss : 1.2191684246063232\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6570001\n",
      "mean reward (100 episodes) 1649.700000\n",
      "best mean reward 1886.900000\n",
      "running time 41555.787893\n",
      "Train_EnvstepsSoFar : 6570001\n",
      "Train_AverageReturn : 1649.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41555.78789281845\n",
      "Training Loss : 0.1477477252483368\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6580001\n",
      "mean reward (100 episodes) 1584.000000\n",
      "best mean reward 1886.900000\n",
      "running time 41618.717354\n",
      "Train_EnvstepsSoFar : 6580001\n",
      "Train_AverageReturn : 1584.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41618.71735429764\n",
      "Training Loss : 0.16189312934875488\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6590001\n",
      "mean reward (100 episodes) 1628.500000\n",
      "best mean reward 1886.900000\n",
      "running time 41681.949760\n",
      "Train_EnvstepsSoFar : 6590001\n",
      "Train_AverageReturn : 1628.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41681.94976043701\n",
      "Training Loss : 1.4550977945327759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6600001\n",
      "mean reward (100 episodes) 1613.400000\n",
      "best mean reward 1886.900000\n",
      "running time 41747.799830\n",
      "Train_EnvstepsSoFar : 6600001\n",
      "Train_AverageReturn : 1613.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41747.79983019829\n",
      "Training Loss : 0.6871263980865479\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6610001\n",
      "mean reward (100 episodes) 1627.400000\n",
      "best mean reward 1886.900000\n",
      "running time 41841.233908\n",
      "Train_EnvstepsSoFar : 6610001\n",
      "Train_AverageReturn : 1627.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41841.233907699585\n",
      "Training Loss : 0.21790838241577148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6620001\n",
      "mean reward (100 episodes) 1610.700000\n",
      "best mean reward 1886.900000\n",
      "running time 41916.943729\n",
      "Train_EnvstepsSoFar : 6620001\n",
      "Train_AverageReturn : 1610.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41916.94372868538\n",
      "Training Loss : 0.9296640157699585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6630001\n",
      "mean reward (100 episodes) 1637.800000\n",
      "best mean reward 1886.900000\n",
      "running time 41980.704986\n",
      "Train_EnvstepsSoFar : 6630001\n",
      "Train_AverageReturn : 1637.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 41980.70498585701\n",
      "Training Loss : 2.4228878021240234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6640001\n",
      "mean reward (100 episodes) 1674.800000\n",
      "best mean reward 1886.900000\n",
      "running time 42053.295372\n",
      "Train_EnvstepsSoFar : 6640001\n",
      "Train_AverageReturn : 1674.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42053.29537177086\n",
      "Training Loss : 0.2800542712211609\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6650001\n",
      "mean reward (100 episodes) 1685.000000\n",
      "best mean reward 1886.900000\n",
      "running time 42118.376018\n",
      "Train_EnvstepsSoFar : 6650001\n",
      "Train_AverageReturn : 1685.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42118.37601804733\n",
      "Training Loss : 0.2689175307750702\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6660001\n",
      "mean reward (100 episodes) 1671.500000\n",
      "best mean reward 1886.900000\n",
      "running time 42190.568360\n",
      "Train_EnvstepsSoFar : 6660001\n",
      "Train_AverageReturn : 1671.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42190.568360328674\n",
      "Training Loss : 1.6001166105270386\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6670001\n",
      "mean reward (100 episodes) 1672.400000\n",
      "best mean reward 1886.900000\n",
      "running time 42281.549064\n",
      "Train_EnvstepsSoFar : 6670001\n",
      "Train_AverageReturn : 1672.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42281.54906439781\n",
      "Training Loss : 0.5929379463195801\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6680001\n",
      "mean reward (100 episodes) 1713.900000\n",
      "best mean reward 1886.900000\n",
      "running time 42357.930473\n",
      "Train_EnvstepsSoFar : 6680001\n",
      "Train_AverageReturn : 1713.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42357.93047261238\n",
      "Training Loss : 0.9642962217330933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6690001\n",
      "mean reward (100 episodes) 1726.500000\n",
      "best mean reward 1886.900000\n",
      "running time 42454.752267\n",
      "Train_EnvstepsSoFar : 6690001\n",
      "Train_AverageReturn : 1726.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42454.75226688385\n",
      "Training Loss : 0.4876445233821869\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6700001\n",
      "mean reward (100 episodes) 1693.300000\n",
      "best mean reward 1886.900000\n",
      "running time 42524.019605\n",
      "Train_EnvstepsSoFar : 6700001\n",
      "Train_AverageReturn : 1693.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42524.01960515976\n",
      "Training Loss : 0.20977675914764404\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6710001\n",
      "mean reward (100 episodes) 1657.300000\n",
      "best mean reward 1886.900000\n",
      "running time 42589.532027\n",
      "Train_EnvstepsSoFar : 6710001\n",
      "Train_AverageReturn : 1657.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42589.53202748299\n",
      "Training Loss : 0.2491319626569748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6720001\n",
      "mean reward (100 episodes) 1638.100000\n",
      "best mean reward 1886.900000\n",
      "running time 42653.201667\n",
      "Train_EnvstepsSoFar : 6720001\n",
      "Train_AverageReturn : 1638.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42653.20166659355\n",
      "Training Loss : 0.2151823192834854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6730001\n",
      "mean reward (100 episodes) 1620.500000\n",
      "best mean reward 1886.900000\n",
      "running time 42716.111935\n",
      "Train_EnvstepsSoFar : 6730001\n",
      "Train_AverageReturn : 1620.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42716.11193537712\n",
      "Training Loss : 0.40786173939704895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6740001\n",
      "mean reward (100 episodes) 1635.400000\n",
      "best mean reward 1886.900000\n",
      "running time 42778.171236\n",
      "Train_EnvstepsSoFar : 6740001\n",
      "Train_AverageReturn : 1635.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42778.17123579979\n",
      "Training Loss : 0.1561817228794098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6750001\n",
      "mean reward (100 episodes) 1648.200000\n",
      "best mean reward 1886.900000\n",
      "running time 42840.140874\n",
      "Train_EnvstepsSoFar : 6750001\n",
      "Train_AverageReturn : 1648.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42840.140874385834\n",
      "Training Loss : 0.5376064777374268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6760001\n",
      "mean reward (100 episodes) 1705.100000\n",
      "best mean reward 1886.900000\n",
      "running time 42902.142101\n",
      "Train_EnvstepsSoFar : 6760001\n",
      "Train_AverageReturn : 1705.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42902.142100572586\n",
      "Training Loss : 0.13637351989746094\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6770001\n",
      "mean reward (100 episodes) 1732.700000\n",
      "best mean reward 1886.900000\n",
      "running time 42966.998351\n",
      "Train_EnvstepsSoFar : 6770001\n",
      "Train_AverageReturn : 1732.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 42966.99835085869\n",
      "Training Loss : 0.6529714465141296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6780001\n",
      "mean reward (100 episodes) 1708.600000\n",
      "best mean reward 1886.900000\n",
      "running time 43033.057546\n",
      "Train_EnvstepsSoFar : 6780001\n",
      "Train_AverageReturn : 1708.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43033.05754613876\n",
      "Training Loss : 0.11070729792118073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6790001\n",
      "mean reward (100 episodes) 1662.900000\n",
      "best mean reward 1886.900000\n",
      "running time 43100.503861\n",
      "Train_EnvstepsSoFar : 6790001\n",
      "Train_AverageReturn : 1662.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43100.50386142731\n",
      "Training Loss : 0.12120413780212402\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6800001\n",
      "mean reward (100 episodes) 1644.300000\n",
      "best mean reward 1886.900000\n",
      "running time 43168.934389\n",
      "Train_EnvstepsSoFar : 6800001\n",
      "Train_AverageReturn : 1644.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43168.93438887596\n",
      "Training Loss : 0.29393163323402405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6810001\n",
      "mean reward (100 episodes) 1645.200000\n",
      "best mean reward 1886.900000\n",
      "running time 43236.763173\n",
      "Train_EnvstepsSoFar : 6810001\n",
      "Train_AverageReturn : 1645.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43236.76317310333\n",
      "Training Loss : 0.34031742811203003\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6820001\n",
      "mean reward (100 episodes) 1701.300000\n",
      "best mean reward 1886.900000\n",
      "running time 43306.024052\n",
      "Train_EnvstepsSoFar : 6820001\n",
      "Train_AverageReturn : 1701.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43306.02405190468\n",
      "Training Loss : 0.8193727731704712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6830001\n",
      "mean reward (100 episodes) 1728.000000\n",
      "best mean reward 1886.900000\n",
      "running time 43373.109781\n",
      "Train_EnvstepsSoFar : 6830001\n",
      "Train_AverageReturn : 1728.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43373.10978102684\n",
      "Training Loss : 1.6144238710403442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6840001\n",
      "mean reward (100 episodes) 1792.100000\n",
      "best mean reward 1886.900000\n",
      "running time 43441.158897\n",
      "Train_EnvstepsSoFar : 6840001\n",
      "Train_AverageReturn : 1792.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43441.158897161484\n",
      "Training Loss : 0.16044999659061432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6850001\n",
      "mean reward (100 episodes) 1740.900000\n",
      "best mean reward 1886.900000\n",
      "running time 43509.859887\n",
      "Train_EnvstepsSoFar : 6850001\n",
      "Train_AverageReturn : 1740.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43509.85988664627\n",
      "Training Loss : 0.15577158331871033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6860001\n",
      "mean reward (100 episodes) 1729.600000\n",
      "best mean reward 1886.900000\n",
      "running time 43577.524368\n",
      "Train_EnvstepsSoFar : 6860001\n",
      "Train_AverageReturn : 1729.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43577.524367809296\n",
      "Training Loss : 1.7688226699829102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6870001\n",
      "mean reward (100 episodes) 1740.400000\n",
      "best mean reward 1886.900000\n",
      "running time 43647.160222\n",
      "Train_EnvstepsSoFar : 6870001\n",
      "Train_AverageReturn : 1740.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43647.16022229195\n",
      "Training Loss : 0.15773171186447144\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6880001\n",
      "mean reward (100 episodes) 1759.000000\n",
      "best mean reward 1886.900000\n",
      "running time 43715.659744\n",
      "Train_EnvstepsSoFar : 6880001\n",
      "Train_AverageReturn : 1759.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43715.659744262695\n",
      "Training Loss : 0.2675759792327881\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6890001\n",
      "mean reward (100 episodes) 1786.200000\n",
      "best mean reward 1886.900000\n",
      "running time 43785.159887\n",
      "Train_EnvstepsSoFar : 6890001\n",
      "Train_AverageReturn : 1786.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43785.159887075424\n",
      "Training Loss : 0.1112947016954422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6900001\n",
      "mean reward (100 episodes) 1796.600000\n",
      "best mean reward 1886.900000\n",
      "running time 43853.294580\n",
      "Train_EnvstepsSoFar : 6900001\n",
      "Train_AverageReturn : 1796.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43853.29457974434\n",
      "Training Loss : 0.07981794327497482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6910001\n",
      "mean reward (100 episodes) 1794.800000\n",
      "best mean reward 1886.900000\n",
      "running time 43922.730186\n",
      "Train_EnvstepsSoFar : 6910001\n",
      "Train_AverageReturn : 1794.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43922.73018550873\n",
      "Training Loss : 0.1554141342639923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6920001\n",
      "mean reward (100 episodes) 1749.800000\n",
      "best mean reward 1886.900000\n",
      "running time 43992.274704\n",
      "Train_EnvstepsSoFar : 6920001\n",
      "Train_AverageReturn : 1749.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 43992.27470421791\n",
      "Training Loss : 0.3139757513999939\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6930001\n",
      "mean reward (100 episodes) 1656.300000\n",
      "best mean reward 1886.900000\n",
      "running time 44058.792634\n",
      "Train_EnvstepsSoFar : 6930001\n",
      "Train_AverageReturn : 1656.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44058.79263353348\n",
      "Training Loss : 0.173251211643219\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6940001\n",
      "mean reward (100 episodes) 1677.900000\n",
      "best mean reward 1886.900000\n",
      "running time 44122.689970\n",
      "Train_EnvstepsSoFar : 6940001\n",
      "Train_AverageReturn : 1677.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44122.68996953964\n",
      "Training Loss : 0.18502870202064514\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6950001\n",
      "mean reward (100 episodes) 1665.900000\n",
      "best mean reward 1886.900000\n",
      "running time 44189.542548\n",
      "Train_EnvstepsSoFar : 6950001\n",
      "Train_AverageReturn : 1665.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44189.54254770279\n",
      "Training Loss : 0.2700327932834625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6960001\n",
      "mean reward (100 episodes) 1693.700000\n",
      "best mean reward 1886.900000\n",
      "running time 44259.990570\n",
      "Train_EnvstepsSoFar : 6960001\n",
      "Train_AverageReturn : 1693.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44259.99056982994\n",
      "Training Loss : 0.190509632229805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6970001\n",
      "mean reward (100 episodes) 1755.700000\n",
      "best mean reward 1886.900000\n",
      "running time 44326.801029\n",
      "Train_EnvstepsSoFar : 6970001\n",
      "Train_AverageReturn : 1755.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44326.80102944374\n",
      "Training Loss : 0.17622974514961243\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6980001\n",
      "mean reward (100 episodes) 1720.800000\n",
      "best mean reward 1886.900000\n",
      "running time 44393.855592\n",
      "Train_EnvstepsSoFar : 6980001\n",
      "Train_AverageReturn : 1720.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44393.855592012405\n",
      "Training Loss : 0.12728488445281982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 6990001\n",
      "mean reward (100 episodes) 1721.600000\n",
      "best mean reward 1886.900000\n",
      "running time 44463.211709\n",
      "Train_EnvstepsSoFar : 6990001\n",
      "Train_AverageReturn : 1721.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44463.211708545685\n",
      "Training Loss : 1.3319686651229858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7000001\n",
      "mean reward (100 episodes) 1676.000000\n",
      "best mean reward 1886.900000\n",
      "running time 44530.116867\n",
      "Train_EnvstepsSoFar : 7000001\n",
      "Train_AverageReturn : 1676.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44530.11686658859\n",
      "Training Loss : 0.16093675792217255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7010001\n",
      "mean reward (100 episodes) 1676.700000\n",
      "best mean reward 1886.900000\n",
      "running time 44594.851451\n",
      "Train_EnvstepsSoFar : 7010001\n",
      "Train_AverageReturn : 1676.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44594.85145139694\n",
      "Training Loss : 0.527597188949585\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7020001\n",
      "mean reward (100 episodes) 1698.600000\n",
      "best mean reward 1886.900000\n",
      "running time 44659.025591\n",
      "Train_EnvstepsSoFar : 7020001\n",
      "Train_AverageReturn : 1698.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44659.025591135025\n",
      "Training Loss : 0.3366314470767975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7030001\n",
      "mean reward (100 episodes) 1604.300000\n",
      "best mean reward 1886.900000\n",
      "running time 44722.560828\n",
      "Train_EnvstepsSoFar : 7030001\n",
      "Train_AverageReturn : 1604.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44722.560827732086\n",
      "Training Loss : 0.6913973689079285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7040001\n",
      "mean reward (100 episodes) 1590.200000\n",
      "best mean reward 1886.900000\n",
      "running time 44785.138873\n",
      "Train_EnvstepsSoFar : 7040001\n",
      "Train_AverageReturn : 1590.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44785.13887286186\n",
      "Training Loss : 0.2200918197631836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7050001\n",
      "mean reward (100 episodes) 1581.800000\n",
      "best mean reward 1886.900000\n",
      "running time 44848.383237\n",
      "Train_EnvstepsSoFar : 7050001\n",
      "Train_AverageReturn : 1581.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44848.38323688507\n",
      "Training Loss : 0.5509274005889893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7060001\n",
      "mean reward (100 episodes) 1648.600000\n",
      "best mean reward 1886.900000\n",
      "running time 44910.933021\n",
      "Train_EnvstepsSoFar : 7060001\n",
      "Train_AverageReturn : 1648.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44910.93302106857\n",
      "Training Loss : 2.8877999782562256\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7070001\n",
      "mean reward (100 episodes) 1646.000000\n",
      "best mean reward 1886.900000\n",
      "running time 44974.342415\n",
      "Train_EnvstepsSoFar : 7070001\n",
      "Train_AverageReturn : 1646.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 44974.34241461754\n",
      "Training Loss : 1.1149251461029053\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7080001\n",
      "mean reward (100 episodes) 1776.300000\n",
      "best mean reward 1886.900000\n",
      "running time 45037.948566\n",
      "Train_EnvstepsSoFar : 7080001\n",
      "Train_AverageReturn : 1776.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45037.94856643677\n",
      "Training Loss : 0.629446804523468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7090001\n",
      "mean reward (100 episodes) 1786.000000\n",
      "best mean reward 1886.900000\n",
      "running time 45100.913100\n",
      "Train_EnvstepsSoFar : 7090001\n",
      "Train_AverageReturn : 1786.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45100.91309976578\n",
      "Training Loss : 0.189717635512352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7100001\n",
      "mean reward (100 episodes) 1717.500000\n",
      "best mean reward 1886.900000\n",
      "running time 45163.627199\n",
      "Train_EnvstepsSoFar : 7100001\n",
      "Train_AverageReturn : 1717.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45163.62719941139\n",
      "Training Loss : 0.861177921295166\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7110001\n",
      "mean reward (100 episodes) 1703.400000\n",
      "best mean reward 1886.900000\n",
      "running time 45226.948684\n",
      "Train_EnvstepsSoFar : 7110001\n",
      "Train_AverageReturn : 1703.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45226.948684215546\n",
      "Training Loss : 0.1024341732263565\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7120001\n",
      "mean reward (100 episodes) 1671.200000\n",
      "best mean reward 1886.900000\n",
      "running time 45290.123327\n",
      "Train_EnvstepsSoFar : 7120001\n",
      "Train_AverageReturn : 1671.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45290.12332677841\n",
      "Training Loss : 0.3688476085662842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7130001\n",
      "mean reward (100 episodes) 1633.600000\n",
      "best mean reward 1886.900000\n",
      "running time 45353.574056\n",
      "Train_EnvstepsSoFar : 7130001\n",
      "Train_AverageReturn : 1633.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45353.57405591011\n",
      "Training Loss : 1.8221445083618164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7140001\n",
      "mean reward (100 episodes) 1556.900000\n",
      "best mean reward 1886.900000\n",
      "running time 45416.585310\n",
      "Train_EnvstepsSoFar : 7140001\n",
      "Train_AverageReturn : 1556.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45416.58530974388\n",
      "Training Loss : 0.13645237684249878\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7150001\n",
      "mean reward (100 episodes) 1620.900000\n",
      "best mean reward 1886.900000\n",
      "running time 45479.788040\n",
      "Train_EnvstepsSoFar : 7150001\n",
      "Train_AverageReturn : 1620.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45479.788039922714\n",
      "Training Loss : 1.762228012084961\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7160001\n",
      "mean reward (100 episodes) 1682.300000\n",
      "best mean reward 1886.900000\n",
      "running time 45542.763116\n",
      "Train_EnvstepsSoFar : 7160001\n",
      "Train_AverageReturn : 1682.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45542.76311612129\n",
      "Training Loss : 0.13714873790740967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7170001\n",
      "mean reward (100 episodes) 1685.600000\n",
      "best mean reward 1886.900000\n",
      "running time 45606.164558\n",
      "Train_EnvstepsSoFar : 7170001\n",
      "Train_AverageReturn : 1685.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45606.164558172226\n",
      "Training Loss : 0.5649248361587524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7180001\n",
      "mean reward (100 episodes) 1581.600000\n",
      "best mean reward 1886.900000\n",
      "running time 45669.545148\n",
      "Train_EnvstepsSoFar : 7180001\n",
      "Train_AverageReturn : 1581.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45669.54514837265\n",
      "Training Loss : 0.1892329454421997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7190001\n",
      "mean reward (100 episodes) 1586.100000\n",
      "best mean reward 1886.900000\n",
      "running time 45732.378590\n",
      "Train_EnvstepsSoFar : 7190001\n",
      "Train_AverageReturn : 1586.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45732.378590106964\n",
      "Training Loss : 0.15354256331920624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7200001\n",
      "mean reward (100 episodes) 1634.100000\n",
      "best mean reward 1886.900000\n",
      "running time 45796.385193\n",
      "Train_EnvstepsSoFar : 7200001\n",
      "Train_AverageReturn : 1634.1\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45796.385192871094\n",
      "Training Loss : 0.1804155707359314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7210001\n",
      "mean reward (100 episodes) 1611.300000\n",
      "best mean reward 1886.900000\n",
      "running time 45858.992041\n",
      "Train_EnvstepsSoFar : 7210001\n",
      "Train_AverageReturn : 1611.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45858.99204134941\n",
      "Training Loss : 0.17486903071403503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7220001\n",
      "mean reward (100 episodes) 1647.200000\n",
      "best mean reward 1886.900000\n",
      "running time 45921.712601\n",
      "Train_EnvstepsSoFar : 7220001\n",
      "Train_AverageReturn : 1647.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45921.71260070801\n",
      "Training Loss : 0.2076704204082489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7230001\n",
      "mean reward (100 episodes) 1666.800000\n",
      "best mean reward 1886.900000\n",
      "running time 45984.724402\n",
      "Train_EnvstepsSoFar : 7230001\n",
      "Train_AverageReturn : 1666.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 45984.724401950836\n",
      "Training Loss : 0.22425439953804016\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7240001\n",
      "mean reward (100 episodes) 1759.400000\n",
      "best mean reward 1886.900000\n",
      "running time 46047.791932\n",
      "Train_EnvstepsSoFar : 7240001\n",
      "Train_AverageReturn : 1759.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46047.7919318676\n",
      "Training Loss : 0.1529926061630249\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7250001\n",
      "mean reward (100 episodes) 1727.900000\n",
      "best mean reward 1886.900000\n",
      "running time 46110.309904\n",
      "Train_EnvstepsSoFar : 7250001\n",
      "Train_AverageReturn : 1727.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46110.30990433693\n",
      "Training Loss : 2.0682590007781982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7260001\n",
      "mean reward (100 episodes) 1739.200000\n",
      "best mean reward 1886.900000\n",
      "running time 46173.700165\n",
      "Train_EnvstepsSoFar : 7260001\n",
      "Train_AverageReturn : 1739.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46173.70016527176\n",
      "Training Loss : 0.10837095975875854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7270001\n",
      "mean reward (100 episodes) 1700.300000\n",
      "best mean reward 1886.900000\n",
      "running time 46236.474000\n",
      "Train_EnvstepsSoFar : 7270001\n",
      "Train_AverageReturn : 1700.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46236.47400021553\n",
      "Training Loss : 0.23137764632701874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7280001\n",
      "mean reward (100 episodes) 1747.500000\n",
      "best mean reward 1886.900000\n",
      "running time 46299.093014\n",
      "Train_EnvstepsSoFar : 7280001\n",
      "Train_AverageReturn : 1747.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46299.09301352501\n",
      "Training Loss : 1.2979695796966553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7290001\n",
      "mean reward (100 episodes) 1719.300000\n",
      "best mean reward 1886.900000\n",
      "running time 46361.398765\n",
      "Train_EnvstepsSoFar : 7290001\n",
      "Train_AverageReturn : 1719.3\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46361.39876461029\n",
      "Training Loss : 0.2679191827774048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7300001\n",
      "mean reward (100 episodes) 1700.600000\n",
      "best mean reward 1886.900000\n",
      "running time 46424.065802\n",
      "Train_EnvstepsSoFar : 7300001\n",
      "Train_AverageReturn : 1700.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46424.0658018589\n",
      "Training Loss : 0.6530437469482422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7310001\n",
      "mean reward (100 episodes) 1663.200000\n",
      "best mean reward 1886.900000\n",
      "running time 46486.960239\n",
      "Train_EnvstepsSoFar : 7310001\n",
      "Train_AverageReturn : 1663.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46486.96023917198\n",
      "Training Loss : 0.20993730425834656\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7320001\n",
      "mean reward (100 episodes) 1655.500000\n",
      "best mean reward 1886.900000\n",
      "running time 46549.479074\n",
      "Train_EnvstepsSoFar : 7320001\n",
      "Train_AverageReturn : 1655.5\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46549.479073762894\n",
      "Training Loss : 0.27588194608688354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7330001\n",
      "mean reward (100 episodes) 1671.600000\n",
      "best mean reward 1886.900000\n",
      "running time 46612.096765\n",
      "Train_EnvstepsSoFar : 7330001\n",
      "Train_AverageReturn : 1671.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46612.096764564514\n",
      "Training Loss : 0.1350311040878296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7340001\n",
      "mean reward (100 episodes) 1732.200000\n",
      "best mean reward 1886.900000\n",
      "running time 46674.453123\n",
      "Train_EnvstepsSoFar : 7340001\n",
      "Train_AverageReturn : 1732.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46674.45312333107\n",
      "Training Loss : 0.14489693939685822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7350001\n",
      "mean reward (100 episodes) 1744.000000\n",
      "best mean reward 1886.900000\n",
      "running time 46736.561307\n",
      "Train_EnvstepsSoFar : 7350001\n",
      "Train_AverageReturn : 1744.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46736.56130671501\n",
      "Training Loss : 0.6492716073989868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7360001\n",
      "mean reward (100 episodes) 1746.400000\n",
      "best mean reward 1886.900000\n",
      "running time 46798.753976\n",
      "Train_EnvstepsSoFar : 7360001\n",
      "Train_AverageReturn : 1746.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46798.75397562981\n",
      "Training Loss : 0.194297194480896\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7370001\n",
      "mean reward (100 episodes) 1711.800000\n",
      "best mean reward 1886.900000\n",
      "running time 46861.218083\n",
      "Train_EnvstepsSoFar : 7370001\n",
      "Train_AverageReturn : 1711.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46861.21808338165\n",
      "Training Loss : 0.1149892807006836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7380001\n",
      "mean reward (100 episodes) 1745.800000\n",
      "best mean reward 1886.900000\n",
      "running time 46924.032328\n",
      "Train_EnvstepsSoFar : 7380001\n",
      "Train_AverageReturn : 1745.8\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46924.03232765198\n",
      "Training Loss : 0.18963155150413513\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7390001\n",
      "mean reward (100 episodes) 1857.200000\n",
      "best mean reward 1886.900000\n",
      "running time 46986.969748\n",
      "Train_EnvstepsSoFar : 7390001\n",
      "Train_AverageReturn : 1857.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 46986.96974849701\n",
      "Training Loss : 0.14438359439373016\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7400001\n",
      "mean reward (100 episodes) 1834.700000\n",
      "best mean reward 1886.900000\n",
      "running time 47049.435334\n",
      "Train_EnvstepsSoFar : 7400001\n",
      "Train_AverageReturn : 1834.7\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47049.43533396721\n",
      "Training Loss : 0.13173934817314148\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7410001\n",
      "mean reward (100 episodes) 1846.600000\n",
      "best mean reward 1886.900000\n",
      "running time 47111.877944\n",
      "Train_EnvstepsSoFar : 7410001\n",
      "Train_AverageReturn : 1846.6\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47111.87794351578\n",
      "Training Loss : 0.4148975610733032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7420001\n",
      "mean reward (100 episodes) 1703.200000\n",
      "best mean reward 1886.900000\n",
      "running time 47174.390191\n",
      "Train_EnvstepsSoFar : 7420001\n",
      "Train_AverageReturn : 1703.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47174.390191078186\n",
      "Training Loss : 0.17231255769729614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7430001\n",
      "mean reward (100 episodes) 1836.000000\n",
      "best mean reward 1886.900000\n",
      "running time 47237.064062\n",
      "Train_EnvstepsSoFar : 7430001\n",
      "Train_AverageReturn : 1836.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47237.06406211853\n",
      "Training Loss : 0.0804399698972702\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7440001\n",
      "mean reward (100 episodes) 1796.000000\n",
      "best mean reward 1886.900000\n",
      "running time 47299.594921\n",
      "Train_EnvstepsSoFar : 7440001\n",
      "Train_AverageReturn : 1796.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47299.59492111206\n",
      "Training Loss : 0.35008081793785095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7450001\n",
      "mean reward (100 episodes) 1785.900000\n",
      "best mean reward 1886.900000\n",
      "running time 47362.143282\n",
      "Train_EnvstepsSoFar : 7450001\n",
      "Train_AverageReturn : 1785.9\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47362.14328241348\n",
      "Training Loss : 0.7886698246002197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7460001\n",
      "mean reward (100 episodes) 1701.200000\n",
      "best mean reward 1886.900000\n",
      "running time 47425.558444\n",
      "Train_EnvstepsSoFar : 7460001\n",
      "Train_AverageReturn : 1701.2\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47425.558443784714\n",
      "Training Loss : 0.4980258345603943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7470001\n",
      "mean reward (100 episodes) 1816.000000\n",
      "best mean reward 1886.900000\n",
      "running time 47488.085890\n",
      "Train_EnvstepsSoFar : 7470001\n",
      "Train_AverageReturn : 1816.0\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47488.08589029312\n",
      "Training Loss : 0.1802656650543213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7480001\n",
      "mean reward (100 episodes) 1833.400000\n",
      "best mean reward 1886.900000\n",
      "running time 47550.808899\n",
      "Train_EnvstepsSoFar : 7480001\n",
      "Train_AverageReturn : 1833.4\n",
      "Train_BestReturn : 1886.9\n",
      "TimeSinceStart : 47550.80889868736\n",
      "Training Loss : 2.7605104446411133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7490001\n",
      "mean reward (100 episodes) 1911.100000\n",
      "best mean reward 1911.100000\n",
      "running time 47612.953225\n",
      "Train_EnvstepsSoFar : 7490001\n",
      "Train_AverageReturn : 1911.1\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47612.95322537422\n",
      "Training Loss : 0.3224951922893524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7500001\n",
      "mean reward (100 episodes) 1802.600000\n",
      "best mean reward 1911.100000\n",
      "running time 47675.536116\n",
      "Train_EnvstepsSoFar : 7500001\n",
      "Train_AverageReturn : 1802.6\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47675.53611564636\n",
      "Training Loss : 0.19363893568515778\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7510001\n",
      "mean reward (100 episodes) 1818.700000\n",
      "best mean reward 1911.100000\n",
      "running time 47738.256955\n",
      "Train_EnvstepsSoFar : 7510001\n",
      "Train_AverageReturn : 1818.7\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47738.25695538521\n",
      "Training Loss : 1.2942829132080078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7520001\n",
      "mean reward (100 episodes) 1731.500000\n",
      "best mean reward 1911.100000\n",
      "running time 47801.111687\n",
      "Train_EnvstepsSoFar : 7520001\n",
      "Train_AverageReturn : 1731.5\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47801.1116874218\n",
      "Training Loss : 0.10544825345277786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7530001\n",
      "mean reward (100 episodes) 1805.300000\n",
      "best mean reward 1911.100000\n",
      "running time 47863.539577\n",
      "Train_EnvstepsSoFar : 7530001\n",
      "Train_AverageReturn : 1805.3\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47863.53957724571\n",
      "Training Loss : 0.19161656498908997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7540001\n",
      "mean reward (100 episodes) 1779.900000\n",
      "best mean reward 1911.100000\n",
      "running time 47925.966159\n",
      "Train_EnvstepsSoFar : 7540001\n",
      "Train_AverageReturn : 1779.9\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47925.9661591053\n",
      "Training Loss : 1.6299262046813965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7550001\n",
      "mean reward (100 episodes) 1859.300000\n",
      "best mean reward 1911.100000\n",
      "running time 47988.198875\n",
      "Train_EnvstepsSoFar : 7550001\n",
      "Train_AverageReturn : 1859.3\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 47988.198875427246\n",
      "Training Loss : 0.17300258576869965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7560001\n",
      "mean reward (100 episodes) 1815.700000\n",
      "best mean reward 1911.100000\n",
      "running time 48051.890676\n",
      "Train_EnvstepsSoFar : 7560001\n",
      "Train_AverageReturn : 1815.7\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48051.89067554474\n",
      "Training Loss : 0.09760531038045883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7570001\n",
      "mean reward (100 episodes) 1822.400000\n",
      "best mean reward 1911.100000\n",
      "running time 48114.451889\n",
      "Train_EnvstepsSoFar : 7570001\n",
      "Train_AverageReturn : 1822.4\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48114.45188879967\n",
      "Training Loss : 1.3724631071090698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7580001\n",
      "mean reward (100 episodes) 1812.800000\n",
      "best mean reward 1911.100000\n",
      "running time 48177.006186\n",
      "Train_EnvstepsSoFar : 7580001\n",
      "Train_AverageReturn : 1812.8\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48177.006185770035\n",
      "Training Loss : 1.0126007795333862\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7590001\n",
      "mean reward (100 episodes) 1889.800000\n",
      "best mean reward 1911.100000\n",
      "running time 48239.521746\n",
      "Train_EnvstepsSoFar : 7590001\n",
      "Train_AverageReturn : 1889.8\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48239.52174592018\n",
      "Training Loss : 1.5883150100708008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7600001\n",
      "mean reward (100 episodes) 1849.700000\n",
      "best mean reward 1911.100000\n",
      "running time 48302.157337\n",
      "Train_EnvstepsSoFar : 7600001\n",
      "Train_AverageReturn : 1849.7\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48302.15733718872\n",
      "Training Loss : 1.6132752895355225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7610001\n",
      "mean reward (100 episodes) 1781.100000\n",
      "best mean reward 1911.100000\n",
      "running time 48365.193832\n",
      "Train_EnvstepsSoFar : 7610001\n",
      "Train_AverageReturn : 1781.1\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48365.193831920624\n",
      "Training Loss : 0.49537724256515503\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7620001\n",
      "mean reward (100 episodes) 1745.400000\n",
      "best mean reward 1911.100000\n",
      "running time 48427.942566\n",
      "Train_EnvstepsSoFar : 7620001\n",
      "Train_AverageReturn : 1745.4\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48427.94256567955\n",
      "Training Loss : 1.5677168369293213\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7630001\n",
      "mean reward (100 episodes) 1766.100000\n",
      "best mean reward 1911.100000\n",
      "running time 48490.765751\n",
      "Train_EnvstepsSoFar : 7630001\n",
      "Train_AverageReturn : 1766.1\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48490.76575088501\n",
      "Training Loss : 0.9046761393547058\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7640001\n",
      "mean reward (100 episodes) 1798.400000\n",
      "best mean reward 1911.100000\n",
      "running time 48553.091205\n",
      "Train_EnvstepsSoFar : 7640001\n",
      "Train_AverageReturn : 1798.4\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48553.091205358505\n",
      "Training Loss : 0.9486188888549805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7650001\n",
      "mean reward (100 episodes) 1821.100000\n",
      "best mean reward 1911.100000\n",
      "running time 48615.998619\n",
      "Train_EnvstepsSoFar : 7650001\n",
      "Train_AverageReturn : 1821.1\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48615.99861931801\n",
      "Training Loss : 2.2788589000701904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7660001\n",
      "mean reward (100 episodes) 1780.500000\n",
      "best mean reward 1911.100000\n",
      "running time 48679.512018\n",
      "Train_EnvstepsSoFar : 7660001\n",
      "Train_AverageReturn : 1780.5\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48679.512018442154\n",
      "Training Loss : 0.34519803524017334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7670001\n",
      "mean reward (100 episodes) 1723.300000\n",
      "best mean reward 1911.100000\n",
      "running time 48742.334826\n",
      "Train_EnvstepsSoFar : 7670001\n",
      "Train_AverageReturn : 1723.3\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48742.33482646942\n",
      "Training Loss : 1.4331774711608887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7680001\n",
      "mean reward (100 episodes) 1781.300000\n",
      "best mean reward 1911.100000\n",
      "running time 48804.927213\n",
      "Train_EnvstepsSoFar : 7680001\n",
      "Train_AverageReturn : 1781.3\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48804.927213191986\n",
      "Training Loss : 0.9311586618423462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7690001\n",
      "mean reward (100 episodes) 1771.300000\n",
      "best mean reward 1911.100000\n",
      "running time 48868.331818\n",
      "Train_EnvstepsSoFar : 7690001\n",
      "Train_AverageReturn : 1771.3\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48868.33181786537\n",
      "Training Loss : 0.19681942462921143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7700001\n",
      "mean reward (100 episodes) 1785.500000\n",
      "best mean reward 1911.100000\n",
      "running time 48931.267721\n",
      "Train_EnvstepsSoFar : 7700001\n",
      "Train_AverageReturn : 1785.5\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48931.26772069931\n",
      "Training Loss : 1.7002543210983276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7710001\n",
      "mean reward (100 episodes) 1693.200000\n",
      "best mean reward 1911.100000\n",
      "running time 48994.194263\n",
      "Train_EnvstepsSoFar : 7710001\n",
      "Train_AverageReturn : 1693.2\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 48994.19426321983\n",
      "Training Loss : 0.2797245979309082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7720001\n",
      "mean reward (100 episodes) 1730.800000\n",
      "best mean reward 1911.100000\n",
      "running time 49057.231693\n",
      "Train_EnvstepsSoFar : 7720001\n",
      "Train_AverageReturn : 1730.8\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49057.231693029404\n",
      "Training Loss : 0.18963272869586945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7730001\n",
      "mean reward (100 episodes) 1785.500000\n",
      "best mean reward 1911.100000\n",
      "running time 49120.131773\n",
      "Train_EnvstepsSoFar : 7730001\n",
      "Train_AverageReturn : 1785.5\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49120.13177275658\n",
      "Training Loss : 1.7726771831512451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7740001\n",
      "mean reward (100 episodes) 1838.700000\n",
      "best mean reward 1911.100000\n",
      "running time 49182.972992\n",
      "Train_EnvstepsSoFar : 7740001\n",
      "Train_AverageReturn : 1838.7\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49182.97299194336\n",
      "Training Loss : 0.27591222524642944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7750001\n",
      "mean reward (100 episodes) 1869.900000\n",
      "best mean reward 1911.100000\n",
      "running time 49245.550032\n",
      "Train_EnvstepsSoFar : 7750001\n",
      "Train_AverageReturn : 1869.9\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49245.55003166199\n",
      "Training Loss : 0.17502348124980927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7760001\n",
      "mean reward (100 episodes) 1833.600000\n",
      "best mean reward 1911.100000\n",
      "running time 49308.112441\n",
      "Train_EnvstepsSoFar : 7760001\n",
      "Train_AverageReturn : 1833.6\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49308.112441301346\n",
      "Training Loss : 0.14933614432811737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7770001\n",
      "mean reward (100 episodes) 1814.000000\n",
      "best mean reward 1911.100000\n",
      "running time 49371.085537\n",
      "Train_EnvstepsSoFar : 7770001\n",
      "Train_AverageReturn : 1814.0\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49371.08553671837\n",
      "Training Loss : 0.18643376231193542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7780001\n",
      "mean reward (100 episodes) 1836.000000\n",
      "best mean reward 1911.100000\n",
      "running time 49434.007094\n",
      "Train_EnvstepsSoFar : 7780001\n",
      "Train_AverageReturn : 1836.0\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49434.007093667984\n",
      "Training Loss : 0.2839112877845764\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7790001\n",
      "mean reward (100 episodes) 1881.900000\n",
      "best mean reward 1911.100000\n",
      "running time 49497.083224\n",
      "Train_EnvstepsSoFar : 7790001\n",
      "Train_AverageReturn : 1881.9\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49497.08322429657\n",
      "Training Loss : 0.12423115223646164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7800001\n",
      "mean reward (100 episodes) 1851.200000\n",
      "best mean reward 1911.100000\n",
      "running time 49560.291322\n",
      "Train_EnvstepsSoFar : 7800001\n",
      "Train_AverageReturn : 1851.2\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49560.29132151604\n",
      "Training Loss : 0.1577386111021042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7810001\n",
      "mean reward (100 episodes) 1799.100000\n",
      "best mean reward 1911.100000\n",
      "running time 49623.899226\n",
      "Train_EnvstepsSoFar : 7810001\n",
      "Train_AverageReturn : 1799.1\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49623.89922642708\n",
      "Training Loss : 1.7998415231704712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7820001\n",
      "mean reward (100 episodes) 1768.100000\n",
      "best mean reward 1911.100000\n",
      "running time 49687.427805\n",
      "Train_EnvstepsSoFar : 7820001\n",
      "Train_AverageReturn : 1768.1\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49687.42780470848\n",
      "Training Loss : 0.15603648126125336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7830001\n",
      "mean reward (100 episodes) 1798.300000\n",
      "best mean reward 1911.100000\n",
      "running time 49750.779640\n",
      "Train_EnvstepsSoFar : 7830001\n",
      "Train_AverageReturn : 1798.3\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49750.779640197754\n",
      "Training Loss : 0.14252981543540955\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7840001\n",
      "mean reward (100 episodes) 1792.700000\n",
      "best mean reward 1911.100000\n",
      "running time 49814.113633\n",
      "Train_EnvstepsSoFar : 7840001\n",
      "Train_AverageReturn : 1792.7\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49814.113632917404\n",
      "Training Loss : 3.136897563934326\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7850001\n",
      "mean reward (100 episodes) 1779.400000\n",
      "best mean reward 1911.100000\n",
      "running time 49878.847534\n",
      "Train_EnvstepsSoFar : 7850001\n",
      "Train_AverageReturn : 1779.4\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49878.84753417969\n",
      "Training Loss : 2.9032199382781982\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7860001\n",
      "mean reward (100 episodes) 1740.700000\n",
      "best mean reward 1911.100000\n",
      "running time 49942.676341\n",
      "Train_EnvstepsSoFar : 7860001\n",
      "Train_AverageReturn : 1740.7\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 49942.676341056824\n",
      "Training Loss : 0.17479290068149567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7870001\n",
      "mean reward (100 episodes) 1756.000000\n",
      "best mean reward 1911.100000\n",
      "running time 50005.651585\n",
      "Train_EnvstepsSoFar : 7870001\n",
      "Train_AverageReturn : 1756.0\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 50005.65158486366\n",
      "Training Loss : 2.698824882507324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7880001\n",
      "mean reward (100 episodes) 1754.900000\n",
      "best mean reward 1911.100000\n",
      "running time 50068.675181\n",
      "Train_EnvstepsSoFar : 7880001\n",
      "Train_AverageReturn : 1754.9\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 50068.67518115044\n",
      "Training Loss : 0.13003133237361908\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7890001\n",
      "mean reward (100 episodes) 1837.600000\n",
      "best mean reward 1911.100000\n",
      "running time 50131.492909\n",
      "Train_EnvstepsSoFar : 7890001\n",
      "Train_AverageReturn : 1837.6\n",
      "Train_BestReturn : 1911.1\n",
      "TimeSinceStart : 50131.49290895462\n",
      "Training Loss : 1.2215073108673096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7900001\n",
      "mean reward (100 episodes) 1911.500000\n",
      "best mean reward 1911.500000\n",
      "running time 50194.507450\n",
      "Train_EnvstepsSoFar : 7900001\n",
      "Train_AverageReturn : 1911.5\n",
      "Train_BestReturn : 1911.5\n",
      "TimeSinceStart : 50194.50744962692\n",
      "Training Loss : 0.7729524374008179\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7910001\n",
      "mean reward (100 episodes) 1926.800000\n",
      "best mean reward 1926.800000\n",
      "running time 50257.117866\n",
      "Train_EnvstepsSoFar : 7910001\n",
      "Train_AverageReturn : 1926.8\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50257.11786580086\n",
      "Training Loss : 0.16849887371063232\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7920001\n",
      "mean reward (100 episodes) 1895.400000\n",
      "best mean reward 1926.800000\n",
      "running time 50319.707425\n",
      "Train_EnvstepsSoFar : 7920001\n",
      "Train_AverageReturn : 1895.4\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50319.707424640656\n",
      "Training Loss : 1.2276043891906738\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7930001\n",
      "mean reward (100 episodes) 1879.300000\n",
      "best mean reward 1926.800000\n",
      "running time 50382.509250\n",
      "Train_EnvstepsSoFar : 7930001\n",
      "Train_AverageReturn : 1879.3\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50382.50925040245\n",
      "Training Loss : 0.20739203691482544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7940001\n",
      "mean reward (100 episodes) 1816.700000\n",
      "best mean reward 1926.800000\n",
      "running time 50445.390289\n",
      "Train_EnvstepsSoFar : 7940001\n",
      "Train_AverageReturn : 1816.7\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50445.39028906822\n",
      "Training Loss : 0.2647436261177063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7950001\n",
      "mean reward (100 episodes) 1862.200000\n",
      "best mean reward 1926.800000\n",
      "running time 50507.825237\n",
      "Train_EnvstepsSoFar : 7950001\n",
      "Train_AverageReturn : 1862.2\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50507.82523727417\n",
      "Training Loss : 0.9050281047821045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7960001\n",
      "mean reward (100 episodes) 1857.600000\n",
      "best mean reward 1926.800000\n",
      "running time 50570.634121\n",
      "Train_EnvstepsSoFar : 7960001\n",
      "Train_AverageReturn : 1857.6\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50570.63412070274\n",
      "Training Loss : 1.3229002952575684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7970001\n",
      "mean reward (100 episodes) 1903.800000\n",
      "best mean reward 1926.800000\n",
      "running time 50634.162020\n",
      "Train_EnvstepsSoFar : 7970001\n",
      "Train_AverageReturn : 1903.8\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50634.16201996803\n",
      "Training Loss : 1.0679960250854492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7980001\n",
      "mean reward (100 episodes) 1897.300000\n",
      "best mean reward 1926.800000\n",
      "running time 50696.834043\n",
      "Train_EnvstepsSoFar : 7980001\n",
      "Train_AverageReturn : 1897.3\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50696.83404302597\n",
      "Training Loss : 0.18494854867458344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 7990001\n",
      "mean reward (100 episodes) 1903.700000\n",
      "best mean reward 1926.800000\n",
      "running time 50759.687421\n",
      "Train_EnvstepsSoFar : 7990001\n",
      "Train_AverageReturn : 1903.7\n",
      "Train_BestReturn : 1926.8\n",
      "TimeSinceStart : 50759.68742132187\n",
      "Training Loss : 0.16355016827583313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8000001\n",
      "mean reward (100 episodes) 1987.900000\n",
      "best mean reward 1987.900000\n",
      "running time 50822.655908\n",
      "Train_EnvstepsSoFar : 8000001\n",
      "Train_AverageReturn : 1987.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 50822.65590786934\n",
      "Training Loss : 0.12877070903778076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8010001\n",
      "mean reward (100 episodes) 1978.900000\n",
      "best mean reward 1987.900000\n",
      "running time 50885.676174\n",
      "Train_EnvstepsSoFar : 8010001\n",
      "Train_AverageReturn : 1978.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 50885.67617416382\n",
      "Training Loss : 0.22938776016235352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8020001\n",
      "mean reward (100 episodes) 1971.500000\n",
      "best mean reward 1987.900000\n",
      "running time 50948.280513\n",
      "Train_EnvstepsSoFar : 8020001\n",
      "Train_AverageReturn : 1971.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 50948.280512571335\n",
      "Training Loss : 0.9453209042549133\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8030001\n",
      "mean reward (100 episodes) 1902.400000\n",
      "best mean reward 1987.900000\n",
      "running time 51011.320245\n",
      "Train_EnvstepsSoFar : 8030001\n",
      "Train_AverageReturn : 1902.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51011.320244550705\n",
      "Training Loss : 0.19279178977012634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8040001\n",
      "mean reward (100 episodes) 1847.300000\n",
      "best mean reward 1987.900000\n",
      "running time 51073.744620\n",
      "Train_EnvstepsSoFar : 8040001\n",
      "Train_AverageReturn : 1847.3\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51073.744619846344\n",
      "Training Loss : 1.1491587162017822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8050001\n",
      "mean reward (100 episodes) 1804.400000\n",
      "best mean reward 1987.900000\n",
      "running time 51136.579498\n",
      "Train_EnvstepsSoFar : 8050001\n",
      "Train_AverageReturn : 1804.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51136.57949757576\n",
      "Training Loss : 0.21158161759376526\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8060001\n",
      "mean reward (100 episodes) 1744.900000\n",
      "best mean reward 1987.900000\n",
      "running time 51199.360031\n",
      "Train_EnvstepsSoFar : 8060001\n",
      "Train_AverageReturn : 1744.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51199.36003088951\n",
      "Training Loss : 0.18487849831581116\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8070001\n",
      "mean reward (100 episodes) 1861.100000\n",
      "best mean reward 1987.900000\n",
      "running time 51261.948016\n",
      "Train_EnvstepsSoFar : 8070001\n",
      "Train_AverageReturn : 1861.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51261.948016405106\n",
      "Training Loss : 0.4373980760574341\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8080001\n",
      "mean reward (100 episodes) 1827.600000\n",
      "best mean reward 1987.900000\n",
      "running time 51324.686027\n",
      "Train_EnvstepsSoFar : 8080001\n",
      "Train_AverageReturn : 1827.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51324.68602657318\n",
      "Training Loss : 0.5247253775596619\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8090001\n",
      "mean reward (100 episodes) 1931.400000\n",
      "best mean reward 1987.900000\n",
      "running time 51387.522620\n",
      "Train_EnvstepsSoFar : 8090001\n",
      "Train_AverageReturn : 1931.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51387.52261996269\n",
      "Training Loss : 0.6140324473381042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8100001\n",
      "mean reward (100 episodes) 1865.800000\n",
      "best mean reward 1987.900000\n",
      "running time 51450.516908\n",
      "Train_EnvstepsSoFar : 8100001\n",
      "Train_AverageReturn : 1865.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51450.516907930374\n",
      "Training Loss : 1.9647818803787231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8110001\n",
      "mean reward (100 episodes) 1906.600000\n",
      "best mean reward 1987.900000\n",
      "running time 51513.044101\n",
      "Train_EnvstepsSoFar : 8110001\n",
      "Train_AverageReturn : 1906.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51513.04410076141\n",
      "Training Loss : 0.20763470232486725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8120001\n",
      "mean reward (100 episodes) 1891.000000\n",
      "best mean reward 1987.900000\n",
      "running time 51575.999846\n",
      "Train_EnvstepsSoFar : 8120001\n",
      "Train_AverageReturn : 1891.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51575.99984550476\n",
      "Training Loss : 0.3590393662452698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8130001\n",
      "mean reward (100 episodes) 1850.800000\n",
      "best mean reward 1987.900000\n",
      "running time 51638.898341\n",
      "Train_EnvstepsSoFar : 8130001\n",
      "Train_AverageReturn : 1850.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51638.89834070206\n",
      "Training Loss : 0.09613898396492004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8140001\n",
      "mean reward (100 episodes) 1821.000000\n",
      "best mean reward 1987.900000\n",
      "running time 51701.599151\n",
      "Train_EnvstepsSoFar : 8140001\n",
      "Train_AverageReturn : 1821.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51701.599150657654\n",
      "Training Loss : 0.16570913791656494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8150001\n",
      "mean reward (100 episodes) 1801.600000\n",
      "best mean reward 1987.900000\n",
      "running time 51765.232971\n",
      "Train_EnvstepsSoFar : 8150001\n",
      "Train_AverageReturn : 1801.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51765.23297095299\n",
      "Training Loss : 0.422005832195282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8160001\n",
      "mean reward (100 episodes) 1855.800000\n",
      "best mean reward 1987.900000\n",
      "running time 51828.199217\n",
      "Train_EnvstepsSoFar : 8160001\n",
      "Train_AverageReturn : 1855.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51828.19921660423\n",
      "Training Loss : 0.3131495714187622\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8170001\n",
      "mean reward (100 episodes) 1850.500000\n",
      "best mean reward 1987.900000\n",
      "running time 51891.345594\n",
      "Train_EnvstepsSoFar : 8170001\n",
      "Train_AverageReturn : 1850.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51891.34559440613\n",
      "Training Loss : 0.15509605407714844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8180001\n",
      "mean reward (100 episodes) 1860.100000\n",
      "best mean reward 1987.900000\n",
      "running time 51954.408258\n",
      "Train_EnvstepsSoFar : 8180001\n",
      "Train_AverageReturn : 1860.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 51954.40825843811\n",
      "Training Loss : 0.11413560062646866\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8190001\n",
      "mean reward (100 episodes) 1850.700000\n",
      "best mean reward 1987.900000\n",
      "running time 52017.606256\n",
      "Train_EnvstepsSoFar : 8190001\n",
      "Train_AverageReturn : 1850.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52017.606256484985\n",
      "Training Loss : 0.2317625731229782\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8200001\n",
      "mean reward (100 episodes) 1849.700000\n",
      "best mean reward 1987.900000\n",
      "running time 52080.292891\n",
      "Train_EnvstepsSoFar : 8200001\n",
      "Train_AverageReturn : 1849.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52080.292890787125\n",
      "Training Loss : 1.6931681632995605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8210001\n",
      "mean reward (100 episodes) 1903.600000\n",
      "best mean reward 1987.900000\n",
      "running time 52143.068906\n",
      "Train_EnvstepsSoFar : 8210001\n",
      "Train_AverageReturn : 1903.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52143.06890583038\n",
      "Training Loss : 0.10155409574508667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8220001\n",
      "mean reward (100 episodes) 1856.000000\n",
      "best mean reward 1987.900000\n",
      "running time 52206.306066\n",
      "Train_EnvstepsSoFar : 8220001\n",
      "Train_AverageReturn : 1856.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52206.30606627464\n",
      "Training Loss : 0.20881123840808868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8230001\n",
      "mean reward (100 episodes) 1945.100000\n",
      "best mean reward 1987.900000\n",
      "running time 52269.382162\n",
      "Train_EnvstepsSoFar : 8230001\n",
      "Train_AverageReturn : 1945.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52269.382162094116\n",
      "Training Loss : 0.24264177680015564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8240001\n",
      "mean reward (100 episodes) 1860.600000\n",
      "best mean reward 1987.900000\n",
      "running time 52332.272516\n",
      "Train_EnvstepsSoFar : 8240001\n",
      "Train_AverageReturn : 1860.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52332.27251648903\n",
      "Training Loss : 1.1843868494033813\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8250001\n",
      "mean reward (100 episodes) 1909.400000\n",
      "best mean reward 1987.900000\n",
      "running time 52395.390875\n",
      "Train_EnvstepsSoFar : 8250001\n",
      "Train_AverageReturn : 1909.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52395.39087510109\n",
      "Training Loss : 1.0986982583999634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8260001\n",
      "mean reward (100 episodes) 1877.300000\n",
      "best mean reward 1987.900000\n",
      "running time 52458.708465\n",
      "Train_EnvstepsSoFar : 8260001\n",
      "Train_AverageReturn : 1877.3\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52458.70846533775\n",
      "Training Loss : 0.2101365178823471\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8270001\n",
      "mean reward (100 episodes) 1858.500000\n",
      "best mean reward 1987.900000\n",
      "running time 52521.465768\n",
      "Train_EnvstepsSoFar : 8270001\n",
      "Train_AverageReturn : 1858.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52521.46576833725\n",
      "Training Loss : 1.0624092817306519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8280001\n",
      "mean reward (100 episodes) 1840.800000\n",
      "best mean reward 1987.900000\n",
      "running time 52584.493953\n",
      "Train_EnvstepsSoFar : 8280001\n",
      "Train_AverageReturn : 1840.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52584.49395298958\n",
      "Training Loss : 0.2945617437362671\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8290001\n",
      "mean reward (100 episodes) 1755.100000\n",
      "best mean reward 1987.900000\n",
      "running time 52647.745099\n",
      "Train_EnvstepsSoFar : 8290001\n",
      "Train_AverageReturn : 1755.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52647.74509882927\n",
      "Training Loss : 0.6138524413108826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8300001\n",
      "mean reward (100 episodes) 1838.600000\n",
      "best mean reward 1987.900000\n",
      "running time 52710.814933\n",
      "Train_EnvstepsSoFar : 8300001\n",
      "Train_AverageReturn : 1838.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52710.81493258476\n",
      "Training Loss : 2.012706995010376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8310001\n",
      "mean reward (100 episodes) 1829.600000\n",
      "best mean reward 1987.900000\n",
      "running time 52773.680861\n",
      "Train_EnvstepsSoFar : 8310001\n",
      "Train_AverageReturn : 1829.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52773.680861234665\n",
      "Training Loss : 0.16167166829109192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8320001\n",
      "mean reward (100 episodes) 1907.800000\n",
      "best mean reward 1987.900000\n",
      "running time 52836.918028\n",
      "Train_EnvstepsSoFar : 8320001\n",
      "Train_AverageReturn : 1907.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52836.918028116226\n",
      "Training Loss : 0.17410914599895477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8330001\n",
      "mean reward (100 episodes) 1861.400000\n",
      "best mean reward 1987.900000\n",
      "running time 52900.316962\n",
      "Train_EnvstepsSoFar : 8330001\n",
      "Train_AverageReturn : 1861.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52900.31696200371\n",
      "Training Loss : 0.21695411205291748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8340001\n",
      "mean reward (100 episodes) 1844.100000\n",
      "best mean reward 1987.900000\n",
      "running time 52963.427377\n",
      "Train_EnvstepsSoFar : 8340001\n",
      "Train_AverageReturn : 1844.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 52963.42737698555\n",
      "Training Loss : 0.16111668944358826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8350001\n",
      "mean reward (100 episodes) 1826.200000\n",
      "best mean reward 1987.900000\n",
      "running time 53026.576174\n",
      "Train_EnvstepsSoFar : 8350001\n",
      "Train_AverageReturn : 1826.2\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53026.576174259186\n",
      "Training Loss : 0.7247086763381958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8360001\n",
      "mean reward (100 episodes) 1812.000000\n",
      "best mean reward 1987.900000\n",
      "running time 53089.144912\n",
      "Train_EnvstepsSoFar : 8360001\n",
      "Train_AverageReturn : 1812.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53089.14491176605\n",
      "Training Loss : 0.2785980999469757\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8370001\n",
      "mean reward (100 episodes) 1775.600000\n",
      "best mean reward 1987.900000\n",
      "running time 53152.068245\n",
      "Train_EnvstepsSoFar : 8370001\n",
      "Train_AverageReturn : 1775.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53152.06824493408\n",
      "Training Loss : 0.3541930019855499\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8380001\n",
      "mean reward (100 episodes) 1860.400000\n",
      "best mean reward 1987.900000\n",
      "running time 53215.003968\n",
      "Train_EnvstepsSoFar : 8380001\n",
      "Train_AverageReturn : 1860.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53215.00396776199\n",
      "Training Loss : 0.17649376392364502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8390001\n",
      "mean reward (100 episodes) 1803.800000\n",
      "best mean reward 1987.900000\n",
      "running time 53278.086962\n",
      "Train_EnvstepsSoFar : 8390001\n",
      "Train_AverageReturn : 1803.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53278.086961984634\n",
      "Training Loss : 1.6739917993545532\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8400001\n",
      "mean reward (100 episodes) 1885.100000\n",
      "best mean reward 1987.900000\n",
      "running time 53340.661969\n",
      "Train_EnvstepsSoFar : 8400001\n",
      "Train_AverageReturn : 1885.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53340.66196870804\n",
      "Training Loss : 0.9932334423065186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8410001\n",
      "mean reward (100 episodes) 1816.800000\n",
      "best mean reward 1987.900000\n",
      "running time 53403.284392\n",
      "Train_EnvstepsSoFar : 8410001\n",
      "Train_AverageReturn : 1816.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53403.284391880035\n",
      "Training Loss : 0.17442487180233002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8420001\n",
      "mean reward (100 episodes) 1908.100000\n",
      "best mean reward 1987.900000\n",
      "running time 53466.276757\n",
      "Train_EnvstepsSoFar : 8420001\n",
      "Train_AverageReturn : 1908.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53466.27675652504\n",
      "Training Loss : 1.5397945642471313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8430001\n",
      "mean reward (100 episodes) 1871.700000\n",
      "best mean reward 1987.900000\n",
      "running time 53529.811418\n",
      "Train_EnvstepsSoFar : 8430001\n",
      "Train_AverageReturn : 1871.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53529.81141829491\n",
      "Training Loss : 0.4705854654312134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8440001\n",
      "mean reward (100 episodes) 1851.600000\n",
      "best mean reward 1987.900000\n",
      "running time 53592.854200\n",
      "Train_EnvstepsSoFar : 8440001\n",
      "Train_AverageReturn : 1851.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53592.85420036316\n",
      "Training Loss : 0.2542598247528076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8450001\n",
      "mean reward (100 episodes) 1826.000000\n",
      "best mean reward 1987.900000\n",
      "running time 53655.773099\n",
      "Train_EnvstepsSoFar : 8450001\n",
      "Train_AverageReturn : 1826.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53655.7730987072\n",
      "Training Loss : 0.5807266235351562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8460001\n",
      "mean reward (100 episodes) 1832.200000\n",
      "best mean reward 1987.900000\n",
      "running time 53719.754689\n",
      "Train_EnvstepsSoFar : 8460001\n",
      "Train_AverageReturn : 1832.2\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53719.75468945503\n",
      "Training Loss : 1.595523715019226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8470001\n",
      "mean reward (100 episodes) 1866.500000\n",
      "best mean reward 1987.900000\n",
      "running time 53782.766577\n",
      "Train_EnvstepsSoFar : 8470001\n",
      "Train_AverageReturn : 1866.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53782.76657652855\n",
      "Training Loss : 0.7719392776489258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8480001\n",
      "mean reward (100 episodes) 1804.700000\n",
      "best mean reward 1987.900000\n",
      "running time 53845.637214\n",
      "Train_EnvstepsSoFar : 8480001\n",
      "Train_AverageReturn : 1804.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53845.63721394539\n",
      "Training Loss : 0.657788872718811\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8490001\n",
      "mean reward (100 episodes) 1769.700000\n",
      "best mean reward 1987.900000\n",
      "running time 53908.643290\n",
      "Train_EnvstepsSoFar : 8490001\n",
      "Train_AverageReturn : 1769.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53908.64328980446\n",
      "Training Loss : 1.422417402267456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8500001\n",
      "mean reward (100 episodes) 1734.300000\n",
      "best mean reward 1987.900000\n",
      "running time 53971.430590\n",
      "Train_EnvstepsSoFar : 8500001\n",
      "Train_AverageReturn : 1734.3\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 53971.43058991432\n",
      "Training Loss : 0.16947515308856964\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8510001\n",
      "mean reward (100 episodes) 1802.200000\n",
      "best mean reward 1987.900000\n",
      "running time 54034.386331\n",
      "Train_EnvstepsSoFar : 8510001\n",
      "Train_AverageReturn : 1802.2\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54034.38633108139\n",
      "Training Loss : 0.43656426668167114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8520001\n",
      "mean reward (100 episodes) 1815.200000\n",
      "best mean reward 1987.900000\n",
      "running time 54097.512241\n",
      "Train_EnvstepsSoFar : 8520001\n",
      "Train_AverageReturn : 1815.2\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54097.51224112511\n",
      "Training Loss : 0.33244332671165466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8530001\n",
      "mean reward (100 episodes) 1868.000000\n",
      "best mean reward 1987.900000\n",
      "running time 54160.864683\n",
      "Train_EnvstepsSoFar : 8530001\n",
      "Train_AverageReturn : 1868.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54160.86468267441\n",
      "Training Loss : 0.1969098001718521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8540001\n",
      "mean reward (100 episodes) 1843.000000\n",
      "best mean reward 1987.900000\n",
      "running time 54223.947170\n",
      "Train_EnvstepsSoFar : 8540001\n",
      "Train_AverageReturn : 1843.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54223.94717001915\n",
      "Training Loss : 0.383092999458313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8550001\n",
      "mean reward (100 episodes) 1863.000000\n",
      "best mean reward 1987.900000\n",
      "running time 54286.756236\n",
      "Train_EnvstepsSoFar : 8550001\n",
      "Train_AverageReturn : 1863.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54286.75623583794\n",
      "Training Loss : 1.3774502277374268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8560001\n",
      "mean reward (100 episodes) 1873.200000\n",
      "best mean reward 1987.900000\n",
      "running time 54349.346115\n",
      "Train_EnvstepsSoFar : 8560001\n",
      "Train_AverageReturn : 1873.2\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54349.346114873886\n",
      "Training Loss : 0.14645066857337952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8570001\n",
      "mean reward (100 episodes) 1899.100000\n",
      "best mean reward 1987.900000\n",
      "running time 54412.230048\n",
      "Train_EnvstepsSoFar : 8570001\n",
      "Train_AverageReturn : 1899.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54412.23004770279\n",
      "Training Loss : 0.1410149335861206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8580001\n",
      "mean reward (100 episodes) 1850.900000\n",
      "best mean reward 1987.900000\n",
      "running time 54475.615334\n",
      "Train_EnvstepsSoFar : 8580001\n",
      "Train_AverageReturn : 1850.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54475.615334272385\n",
      "Training Loss : 0.15890668332576752\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8590001\n",
      "mean reward (100 episodes) 1867.400000\n",
      "best mean reward 1987.900000\n",
      "running time 54539.178381\n",
      "Train_EnvstepsSoFar : 8590001\n",
      "Train_AverageReturn : 1867.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54539.17838072777\n",
      "Training Loss : 0.16680704057216644\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8600001\n",
      "mean reward (100 episodes) 1844.700000\n",
      "best mean reward 1987.900000\n",
      "running time 54610.083571\n",
      "Train_EnvstepsSoFar : 8600001\n",
      "Train_AverageReturn : 1844.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54610.0835711956\n",
      "Training Loss : 0.16182971000671387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8610001\n",
      "mean reward (100 episodes) 1835.700000\n",
      "best mean reward 1987.900000\n",
      "running time 54678.973550\n",
      "Train_EnvstepsSoFar : 8610001\n",
      "Train_AverageReturn : 1835.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54678.973549842834\n",
      "Training Loss : 0.09004196524620056\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8620001\n",
      "mean reward (100 episodes) 1742.800000\n",
      "best mean reward 1987.900000\n",
      "running time 54751.193064\n",
      "Train_EnvstepsSoFar : 8620001\n",
      "Train_AverageReturn : 1742.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54751.19306373596\n",
      "Training Loss : 0.1676051765680313\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8630001\n",
      "mean reward (100 episodes) 1745.900000\n",
      "best mean reward 1987.900000\n",
      "running time 54817.359814\n",
      "Train_EnvstepsSoFar : 8630001\n",
      "Train_AverageReturn : 1745.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54817.359813690186\n",
      "Training Loss : 0.09902815520763397\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8640001\n",
      "mean reward (100 episodes) 1708.700000\n",
      "best mean reward 1987.900000\n",
      "running time 54884.026072\n",
      "Train_EnvstepsSoFar : 8640001\n",
      "Train_AverageReturn : 1708.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54884.02607226372\n",
      "Training Loss : 1.236031174659729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8650001\n",
      "mean reward (100 episodes) 1788.400000\n",
      "best mean reward 1987.900000\n",
      "running time 54947.279771\n",
      "Train_EnvstepsSoFar : 8650001\n",
      "Train_AverageReturn : 1788.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 54947.27977132797\n",
      "Training Loss : 0.26862335205078125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8660001\n",
      "mean reward (100 episodes) 1837.600000\n",
      "best mean reward 1987.900000\n",
      "running time 55010.882381\n",
      "Train_EnvstepsSoFar : 8660001\n",
      "Train_AverageReturn : 1837.6\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55010.88238096237\n",
      "Training Loss : 0.574458122253418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8670001\n",
      "mean reward (100 episodes) 1874.700000\n",
      "best mean reward 1987.900000\n",
      "running time 55074.414109\n",
      "Train_EnvstepsSoFar : 8670001\n",
      "Train_AverageReturn : 1874.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55074.41410946846\n",
      "Training Loss : 0.45300567150115967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8680001\n",
      "mean reward (100 episodes) 1917.000000\n",
      "best mean reward 1987.900000\n",
      "running time 55137.980068\n",
      "Train_EnvstepsSoFar : 8680001\n",
      "Train_AverageReturn : 1917.0\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55137.98006772995\n",
      "Training Loss : 0.2217424362897873\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8690001\n",
      "mean reward (100 episodes) 1852.300000\n",
      "best mean reward 1987.900000\n",
      "running time 55201.309839\n",
      "Train_EnvstepsSoFar : 8690001\n",
      "Train_AverageReturn : 1852.3\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55201.309839487076\n",
      "Training Loss : 0.09830380976200104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8700001\n",
      "mean reward (100 episodes) 1891.100000\n",
      "best mean reward 1987.900000\n",
      "running time 55265.371351\n",
      "Train_EnvstepsSoFar : 8700001\n",
      "Train_AverageReturn : 1891.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55265.37135076523\n",
      "Training Loss : 0.3250236511230469\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8710001\n",
      "mean reward (100 episodes) 1904.500000\n",
      "best mean reward 1987.900000\n",
      "running time 55330.154259\n",
      "Train_EnvstepsSoFar : 8710001\n",
      "Train_AverageReturn : 1904.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55330.15425872803\n",
      "Training Loss : 0.8515579104423523\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8720001\n",
      "mean reward (100 episodes) 1858.900000\n",
      "best mean reward 1987.900000\n",
      "running time 55396.112056\n",
      "Train_EnvstepsSoFar : 8720001\n",
      "Train_AverageReturn : 1858.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55396.11205601692\n",
      "Training Loss : 0.219563290476799\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8730001\n",
      "mean reward (100 episodes) 1795.800000\n",
      "best mean reward 1987.900000\n",
      "running time 55459.765009\n",
      "Train_EnvstepsSoFar : 8730001\n",
      "Train_AverageReturn : 1795.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55459.76500916481\n",
      "Training Loss : 0.6994940042495728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8740001\n",
      "mean reward (100 episodes) 1816.700000\n",
      "best mean reward 1987.900000\n",
      "running time 55523.795048\n",
      "Train_EnvstepsSoFar : 8740001\n",
      "Train_AverageReturn : 1816.7\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55523.79504799843\n",
      "Training Loss : 0.1595948040485382\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8750001\n",
      "mean reward (100 episodes) 1846.100000\n",
      "best mean reward 1987.900000\n",
      "running time 55588.110482\n",
      "Train_EnvstepsSoFar : 8750001\n",
      "Train_AverageReturn : 1846.1\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55588.1104824543\n",
      "Training Loss : 0.1044241413474083\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8760001\n",
      "mean reward (100 episodes) 1883.800000\n",
      "best mean reward 1987.900000\n",
      "running time 55653.255239\n",
      "Train_EnvstepsSoFar : 8760001\n",
      "Train_AverageReturn : 1883.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55653.25523900986\n",
      "Training Loss : 0.15863767266273499\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8770001\n",
      "mean reward (100 episodes) 1846.500000\n",
      "best mean reward 1987.900000\n",
      "running time 55717.219811\n",
      "Train_EnvstepsSoFar : 8770001\n",
      "Train_AverageReturn : 1846.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55717.219811201096\n",
      "Training Loss : 0.1474831998348236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8780001\n",
      "mean reward (100 episodes) 1826.900000\n",
      "best mean reward 1987.900000\n",
      "running time 55780.655583\n",
      "Train_EnvstepsSoFar : 8780001\n",
      "Train_AverageReturn : 1826.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55780.65558338165\n",
      "Training Loss : 0.12838900089263916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8790001\n",
      "mean reward (100 episodes) 1844.900000\n",
      "best mean reward 1987.900000\n",
      "running time 55844.007681\n",
      "Train_EnvstepsSoFar : 8790001\n",
      "Train_AverageReturn : 1844.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55844.007680654526\n",
      "Training Loss : 0.14015066623687744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8800001\n",
      "mean reward (100 episodes) 1845.900000\n",
      "best mean reward 1987.900000\n",
      "running time 55907.507775\n",
      "Train_EnvstepsSoFar : 8800001\n",
      "Train_AverageReturn : 1845.9\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55907.50777506828\n",
      "Training Loss : 0.8568689823150635\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8810001\n",
      "mean reward (100 episodes) 1857.800000\n",
      "best mean reward 1987.900000\n",
      "running time 55970.857795\n",
      "Train_EnvstepsSoFar : 8810001\n",
      "Train_AverageReturn : 1857.8\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 55970.85779547691\n",
      "Training Loss : 0.21346712112426758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8820001\n",
      "mean reward (100 episodes) 1814.500000\n",
      "best mean reward 1987.900000\n",
      "running time 56034.590929\n",
      "Train_EnvstepsSoFar : 8820001\n",
      "Train_AverageReturn : 1814.5\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 56034.59092903137\n",
      "Training Loss : 0.24615265429019928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8830001\n",
      "mean reward (100 episodes) 1884.400000\n",
      "best mean reward 1987.900000\n",
      "running time 56097.999202\n",
      "Train_EnvstepsSoFar : 8830001\n",
      "Train_AverageReturn : 1884.4\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 56097.99920153618\n",
      "Training Loss : 0.1741563379764557\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8840001\n",
      "mean reward (100 episodes) 1932.200000\n",
      "best mean reward 1987.900000\n",
      "running time 56161.065516\n",
      "Train_EnvstepsSoFar : 8840001\n",
      "Train_AverageReturn : 1932.2\n",
      "Train_BestReturn : 1987.9\n",
      "TimeSinceStart : 56161.065515995026\n",
      "Training Loss : 0.15696673095226288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8850001\n",
      "mean reward (100 episodes) 1988.100000\n",
      "best mean reward 1988.100000\n",
      "running time 56224.187568\n",
      "Train_EnvstepsSoFar : 8850001\n",
      "Train_AverageReturn : 1988.1\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56224.187567949295\n",
      "Training Loss : 1.5972497463226318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8860001\n",
      "mean reward (100 episodes) 1946.800000\n",
      "best mean reward 1988.100000\n",
      "running time 56287.717786\n",
      "Train_EnvstepsSoFar : 8860001\n",
      "Train_AverageReturn : 1946.8\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56287.717786073685\n",
      "Training Loss : 0.25983384251594543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8870001\n",
      "mean reward (100 episodes) 1882.200000\n",
      "best mean reward 1988.100000\n",
      "running time 56350.873816\n",
      "Train_EnvstepsSoFar : 8870001\n",
      "Train_AverageReturn : 1882.2\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56350.8738155365\n",
      "Training Loss : 0.20006579160690308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8880001\n",
      "mean reward (100 episodes) 1856.200000\n",
      "best mean reward 1988.100000\n",
      "running time 56414.137293\n",
      "Train_EnvstepsSoFar : 8880001\n",
      "Train_AverageReturn : 1856.2\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56414.13729310036\n",
      "Training Loss : 0.18496929109096527\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8890001\n",
      "mean reward (100 episodes) 1853.100000\n",
      "best mean reward 1988.100000\n",
      "running time 56478.042661\n",
      "Train_EnvstepsSoFar : 8890001\n",
      "Train_AverageReturn : 1853.1\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56478.04266119003\n",
      "Training Loss : 0.6964561343193054\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8900001\n",
      "mean reward (100 episodes) 1833.900000\n",
      "best mean reward 1988.100000\n",
      "running time 56541.567677\n",
      "Train_EnvstepsSoFar : 8900001\n",
      "Train_AverageReturn : 1833.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56541.56767654419\n",
      "Training Loss : 0.20697329938411713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8910001\n",
      "mean reward (100 episodes) 1806.300000\n",
      "best mean reward 1988.100000\n",
      "running time 56604.631285\n",
      "Train_EnvstepsSoFar : 8910001\n",
      "Train_AverageReturn : 1806.3\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56604.631285429\n",
      "Training Loss : 0.2216593325138092\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8920001\n",
      "mean reward (100 episodes) 1769.800000\n",
      "best mean reward 1988.100000\n",
      "running time 56672.837768\n",
      "Train_EnvstepsSoFar : 8920001\n",
      "Train_AverageReturn : 1769.8\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56672.83776783943\n",
      "Training Loss : 0.9770776033401489\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8930001\n",
      "mean reward (100 episodes) 1895.900000\n",
      "best mean reward 1988.100000\n",
      "running time 56744.643799\n",
      "Train_EnvstepsSoFar : 8930001\n",
      "Train_AverageReturn : 1895.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56744.64379906654\n",
      "Training Loss : 0.15921683609485626\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8940001\n",
      "mean reward (100 episodes) 1913.900000\n",
      "best mean reward 1988.100000\n",
      "running time 56811.190212\n",
      "Train_EnvstepsSoFar : 8940001\n",
      "Train_AverageReturn : 1913.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56811.190212488174\n",
      "Training Loss : 0.12064012885093689\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8950001\n",
      "mean reward (100 episodes) 1936.900000\n",
      "best mean reward 1988.100000\n",
      "running time 56875.022738\n",
      "Train_EnvstepsSoFar : 8950001\n",
      "Train_AverageReturn : 1936.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56875.02273821831\n",
      "Training Loss : 0.1869519054889679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8960001\n",
      "mean reward (100 episodes) 1869.500000\n",
      "best mean reward 1988.100000\n",
      "running time 56938.949974\n",
      "Train_EnvstepsSoFar : 8960001\n",
      "Train_AverageReturn : 1869.5\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 56938.94997382164\n",
      "Training Loss : 0.35989341139793396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8970001\n",
      "mean reward (100 episodes) 1878.100000\n",
      "best mean reward 1988.100000\n",
      "running time 57002.492931\n",
      "Train_EnvstepsSoFar : 8970001\n",
      "Train_AverageReturn : 1878.1\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57002.49293136597\n",
      "Training Loss : 0.4453933835029602\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8980001\n",
      "mean reward (100 episodes) 1865.900000\n",
      "best mean reward 1988.100000\n",
      "running time 57066.667779\n",
      "Train_EnvstepsSoFar : 8980001\n",
      "Train_AverageReturn : 1865.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57066.66777896881\n",
      "Training Loss : 0.11891649663448334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 8990001\n",
      "mean reward (100 episodes) 1891.600000\n",
      "best mean reward 1988.100000\n",
      "running time 57132.034068\n",
      "Train_EnvstepsSoFar : 8990001\n",
      "Train_AverageReturn : 1891.6\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57132.034068107605\n",
      "Training Loss : 0.8158764243125916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9000001\n",
      "mean reward (100 episodes) 1931.200000\n",
      "best mean reward 1988.100000\n",
      "running time 57204.320667\n",
      "Train_EnvstepsSoFar : 9000001\n",
      "Train_AverageReturn : 1931.2\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57204.32066679001\n",
      "Training Loss : 0.22036071121692657\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9010001\n",
      "mean reward (100 episodes) 1884.300000\n",
      "best mean reward 1988.100000\n",
      "running time 57268.451403\n",
      "Train_EnvstepsSoFar : 9010001\n",
      "Train_AverageReturn : 1884.3\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57268.451402664185\n",
      "Training Loss : 0.5441548228263855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9020001\n",
      "mean reward (100 episodes) 1856.900000\n",
      "best mean reward 1988.100000\n",
      "running time 57333.000439\n",
      "Train_EnvstepsSoFar : 9020001\n",
      "Train_AverageReturn : 1856.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57333.000438690186\n",
      "Training Loss : 0.9864837527275085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9030001\n",
      "mean reward (100 episodes) 1808.600000\n",
      "best mean reward 1988.100000\n",
      "running time 57396.952471\n",
      "Train_EnvstepsSoFar : 9030001\n",
      "Train_AverageReturn : 1808.6\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57396.95247077942\n",
      "Training Loss : 0.07662632316350937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9040001\n",
      "mean reward (100 episodes) 1834.900000\n",
      "best mean reward 1988.100000\n",
      "running time 57462.276396\n",
      "Train_EnvstepsSoFar : 9040001\n",
      "Train_AverageReturn : 1834.9\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57462.27639627457\n",
      "Training Loss : 0.2738056778907776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9050001\n",
      "mean reward (100 episodes) 1835.200000\n",
      "best mean reward 1988.100000\n",
      "running time 57532.945020\n",
      "Train_EnvstepsSoFar : 9050001\n",
      "Train_AverageReturn : 1835.2\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57532.94502043724\n",
      "Training Loss : 1.12030029296875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9060001\n",
      "mean reward (100 episodes) 1862.300000\n",
      "best mean reward 1988.100000\n",
      "running time 57599.719227\n",
      "Train_EnvstepsSoFar : 9060001\n",
      "Train_AverageReturn : 1862.3\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57599.719227313995\n",
      "Training Loss : 0.21584708988666534\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9070001\n",
      "mean reward (100 episodes) 1897.500000\n",
      "best mean reward 1988.100000\n",
      "running time 57663.825336\n",
      "Train_EnvstepsSoFar : 9070001\n",
      "Train_AverageReturn : 1897.5\n",
      "Train_BestReturn : 1988.1\n",
      "TimeSinceStart : 57663.825335502625\n",
      "Training Loss : 0.11077547818422318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9080001\n",
      "mean reward (100 episodes) 2001.900000\n",
      "best mean reward 2001.900000\n",
      "running time 57727.146705\n",
      "Train_EnvstepsSoFar : 9080001\n",
      "Train_AverageReturn : 2001.9\n",
      "Train_BestReturn : 2001.9\n",
      "TimeSinceStart : 57727.14670467377\n",
      "Training Loss : 0.11723288148641586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9090001\n",
      "mean reward (100 episodes) 2026.100000\n",
      "best mean reward 2026.100000\n",
      "running time 57789.751489\n",
      "Train_EnvstepsSoFar : 9090001\n",
      "Train_AverageReturn : 2026.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 57789.75148868561\n",
      "Training Loss : 1.4519973993301392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9100001\n",
      "mean reward (100 episodes) 2024.700000\n",
      "best mean reward 2026.100000\n",
      "running time 57852.229121\n",
      "Train_EnvstepsSoFar : 9100001\n",
      "Train_AverageReturn : 2024.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 57852.22912096977\n",
      "Training Loss : 0.5587643980979919\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9110001\n",
      "mean reward (100 episodes) 1962.000000\n",
      "best mean reward 2026.100000\n",
      "running time 57915.015679\n",
      "Train_EnvstepsSoFar : 9110001\n",
      "Train_AverageReturn : 1962.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 57915.0156788826\n",
      "Training Loss : 0.17596258223056793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9120001\n",
      "mean reward (100 episodes) 1947.800000\n",
      "best mean reward 2026.100000\n",
      "running time 57979.743288\n",
      "Train_EnvstepsSoFar : 9120001\n",
      "Train_AverageReturn : 1947.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 57979.743287563324\n",
      "Training Loss : 0.18903987109661102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9130001\n",
      "mean reward (100 episodes) 2012.300000\n",
      "best mean reward 2026.100000\n",
      "running time 58042.842592\n",
      "Train_EnvstepsSoFar : 9130001\n",
      "Train_AverageReturn : 2012.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58042.84259176254\n",
      "Training Loss : 1.4815207719802856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9140001\n",
      "mean reward (100 episodes) 2007.500000\n",
      "best mean reward 2026.100000\n",
      "running time 58106.947636\n",
      "Train_EnvstepsSoFar : 9140001\n",
      "Train_AverageReturn : 2007.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58106.947635650635\n",
      "Training Loss : 0.425802618265152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9150001\n",
      "mean reward (100 episodes) 2023.400000\n",
      "best mean reward 2026.100000\n",
      "running time 58171.785887\n",
      "Train_EnvstepsSoFar : 9150001\n",
      "Train_AverageReturn : 2023.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58171.785887002945\n",
      "Training Loss : 0.17043650150299072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9160001\n",
      "mean reward (100 episodes) 1949.000000\n",
      "best mean reward 2026.100000\n",
      "running time 58241.507341\n",
      "Train_EnvstepsSoFar : 9160001\n",
      "Train_AverageReturn : 1949.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58241.50734066963\n",
      "Training Loss : 0.21632111072540283\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9170001\n",
      "mean reward (100 episodes) 1971.400000\n",
      "best mean reward 2026.100000\n",
      "running time 58304.867945\n",
      "Train_EnvstepsSoFar : 9170001\n",
      "Train_AverageReturn : 1971.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58304.86794543266\n",
      "Training Loss : 0.15678735077381134\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9180001\n",
      "mean reward (100 episodes) 1921.100000\n",
      "best mean reward 2026.100000\n",
      "running time 58368.191060\n",
      "Train_EnvstepsSoFar : 9180001\n",
      "Train_AverageReturn : 1921.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58368.191059589386\n",
      "Training Loss : 1.6043314933776855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9190001\n",
      "mean reward (100 episodes) 1889.300000\n",
      "best mean reward 2026.100000\n",
      "running time 58431.571323\n",
      "Train_EnvstepsSoFar : 9190001\n",
      "Train_AverageReturn : 1889.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58431.57132315636\n",
      "Training Loss : 0.40036365389823914\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9200001\n",
      "mean reward (100 episodes) 1894.200000\n",
      "best mean reward 2026.100000\n",
      "running time 58494.649225\n",
      "Train_EnvstepsSoFar : 9200001\n",
      "Train_AverageReturn : 1894.2\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58494.649225234985\n",
      "Training Loss : 0.2349773645401001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9210001\n",
      "mean reward (100 episodes) 1871.400000\n",
      "best mean reward 2026.100000\n",
      "running time 58557.151340\n",
      "Train_EnvstepsSoFar : 9210001\n",
      "Train_AverageReturn : 1871.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58557.15134000778\n",
      "Training Loss : 0.07842445373535156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9220001\n",
      "mean reward (100 episodes) 1899.900000\n",
      "best mean reward 2026.100000\n",
      "running time 58620.394464\n",
      "Train_EnvstepsSoFar : 9220001\n",
      "Train_AverageReturn : 1899.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58620.39446353912\n",
      "Training Loss : 0.6923502087593079\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9230001\n",
      "mean reward (100 episodes) 1890.900000\n",
      "best mean reward 2026.100000\n",
      "running time 58683.158580\n",
      "Train_EnvstepsSoFar : 9230001\n",
      "Train_AverageReturn : 1890.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58683.15858006477\n",
      "Training Loss : 0.12426890432834625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9240001\n",
      "mean reward (100 episodes) 1864.900000\n",
      "best mean reward 2026.100000\n",
      "running time 58745.983217\n",
      "Train_EnvstepsSoFar : 9240001\n",
      "Train_AverageReturn : 1864.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58745.983216524124\n",
      "Training Loss : 0.12644323706626892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9250001\n",
      "mean reward (100 episodes) 1847.700000\n",
      "best mean reward 2026.100000\n",
      "running time 58815.746514\n",
      "Train_EnvstepsSoFar : 9250001\n",
      "Train_AverageReturn : 1847.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58815.74651384354\n",
      "Training Loss : 2.2342946529388428\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9260001\n",
      "mean reward (100 episodes) 1790.500000\n",
      "best mean reward 2026.100000\n",
      "running time 58880.382643\n",
      "Train_EnvstepsSoFar : 9260001\n",
      "Train_AverageReturn : 1790.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58880.38264322281\n",
      "Training Loss : 0.3336026072502136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9270001\n",
      "mean reward (100 episodes) 1833.500000\n",
      "best mean reward 2026.100000\n",
      "running time 58943.165729\n",
      "Train_EnvstepsSoFar : 9270001\n",
      "Train_AverageReturn : 1833.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 58943.16572856903\n",
      "Training Loss : 0.17727112770080566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9280001\n",
      "mean reward (100 episodes) 1817.900000\n",
      "best mean reward 2026.100000\n",
      "running time 59005.855511\n",
      "Train_EnvstepsSoFar : 9280001\n",
      "Train_AverageReturn : 1817.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59005.855511426926\n",
      "Training Loss : 0.20486485958099365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9290001\n",
      "mean reward (100 episodes) 1849.100000\n",
      "best mean reward 2026.100000\n",
      "running time 59075.829097\n",
      "Train_EnvstepsSoFar : 9290001\n",
      "Train_AverageReturn : 1849.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59075.829097270966\n",
      "Training Loss : 0.23613882064819336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9300001\n",
      "mean reward (100 episodes) 1843.500000\n",
      "best mean reward 2026.100000\n",
      "running time 59140.187402\n",
      "Train_EnvstepsSoFar : 9300001\n",
      "Train_AverageReturn : 1843.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59140.18740224838\n",
      "Training Loss : 0.14590130746364594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9310001\n",
      "mean reward (100 episodes) 1798.900000\n",
      "best mean reward 2026.100000\n",
      "running time 59204.620119\n",
      "Train_EnvstepsSoFar : 9310001\n",
      "Train_AverageReturn : 1798.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59204.62011885643\n",
      "Training Loss : 0.1596805602312088\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9320001\n",
      "mean reward (100 episodes) 1813.500000\n",
      "best mean reward 2026.100000\n",
      "running time 59268.211193\n",
      "Train_EnvstepsSoFar : 9320001\n",
      "Train_AverageReturn : 1813.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59268.2111928463\n",
      "Training Loss : 0.10077358782291412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9330001\n",
      "mean reward (100 episodes) 1799.600000\n",
      "best mean reward 2026.100000\n",
      "running time 59331.688949\n",
      "Train_EnvstepsSoFar : 9330001\n",
      "Train_AverageReturn : 1799.6\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59331.68894934654\n",
      "Training Loss : 0.3246128261089325\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9340001\n",
      "mean reward (100 episodes) 1828.000000\n",
      "best mean reward 2026.100000\n",
      "running time 59395.522722\n",
      "Train_EnvstepsSoFar : 9340001\n",
      "Train_AverageReturn : 1828.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59395.522722005844\n",
      "Training Loss : 0.43700656294822693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9350001\n",
      "mean reward (100 episodes) 1846.600000\n",
      "best mean reward 2026.100000\n",
      "running time 59458.379351\n",
      "Train_EnvstepsSoFar : 9350001\n",
      "Train_AverageReturn : 1846.6\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59458.37935137749\n",
      "Training Loss : 0.21496382355690002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9360001\n",
      "mean reward (100 episodes) 1811.000000\n",
      "best mean reward 2026.100000\n",
      "running time 59521.066869\n",
      "Train_EnvstepsSoFar : 9360001\n",
      "Train_AverageReturn : 1811.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59521.06686925888\n",
      "Training Loss : 2.491178274154663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9370001\n",
      "mean reward (100 episodes) 1845.300000\n",
      "best mean reward 2026.100000\n",
      "running time 59587.568953\n",
      "Train_EnvstepsSoFar : 9370001\n",
      "Train_AverageReturn : 1845.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59587.568952560425\n",
      "Training Loss : 0.6947852969169617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9380001\n",
      "mean reward (100 episodes) 1830.900000\n",
      "best mean reward 2026.100000\n",
      "running time 59652.503309\n",
      "Train_EnvstepsSoFar : 9380001\n",
      "Train_AverageReturn : 1830.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59652.50330901146\n",
      "Training Loss : 0.3831278383731842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9390001\n",
      "mean reward (100 episodes) 1852.700000\n",
      "best mean reward 2026.100000\n",
      "running time 59714.879847\n",
      "Train_EnvstepsSoFar : 9390001\n",
      "Train_AverageReturn : 1852.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59714.879846572876\n",
      "Training Loss : 0.6381279230117798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9400001\n",
      "mean reward (100 episodes) 1863.600000\n",
      "best mean reward 2026.100000\n",
      "running time 59777.804630\n",
      "Train_EnvstepsSoFar : 9400001\n",
      "Train_AverageReturn : 1863.6\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59777.804629802704\n",
      "Training Loss : 1.540885329246521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9410001\n",
      "mean reward (100 episodes) 1890.200000\n",
      "best mean reward 2026.100000\n",
      "running time 59840.613710\n",
      "Train_EnvstepsSoFar : 9410001\n",
      "Train_AverageReturn : 1890.2\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59840.61371040344\n",
      "Training Loss : 0.31311553716659546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9420001\n",
      "mean reward (100 episodes) 1897.100000\n",
      "best mean reward 2026.100000\n",
      "running time 59903.203818\n",
      "Train_EnvstepsSoFar : 9420001\n",
      "Train_AverageReturn : 1897.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59903.20381760597\n",
      "Training Loss : 0.20662054419517517\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9430001\n",
      "mean reward (100 episodes) 1921.100000\n",
      "best mean reward 2026.100000\n",
      "running time 59965.937022\n",
      "Train_EnvstepsSoFar : 9430001\n",
      "Train_AverageReturn : 1921.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 59965.93702197075\n",
      "Training Loss : 0.1670052707195282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9440001\n",
      "mean reward (100 episodes) 1951.500000\n",
      "best mean reward 2026.100000\n",
      "running time 60028.835357\n",
      "Train_EnvstepsSoFar : 9440001\n",
      "Train_AverageReturn : 1951.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60028.83535718918\n",
      "Training Loss : 0.2097906470298767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9450001\n",
      "mean reward (100 episodes) 1943.300000\n",
      "best mean reward 2026.100000\n",
      "running time 60091.609683\n",
      "Train_EnvstepsSoFar : 9450001\n",
      "Train_AverageReturn : 1943.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60091.609683036804\n",
      "Training Loss : 1.5207452774047852\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9460001\n",
      "mean reward (100 episodes) 1889.100000\n",
      "best mean reward 2026.100000\n",
      "running time 60154.005900\n",
      "Train_EnvstepsSoFar : 9460001\n",
      "Train_AverageReturn : 1889.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60154.00589966774\n",
      "Training Loss : 0.19931060075759888\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9470001\n",
      "mean reward (100 episodes) 1796.800000\n",
      "best mean reward 2026.100000\n",
      "running time 60216.425039\n",
      "Train_EnvstepsSoFar : 9470001\n",
      "Train_AverageReturn : 1796.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60216.42503905296\n",
      "Training Loss : 0.1312968134880066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9480001\n",
      "mean reward (100 episodes) 1841.000000\n",
      "best mean reward 2026.100000\n",
      "running time 60279.077298\n",
      "Train_EnvstepsSoFar : 9480001\n",
      "Train_AverageReturn : 1841.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60279.07729768753\n",
      "Training Loss : 0.24751996994018555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9490001\n",
      "mean reward (100 episodes) 1805.800000\n",
      "best mean reward 2026.100000\n",
      "running time 60341.551812\n",
      "Train_EnvstepsSoFar : 9490001\n",
      "Train_AverageReturn : 1805.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60341.551812171936\n",
      "Training Loss : 1.2109993696212769\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9500000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9500001\n",
      "mean reward (100 episodes) 1889.500000\n",
      "best mean reward 2026.100000\n",
      "running time 60404.245469\n",
      "Train_EnvstepsSoFar : 9500001\n",
      "Train_AverageReturn : 1889.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60404.24546933174\n",
      "Training Loss : 0.4168557822704315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9501000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9502000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9503000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9504000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9505000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9506000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9507000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9508000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9509000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9510000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9510001\n",
      "mean reward (100 episodes) 1897.300000\n",
      "best mean reward 2026.100000\n",
      "running time 60467.128631\n",
      "Train_EnvstepsSoFar : 9510001\n",
      "Train_AverageReturn : 1897.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60467.12863135338\n",
      "Training Loss : 1.5674010515213013\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9511000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9512000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9513000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9514000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9515000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9516000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9517000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9518000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9519000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9520000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9520001\n",
      "mean reward (100 episodes) 1928.400000\n",
      "best mean reward 2026.100000\n",
      "running time 60529.832575\n",
      "Train_EnvstepsSoFar : 9520001\n",
      "Train_AverageReturn : 1928.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60529.8325753212\n",
      "Training Loss : 1.5108240842819214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9521000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9522000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9523000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9524000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9525000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9526000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9527000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9528000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9529000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9530000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9530001\n",
      "mean reward (100 episodes) 1912.100000\n",
      "best mean reward 2026.100000\n",
      "running time 60592.825484\n",
      "Train_EnvstepsSoFar : 9530001\n",
      "Train_AverageReturn : 1912.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60592.82548427582\n",
      "Training Loss : 1.6035064458847046\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9531000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9532000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9533000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9534000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9535000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9536000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9537000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9538000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9539000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9540000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9540001\n",
      "mean reward (100 episodes) 1934.000000\n",
      "best mean reward 2026.100000\n",
      "running time 60655.312341\n",
      "Train_EnvstepsSoFar : 9540001\n",
      "Train_AverageReturn : 1934.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60655.312341451645\n",
      "Training Loss : 0.14865371584892273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9541000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9542000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9543000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9544000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9545000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9546000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9547000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9548000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9549000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9550000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9550001\n",
      "mean reward (100 episodes) 1955.900000\n",
      "best mean reward 2026.100000\n",
      "running time 60717.762108\n",
      "Train_EnvstepsSoFar : 9550001\n",
      "Train_AverageReturn : 1955.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60717.76210784912\n",
      "Training Loss : 0.08335850387811661\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9551000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9552000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9553000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9554000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9555000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9556000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9557000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9558000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9559000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9560000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9560001\n",
      "mean reward (100 episodes) 1900.700000\n",
      "best mean reward 2026.100000\n",
      "running time 60780.343506\n",
      "Train_EnvstepsSoFar : 9560001\n",
      "Train_AverageReturn : 1900.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60780.343505859375\n",
      "Training Loss : 0.24218147993087769\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9561000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9562000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9563000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9564000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9565000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9566000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9567000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9568000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9569000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9570000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9570001\n",
      "mean reward (100 episodes) 1890.000000\n",
      "best mean reward 2026.100000\n",
      "running time 60842.733786\n",
      "Train_EnvstepsSoFar : 9570001\n",
      "Train_AverageReturn : 1890.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60842.73378610611\n",
      "Training Loss : 0.15033648908138275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9571000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9572000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9573000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9574000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9575000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9576000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9577000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9578000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9579000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9580000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9580001\n",
      "mean reward (100 episodes) 1926.300000\n",
      "best mean reward 2026.100000\n",
      "running time 60905.460369\n",
      "Train_EnvstepsSoFar : 9580001\n",
      "Train_AverageReturn : 1926.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60905.46036887169\n",
      "Training Loss : 0.16858303546905518\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9581000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9582000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9583000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9584000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9585000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9586000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9587000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9588000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9589000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9590000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9590001\n",
      "mean reward (100 episodes) 1946.500000\n",
      "best mean reward 2026.100000\n",
      "running time 60967.940996\n",
      "Train_EnvstepsSoFar : 9590001\n",
      "Train_AverageReturn : 1946.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 60967.94099640846\n",
      "Training Loss : 0.13961121439933777\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9591000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9592000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9593000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9594000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9595000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9596000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9597000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9598000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9599000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9600000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9600001\n",
      "mean reward (100 episodes) 1954.000000\n",
      "best mean reward 2026.100000\n",
      "running time 61030.407671\n",
      "Train_EnvstepsSoFar : 9600001\n",
      "Train_AverageReturn : 1954.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61030.40767145157\n",
      "Training Loss : 0.22325971722602844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9601000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9602000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9603000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9604000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9605000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9606000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9607000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9608000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9609000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9610000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9610001\n",
      "mean reward (100 episodes) 1954.400000\n",
      "best mean reward 2026.100000\n",
      "running time 61092.928631\n",
      "Train_EnvstepsSoFar : 9610001\n",
      "Train_AverageReturn : 1954.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61092.92863059044\n",
      "Training Loss : 0.9603539109230042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9611000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9612000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9613000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9614000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9615000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9616000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9617000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9618000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9619000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9620000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9620001\n",
      "mean reward (100 episodes) 2011.300000\n",
      "best mean reward 2026.100000\n",
      "running time 61155.473154\n",
      "Train_EnvstepsSoFar : 9620001\n",
      "Train_AverageReturn : 2011.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61155.473153591156\n",
      "Training Loss : 0.17070776224136353\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9621000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9622000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9623000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9624000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9625000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9626000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9627000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9628000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9629000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9630000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9630001\n",
      "mean reward (100 episodes) 1945.500000\n",
      "best mean reward 2026.100000\n",
      "running time 61217.947035\n",
      "Train_EnvstepsSoFar : 9630001\n",
      "Train_AverageReturn : 1945.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61217.9470345974\n",
      "Training Loss : 0.13503709435462952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9631000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9632000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9633000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9634000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9635000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9636000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9637000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9638000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9639000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9640000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9640001\n",
      "mean reward (100 episodes) 1945.800000\n",
      "best mean reward 2026.100000\n",
      "running time 61280.244803\n",
      "Train_EnvstepsSoFar : 9640001\n",
      "Train_AverageReturn : 1945.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61280.24480295181\n",
      "Training Loss : 1.0063550472259521\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9641000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9642000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9643000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9644000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9645000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9646000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9647000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9648000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9649000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9650000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9650001\n",
      "mean reward (100 episodes) 1904.500000\n",
      "best mean reward 2026.100000\n",
      "running time 61342.843714\n",
      "Train_EnvstepsSoFar : 9650001\n",
      "Train_AverageReturn : 1904.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61342.84371447563\n",
      "Training Loss : 0.1487712264060974\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9651000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9652000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9653000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9654000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9655000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9656000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9657000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9658000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9659000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9660000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9660001\n",
      "mean reward (100 episodes) 1950.800000\n",
      "best mean reward 2026.100000\n",
      "running time 61405.609043\n",
      "Train_EnvstepsSoFar : 9660001\n",
      "Train_AverageReturn : 1950.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61405.60904312134\n",
      "Training Loss : 0.7586578726768494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9661000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9662000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9663000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9664000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9665000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9666000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9667000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9668000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9669000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9670000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9670001\n",
      "mean reward (100 episodes) 1994.400000\n",
      "best mean reward 2026.100000\n",
      "running time 61467.773544\n",
      "Train_EnvstepsSoFar : 9670001\n",
      "Train_AverageReturn : 1994.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61467.77354359627\n",
      "Training Loss : 0.3427865505218506\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9671000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9672000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9673000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9674000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9675000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9676000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9677000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9678000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9679000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9680000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9680001\n",
      "mean reward (100 episodes) 1937.900000\n",
      "best mean reward 2026.100000\n",
      "running time 61530.245353\n",
      "Train_EnvstepsSoFar : 9680001\n",
      "Train_AverageReturn : 1937.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61530.245352745056\n",
      "Training Loss : 0.3940189778804779\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9681000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9682000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9683000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9684000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9685000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9686000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9687000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9688000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9689000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9690000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9690001\n",
      "mean reward (100 episodes) 1896.800000\n",
      "best mean reward 2026.100000\n",
      "running time 61593.880440\n",
      "Train_EnvstepsSoFar : 9690001\n",
      "Train_AverageReturn : 1896.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61593.88043999672\n",
      "Training Loss : 0.9957877993583679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9691000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9692000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9693000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9694000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9695000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9696000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9697000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9698000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9699000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9700000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9700001\n",
      "mean reward (100 episodes) 1870.200000\n",
      "best mean reward 2026.100000\n",
      "running time 61656.279253\n",
      "Train_EnvstepsSoFar : 9700001\n",
      "Train_AverageReturn : 1870.2\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61656.27925348282\n",
      "Training Loss : 1.777289628982544\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9701000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9702000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9703000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9704000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9705000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9706000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9707000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9708000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9709000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9710000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9710001\n",
      "mean reward (100 episodes) 1861.300000\n",
      "best mean reward 2026.100000\n",
      "running time 61718.762875\n",
      "Train_EnvstepsSoFar : 9710001\n",
      "Train_AverageReturn : 1861.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61718.76287531853\n",
      "Training Loss : 0.09650492668151855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9711000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9712000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9713000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9714000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9715000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9716000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9717000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9718000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9719000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9720000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9720001\n",
      "mean reward (100 episodes) 1873.400000\n",
      "best mean reward 2026.100000\n",
      "running time 61781.756233\n",
      "Train_EnvstepsSoFar : 9720001\n",
      "Train_AverageReturn : 1873.4\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61781.75623345375\n",
      "Training Loss : 0.2042403519153595\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9721000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9722000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9723000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9724000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9725000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9726000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9727000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9728000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9729000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9730000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9730001\n",
      "mean reward (100 episodes) 1878.900000\n",
      "best mean reward 2026.100000\n",
      "running time 61844.229160\n",
      "Train_EnvstepsSoFar : 9730001\n",
      "Train_AverageReturn : 1878.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61844.22915959358\n",
      "Training Loss : 0.1788404881954193\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9731000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9732000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9733000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9734000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9735000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9736000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9737000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9738000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9739000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9740000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9740001\n",
      "mean reward (100 episodes) 1809.000000\n",
      "best mean reward 2026.100000\n",
      "running time 61907.111045\n",
      "Train_EnvstepsSoFar : 9740001\n",
      "Train_AverageReturn : 1809.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61907.11104488373\n",
      "Training Loss : 0.8533241748809814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9741000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9742000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9743000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9744000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9745000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9746000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9747000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9748000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9749000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9750000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9750001\n",
      "mean reward (100 episodes) 1852.300000\n",
      "best mean reward 2026.100000\n",
      "running time 61969.583506\n",
      "Train_EnvstepsSoFar : 9750001\n",
      "Train_AverageReturn : 1852.3\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 61969.58350610733\n",
      "Training Loss : 1.1135289669036865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9751000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9752000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9753000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9754000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9755000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9756000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9757000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9758000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9759000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9760000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9760001\n",
      "mean reward (100 episodes) 1853.200000\n",
      "best mean reward 2026.100000\n",
      "running time 62031.905986\n",
      "Train_EnvstepsSoFar : 9760001\n",
      "Train_AverageReturn : 1853.2\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62031.905985832214\n",
      "Training Loss : 0.6866843700408936\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9761000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9762000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9763000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9764000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9765000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9766000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9767000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9768000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9769000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9770000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9770001\n",
      "mean reward (100 episodes) 1949.600000\n",
      "best mean reward 2026.100000\n",
      "running time 62094.358651\n",
      "Train_EnvstepsSoFar : 9770001\n",
      "Train_AverageReturn : 1949.6\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62094.35865068436\n",
      "Training Loss : 2.131584644317627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9771000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9772000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9773000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9774000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9775000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9776000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9777000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9778000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9779000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9780000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9780001\n",
      "mean reward (100 episodes) 1924.700000\n",
      "best mean reward 2026.100000\n",
      "running time 62156.905006\n",
      "Train_EnvstepsSoFar : 9780001\n",
      "Train_AverageReturn : 1924.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62156.90500640869\n",
      "Training Loss : 1.613621711730957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9781000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9782000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9783000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9784000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9785000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9786000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9787000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9788000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9789000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9790000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9790001\n",
      "mean reward (100 episodes) 1934.700000\n",
      "best mean reward 2026.100000\n",
      "running time 62219.580471\n",
      "Train_EnvstepsSoFar : 9790001\n",
      "Train_AverageReturn : 1934.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62219.58047056198\n",
      "Training Loss : 0.10501450300216675\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9791000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9792000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9793000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9794000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9795000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9796000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9797000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9798000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9799000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9800000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9800001\n",
      "mean reward (100 episodes) 1842.000000\n",
      "best mean reward 2026.100000\n",
      "running time 62282.202937\n",
      "Train_EnvstepsSoFar : 9800001\n",
      "Train_AverageReturn : 1842.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62282.20293664932\n",
      "Training Loss : 0.19599659740924835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9801000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9802000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9803000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9804000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9805000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9806000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9807000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9808000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9809000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9810000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9810001\n",
      "mean reward (100 episodes) 1866.900000\n",
      "best mean reward 2026.100000\n",
      "running time 62344.814465\n",
      "Train_EnvstepsSoFar : 9810001\n",
      "Train_AverageReturn : 1866.9\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62344.81446528435\n",
      "Training Loss : 0.10560913383960724\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9811000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9812000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9813000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9814000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9815000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9816000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9817000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9818000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9819000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9820000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9820001\n",
      "mean reward (100 episodes) 1876.800000\n",
      "best mean reward 2026.100000\n",
      "running time 62407.306244\n",
      "Train_EnvstepsSoFar : 9820001\n",
      "Train_AverageReturn : 1876.8\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62407.30624437332\n",
      "Training Loss : 0.16368988156318665\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9821000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9822000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9823000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9824000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9825000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9826000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9827000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9828000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9829000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9830000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9830001\n",
      "mean reward (100 episodes) 1923.700000\n",
      "best mean reward 2026.100000\n",
      "running time 62470.164210\n",
      "Train_EnvstepsSoFar : 9830001\n",
      "Train_AverageReturn : 1923.7\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62470.16420960426\n",
      "Training Loss : 0.26373913884162903\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9831000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9832000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9833000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9834000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9835000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9836000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9837000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9838000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9839000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9840000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9840001\n",
      "mean reward (100 episodes) 1930.100000\n",
      "best mean reward 2026.100000\n",
      "running time 62532.653134\n",
      "Train_EnvstepsSoFar : 9840001\n",
      "Train_AverageReturn : 1930.1\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62532.65313386917\n",
      "Training Loss : 0.18049490451812744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9841000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9842000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9843000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9844000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9845000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9846000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9847000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9848000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9849000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9850000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9850001\n",
      "mean reward (100 episodes) 1973.500000\n",
      "best mean reward 2026.100000\n",
      "running time 62595.100430\n",
      "Train_EnvstepsSoFar : 9850001\n",
      "Train_AverageReturn : 1973.5\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62595.10043048859\n",
      "Training Loss : 1.622280240058899\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9851000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9852000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9853000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9854000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9855000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9856000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9857000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9858000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9859000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9860000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9860001\n",
      "mean reward (100 episodes) 1980.000000\n",
      "best mean reward 2026.100000\n",
      "running time 62657.520819\n",
      "Train_EnvstepsSoFar : 9860001\n",
      "Train_AverageReturn : 1980.0\n",
      "Train_BestReturn : 2026.1\n",
      "TimeSinceStart : 62657.52081871033\n",
      "Training Loss : 0.20427224040031433\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9861000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9862000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9863000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9864000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9865000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9866000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9867000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9868000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9869000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9870000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9870001\n",
      "mean reward (100 episodes) 2091.900000\n",
      "best mean reward 2091.900000\n",
      "running time 62720.189940\n",
      "Train_EnvstepsSoFar : 9870001\n",
      "Train_AverageReturn : 2091.9\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 62720.189940452576\n",
      "Training Loss : 0.12190461158752441\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9871000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9872000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9873000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9874000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9875000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9876000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9877000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9878000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9879000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9880000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9880001\n",
      "mean reward (100 episodes) 2069.300000\n",
      "best mean reward 2091.900000\n",
      "running time 62782.645210\n",
      "Train_EnvstepsSoFar : 9880001\n",
      "Train_AverageReturn : 2069.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 62782.64521026611\n",
      "Training Loss : 0.5966230630874634\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9881000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9882000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9883000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9884000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9885000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9886000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9887000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9888000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9889000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9890000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9890001\n",
      "mean reward (100 episodes) 2008.300000\n",
      "best mean reward 2091.900000\n",
      "running time 62845.196257\n",
      "Train_EnvstepsSoFar : 9890001\n",
      "Train_AverageReturn : 2008.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 62845.19625711441\n",
      "Training Loss : 0.12804155051708221\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9891000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9892000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9893000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9894000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9895000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9896000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9897000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9898000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9899000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9900000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9900001\n",
      "mean reward (100 episodes) 1988.500000\n",
      "best mean reward 2091.900000\n",
      "running time 62908.609084\n",
      "Train_EnvstepsSoFar : 9900001\n",
      "Train_AverageReturn : 1988.5\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 62908.609083890915\n",
      "Training Loss : 0.1005738228559494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9901000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9902000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9903000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9904000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9905000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9906000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9907000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9908000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9909000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9910000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9910001\n",
      "mean reward (100 episodes) 1956.500000\n",
      "best mean reward 2091.900000\n",
      "running time 62973.100147\n",
      "Train_EnvstepsSoFar : 9910001\n",
      "Train_AverageReturn : 1956.5\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 62973.100147247314\n",
      "Training Loss : 0.08321860432624817\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9911000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9912000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9913000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9914000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9915000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9916000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9917000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9918000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9919000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9920000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9920001\n",
      "mean reward (100 episodes) 1956.300000\n",
      "best mean reward 2091.900000\n",
      "running time 63037.965683\n",
      "Train_EnvstepsSoFar : 9920001\n",
      "Train_AverageReturn : 1956.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63037.965683460236\n",
      "Training Loss : 0.21063847839832306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9921000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9922000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9923000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9924000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9925000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9926000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9927000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9928000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9929000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9930000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9930001\n",
      "mean reward (100 episodes) 1961.400000\n",
      "best mean reward 2091.900000\n",
      "running time 63102.397716\n",
      "Train_EnvstepsSoFar : 9930001\n",
      "Train_AverageReturn : 1961.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63102.39771580696\n",
      "Training Loss : 0.09978248924016953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9931000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9932000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9933000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9934000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9935000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9936000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9937000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9938000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9939000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9940000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9940001\n",
      "mean reward (100 episodes) 1989.800000\n",
      "best mean reward 2091.900000\n",
      "running time 63166.858535\n",
      "Train_EnvstepsSoFar : 9940001\n",
      "Train_AverageReturn : 1989.8\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63166.85853481293\n",
      "Training Loss : 0.14331644773483276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9941000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9942000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9943000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9944000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9945000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9946000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9947000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9948000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9949000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9950000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9950001\n",
      "mean reward (100 episodes) 1941.100000\n",
      "best mean reward 2091.900000\n",
      "running time 63232.535435\n",
      "Train_EnvstepsSoFar : 9950001\n",
      "Train_AverageReturn : 1941.1\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63232.5354347229\n",
      "Training Loss : 1.5911277532577515\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9951000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9952000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9953000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9954000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9955000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9956000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9957000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9958000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9959000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9960000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9960001\n",
      "mean reward (100 episodes) 1887.600000\n",
      "best mean reward 2091.900000\n",
      "running time 63302.046279\n",
      "Train_EnvstepsSoFar : 9960001\n",
      "Train_AverageReturn : 1887.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63302.046278715134\n",
      "Training Loss : 0.13387401401996613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9961000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9962000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9963000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9964000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9965000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9966000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9967000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9968000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9969000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9970000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9970001\n",
      "mean reward (100 episodes) 1862.400000\n",
      "best mean reward 2091.900000\n",
      "running time 63375.466875\n",
      "Train_EnvstepsSoFar : 9970001\n",
      "Train_AverageReturn : 1862.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63375.46687531471\n",
      "Training Loss : 0.5731014609336853\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9971000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9972000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9973000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9974000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9975000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9976000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9977000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9978000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9979000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9980000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9980001\n",
      "mean reward (100 episodes) 1947.900000\n",
      "best mean reward 2091.900000\n",
      "running time 63444.247531\n",
      "Train_EnvstepsSoFar : 9980001\n",
      "Train_AverageReturn : 1947.9\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63444.247530698776\n",
      "Training Loss : 1.3065663576126099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9981000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9982000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9983000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9984000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9985000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9986000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9987000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9988000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9989000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9990000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 9990001\n",
      "mean reward (100 episodes) 1978.100000\n",
      "best mean reward 2091.900000\n",
      "running time 63511.217199\n",
      "Train_EnvstepsSoFar : 9990001\n",
      "Train_AverageReturn : 1978.1\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63511.21719908714\n",
      "Training Loss : 0.3972353935241699\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9991000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9992000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9993000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9994000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9995000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9996000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9997000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9998000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9999000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10000001\n",
      "mean reward (100 episodes) 1966.400000\n",
      "best mean reward 2091.900000\n",
      "running time 63577.198655\n",
      "Train_EnvstepsSoFar : 10000001\n",
      "Train_AverageReturn : 1966.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63577.1986553669\n",
      "Training Loss : 0.20068854093551636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10001000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10002000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10003000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10004000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10005000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10006000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10007000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10008000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10009000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10010000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10010001\n",
      "mean reward (100 episodes) 1954.200000\n",
      "best mean reward 2091.900000\n",
      "running time 63648.601257\n",
      "Train_EnvstepsSoFar : 10010001\n",
      "Train_AverageReturn : 1954.2\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63648.6012570858\n",
      "Training Loss : 0.19817590713500977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10011000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10012000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10013000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10014000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10015000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10016000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10017000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10018000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10019000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10020000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10020001\n",
      "mean reward (100 episodes) 1858.100000\n",
      "best mean reward 2091.900000\n",
      "running time 63716.030675\n",
      "Train_EnvstepsSoFar : 10020001\n",
      "Train_AverageReturn : 1858.1\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63716.03067469597\n",
      "Training Loss : 0.9149494767189026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10021000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10022000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10023000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10024000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10025000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10026000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10027000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10028000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10029000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10030000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10030001\n",
      "mean reward (100 episodes) 1933.200000\n",
      "best mean reward 2091.900000\n",
      "running time 63784.118423\n",
      "Train_EnvstepsSoFar : 10030001\n",
      "Train_AverageReturn : 1933.2\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63784.11842274666\n",
      "Training Loss : 0.1870829463005066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10031000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10032000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10033000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10034000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10035000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10036000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10037000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10038000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10039000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10040000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10040001\n",
      "mean reward (100 episodes) 1962.100000\n",
      "best mean reward 2091.900000\n",
      "running time 63850.971562\n",
      "Train_EnvstepsSoFar : 10040001\n",
      "Train_AverageReturn : 1962.1\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63850.97156190872\n",
      "Training Loss : 0.1793263554573059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10041000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10042000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10043000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10044000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10045000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10046000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10047000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10048000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10049000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10050000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10050001\n",
      "mean reward (100 episodes) 2025.600000\n",
      "best mean reward 2091.900000\n",
      "running time 63919.621965\n",
      "Train_EnvstepsSoFar : 10050001\n",
      "Train_AverageReturn : 2025.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63919.621965408325\n",
      "Training Loss : 0.44364678859710693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10051000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10052000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10053000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10054000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10055000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10056000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10057000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10058000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10059000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10060000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10060001\n",
      "mean reward (100 episodes) 2025.700000\n",
      "best mean reward 2091.900000\n",
      "running time 63985.629548\n",
      "Train_EnvstepsSoFar : 10060001\n",
      "Train_AverageReturn : 2025.7\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 63985.62954831123\n",
      "Training Loss : 1.5679337978363037\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10061000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10062000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10063000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10064000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10065000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10066000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10067000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10068000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10069000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10070000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10070001\n",
      "mean reward (100 episodes) 2031.400000\n",
      "best mean reward 2091.900000\n",
      "running time 64054.393632\n",
      "Train_EnvstepsSoFar : 10070001\n",
      "Train_AverageReturn : 2031.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64054.3936316967\n",
      "Training Loss : 0.09216535091400146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10071000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10072000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10073000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10074000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10075000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10076000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10077000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10078000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10079000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10080000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10080001\n",
      "mean reward (100 episodes) 2040.600000\n",
      "best mean reward 2091.900000\n",
      "running time 64126.809546\n",
      "Train_EnvstepsSoFar : 10080001\n",
      "Train_AverageReturn : 2040.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64126.80954647064\n",
      "Training Loss : 0.10340374708175659\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10081000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10082000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10083000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10084000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10085000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10086000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10087000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10088000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10089000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10090000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10090001\n",
      "mean reward (100 episodes) 1999.600000\n",
      "best mean reward 2091.900000\n",
      "running time 64195.845093\n",
      "Train_EnvstepsSoFar : 10090001\n",
      "Train_AverageReturn : 1999.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64195.84509277344\n",
      "Training Loss : 0.276980996131897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10091000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10092000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10093000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10094000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10095000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10096000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10097000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10098000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10099000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10100001\n",
      "mean reward (100 episodes) 1980.000000\n",
      "best mean reward 2091.900000\n",
      "running time 64262.455891\n",
      "Train_EnvstepsSoFar : 10100001\n",
      "Train_AverageReturn : 1980.0\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64262.455891132355\n",
      "Training Loss : 0.3431345820426941\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10110001\n",
      "mean reward (100 episodes) 1909.600000\n",
      "best mean reward 2091.900000\n",
      "running time 64327.485471\n",
      "Train_EnvstepsSoFar : 10110001\n",
      "Train_AverageReturn : 1909.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64327.48547124863\n",
      "Training Loss : 0.19911465048789978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10120001\n",
      "mean reward (100 episodes) 1905.300000\n",
      "best mean reward 2091.900000\n",
      "running time 64393.551903\n",
      "Train_EnvstepsSoFar : 10120001\n",
      "Train_AverageReturn : 1905.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64393.55190348625\n",
      "Training Loss : 1.2087657451629639\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10130001\n",
      "mean reward (100 episodes) 1918.000000\n",
      "best mean reward 2091.900000\n",
      "running time 64458.901541\n",
      "Train_EnvstepsSoFar : 10130001\n",
      "Train_AverageReturn : 1918.0\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64458.901540994644\n",
      "Training Loss : 0.15227696299552917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10140001\n",
      "mean reward (100 episodes) 1889.100000\n",
      "best mean reward 2091.900000\n",
      "running time 64524.605075\n",
      "Train_EnvstepsSoFar : 10140001\n",
      "Train_AverageReturn : 1889.1\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64524.605075359344\n",
      "Training Loss : 0.231230229139328\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10150001\n",
      "mean reward (100 episodes) 1881.900000\n",
      "best mean reward 2091.900000\n",
      "running time 64590.172350\n",
      "Train_EnvstepsSoFar : 10150001\n",
      "Train_AverageReturn : 1881.9\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64590.17234992981\n",
      "Training Loss : 0.36388343572616577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10160001\n",
      "mean reward (100 episodes) 1871.800000\n",
      "best mean reward 2091.900000\n",
      "running time 64655.476375\n",
      "Train_EnvstepsSoFar : 10160001\n",
      "Train_AverageReturn : 1871.8\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64655.476375341415\n",
      "Training Loss : 0.6424728035926819\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10170001\n",
      "mean reward (100 episodes) 1880.600000\n",
      "best mean reward 2091.900000\n",
      "running time 64721.037492\n",
      "Train_EnvstepsSoFar : 10170001\n",
      "Train_AverageReturn : 1880.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64721.03749155998\n",
      "Training Loss : 0.2303086221218109\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10180001\n",
      "mean reward (100 episodes) 1921.200000\n",
      "best mean reward 2091.900000\n",
      "running time 64787.729773\n",
      "Train_EnvstepsSoFar : 10180001\n",
      "Train_AverageReturn : 1921.2\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64787.72977280617\n",
      "Training Loss : 1.3745079040527344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10190001\n",
      "mean reward (100 episodes) 1898.800000\n",
      "best mean reward 2091.900000\n",
      "running time 64855.004640\n",
      "Train_EnvstepsSoFar : 10190001\n",
      "Train_AverageReturn : 1898.8\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64855.004640340805\n",
      "Training Loss : 0.16043654084205627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10200001\n",
      "mean reward (100 episodes) 1895.300000\n",
      "best mean reward 2091.900000\n",
      "running time 64921.677341\n",
      "Train_EnvstepsSoFar : 10200001\n",
      "Train_AverageReturn : 1895.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64921.67734122276\n",
      "Training Loss : 0.10075785219669342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10210001\n",
      "mean reward (100 episodes) 1916.700000\n",
      "best mean reward 2091.900000\n",
      "running time 64992.939612\n",
      "Train_EnvstepsSoFar : 10210001\n",
      "Train_AverageReturn : 1916.7\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 64992.93961215019\n",
      "Training Loss : 0.27979448437690735\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10220001\n",
      "mean reward (100 episodes) 1928.300000\n",
      "best mean reward 2091.900000\n",
      "running time 65058.880844\n",
      "Train_EnvstepsSoFar : 10220001\n",
      "Train_AverageReturn : 1928.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65058.88084387779\n",
      "Training Loss : 0.44629064202308655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10230001\n",
      "mean reward (100 episodes) 1912.700000\n",
      "best mean reward 2091.900000\n",
      "running time 65127.686318\n",
      "Train_EnvstepsSoFar : 10230001\n",
      "Train_AverageReturn : 1912.7\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65127.68631839752\n",
      "Training Loss : 0.8661418557167053\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10240001\n",
      "mean reward (100 episodes) 1886.400000\n",
      "best mean reward 2091.900000\n",
      "running time 65197.644144\n",
      "Train_EnvstepsSoFar : 10240001\n",
      "Train_AverageReturn : 1886.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65197.644144296646\n",
      "Training Loss : 2.751270055770874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10250001\n",
      "mean reward (100 episodes) 1946.200000\n",
      "best mean reward 2091.900000\n",
      "running time 65267.642464\n",
      "Train_EnvstepsSoFar : 10250001\n",
      "Train_AverageReturn : 1946.2\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65267.64246368408\n",
      "Training Loss : 0.33198967576026917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10260001\n",
      "mean reward (100 episodes) 1945.300000\n",
      "best mean reward 2091.900000\n",
      "running time 65335.288408\n",
      "Train_EnvstepsSoFar : 10260001\n",
      "Train_AverageReturn : 1945.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65335.28840780258\n",
      "Training Loss : 1.4581356048583984\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10270001\n",
      "mean reward (100 episodes) 1947.400000\n",
      "best mean reward 2091.900000\n",
      "running time 65408.174121\n",
      "Train_EnvstepsSoFar : 10270001\n",
      "Train_AverageReturn : 1947.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65408.1741206646\n",
      "Training Loss : 0.14916211366653442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10280001\n",
      "mean reward (100 episodes) 1889.600000\n",
      "best mean reward 2091.900000\n",
      "running time 65475.766430\n",
      "Train_EnvstepsSoFar : 10280001\n",
      "Train_AverageReturn : 1889.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65475.76642990112\n",
      "Training Loss : 0.18982693552970886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10290001\n",
      "mean reward (100 episodes) 1876.300000\n",
      "best mean reward 2091.900000\n",
      "running time 65546.844834\n",
      "Train_EnvstepsSoFar : 10290001\n",
      "Train_AverageReturn : 1876.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65546.8448343277\n",
      "Training Loss : 1.5591356754302979\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10300001\n",
      "mean reward (100 episodes) 1810.300000\n",
      "best mean reward 2091.900000\n",
      "running time 65615.350322\n",
      "Train_EnvstepsSoFar : 10300001\n",
      "Train_AverageReturn : 1810.3\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65615.35032224655\n",
      "Training Loss : 0.2709672451019287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10310001\n",
      "mean reward (100 episodes) 1826.600000\n",
      "best mean reward 2091.900000\n",
      "running time 65682.939723\n",
      "Train_EnvstepsSoFar : 10310001\n",
      "Train_AverageReturn : 1826.6\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65682.93972325325\n",
      "Training Loss : 0.1426810920238495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10320001\n",
      "mean reward (100 episodes) 1845.500000\n",
      "best mean reward 2091.900000\n",
      "running time 65759.170736\n",
      "Train_EnvstepsSoFar : 10320001\n",
      "Train_AverageReturn : 1845.5\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65759.17073559761\n",
      "Training Loss : 1.5573904514312744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10330001\n",
      "mean reward (100 episodes) 1925.000000\n",
      "best mean reward 2091.900000\n",
      "running time 65835.372000\n",
      "Train_EnvstepsSoFar : 10330001\n",
      "Train_AverageReturn : 1925.0\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65835.37199997902\n",
      "Training Loss : 0.11419306695461273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10340001\n",
      "mean reward (100 episodes) 1971.000000\n",
      "best mean reward 2091.900000\n",
      "running time 65915.646938\n",
      "Train_EnvstepsSoFar : 10340001\n",
      "Train_AverageReturn : 1971.0\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65915.64693784714\n",
      "Training Loss : 0.15334656834602356\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10350001\n",
      "mean reward (100 episodes) 2006.400000\n",
      "best mean reward 2091.900000\n",
      "running time 65997.148344\n",
      "Train_EnvstepsSoFar : 10350001\n",
      "Train_AverageReturn : 2006.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 65997.1483438015\n",
      "Training Loss : 0.13539527356624603\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10360001\n",
      "mean reward (100 episodes) 1922.800000\n",
      "best mean reward 2091.900000\n",
      "running time 66068.311858\n",
      "Train_EnvstepsSoFar : 10360001\n",
      "Train_AverageReturn : 1922.8\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 66068.31185817719\n",
      "Training Loss : 1.1030724048614502\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10370001\n",
      "mean reward (100 episodes) 1988.500000\n",
      "best mean reward 2091.900000\n",
      "running time 66139.868880\n",
      "Train_EnvstepsSoFar : 10370001\n",
      "Train_AverageReturn : 1988.5\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 66139.86888027191\n",
      "Training Loss : 0.16547942161560059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10380001\n",
      "mean reward (100 episodes) 1990.500000\n",
      "best mean reward 2091.900000\n",
      "running time 66213.049542\n",
      "Train_EnvstepsSoFar : 10380001\n",
      "Train_AverageReturn : 1990.5\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 66213.04954218864\n",
      "Training Loss : 0.1566760540008545\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10390001\n",
      "mean reward (100 episodes) 2023.900000\n",
      "best mean reward 2091.900000\n",
      "running time 66282.954119\n",
      "Train_EnvstepsSoFar : 10390001\n",
      "Train_AverageReturn : 2023.9\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 66282.95411920547\n",
      "Training Loss : 0.09716134518384933\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10400001\n",
      "mean reward (100 episodes) 2018.100000\n",
      "best mean reward 2091.900000\n",
      "running time 66350.737660\n",
      "Train_EnvstepsSoFar : 10400001\n",
      "Train_AverageReturn : 2018.1\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 66350.7376601696\n",
      "Training Loss : 0.09995267540216446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10410001\n",
      "mean reward (100 episodes) 2025.400000\n",
      "best mean reward 2091.900000\n",
      "running time 66427.797048\n",
      "Train_EnvstepsSoFar : 10410001\n",
      "Train_AverageReturn : 2025.4\n",
      "Train_BestReturn : 2091.9\n",
      "TimeSinceStart : 66427.79704761505\n",
      "Training Loss : 0.22766712307929993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10419000 ************\n",
      "\n",
      "Training agent...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_hw3_dqn.py\", line 94, in <module>\n",
      "    main()\n",
      "  File \"./run_hw3_dqn.py\", line 90, in main\n",
      "    trainer.run_training_loop()\n",
      "  File \"./run_hw3_dqn.py\", line 36, in run_training_loop\n",
      "    eval_policy = self.rl_trainer.agent.actor,\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/infrastructure/rl_trainer.py\", line 176, in run_training_loop\n",
      "    all_logs = self.train_agent()\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/infrastructure/rl_trainer.py\", line 224, in train_agent\n",
      "    train_log = self.agent.train(ob_batch, ac_batch, re_batch, next_ob_batch, terminal_batch)\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/agents/dqn_agent.py\", line 97, in train\n",
      "    ob_no, ac_na, next_ob_no, re_n, terminal_n\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/critics/dqn_critic.py\", line 64, in update\n",
      "    terminal_n = ptu.from_numpy(terminal_n)\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/infrastructure/pytorch_util.py\", line 75, in from_numpy\n",
      "    return torch.from_numpy(*args, **kwargs).float().to(device)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name MsPacman-v0 --exp_name q1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_4_LunarLander-v3_17-10-2020_14-14-48 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_4_LunarLander-v3_17-10-2020_14-14-48\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.007685\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.007685184478759766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -248.663610\n",
      "best mean reward -inf\n",
      "running time 40.360711\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -248.66361017505048\n",
      "TimeSinceStart : 40.360711336135864\n",
      "Training Loss : 3.5306460857391357\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -175.622460\n",
      "best mean reward -175.622460\n",
      "running time 69.116774\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -175.62246025054034\n",
      "Train_BestReturn : -175.62246025054034\n",
      "TimeSinceStart : 69.11677432060242\n",
      "Training Loss : 0.34539926052093506\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -157.167177\n",
      "best mean reward -157.167177\n",
      "running time 104.647582\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -157.1671773810082\n",
      "Train_BestReturn : -157.1671773810082\n",
      "TimeSinceStart : 104.64758229255676\n",
      "Training Loss : 1.1791067123413086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -141.502597\n",
      "best mean reward -141.502597\n",
      "running time 156.437387\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -141.50259678808374\n",
      "Train_BestReturn : -141.50259678808374\n",
      "TimeSinceStart : 156.43738722801208\n",
      "Training Loss : 0.5049508810043335\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -135.658451\n",
      "best mean reward -135.658451\n",
      "running time 193.264794\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -135.65845125053048\n",
      "Train_BestReturn : -135.65845125053048\n",
      "TimeSinceStart : 193.26479387283325\n",
      "Training Loss : 0.4540313482284546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -120.060795\n",
      "best mean reward -120.060795\n",
      "running time 230.823465\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -120.0607951174476\n",
      "Train_BestReturn : -120.0607951174476\n",
      "TimeSinceStart : 230.82346534729004\n",
      "Training Loss : 0.1508062481880188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -108.714423\n",
      "best mean reward -108.714423\n",
      "running time 265.397559\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -108.7144230690888\n",
      "Train_BestReturn : -108.7144230690888\n",
      "TimeSinceStart : 265.3975591659546\n",
      "Training Loss : 0.23462827503681183\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -89.003746\n",
      "best mean reward -89.003746\n",
      "running time 298.813529\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -89.00374640411121\n",
      "Train_BestReturn : -89.00374640411121\n",
      "TimeSinceStart : 298.813529253006\n",
      "Training Loss : 1.2071596384048462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -93.516346\n",
      "best mean reward -89.003746\n",
      "running time 332.952224\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -93.51634578420477\n",
      "Train_BestReturn : -89.00374640411121\n",
      "TimeSinceStart : 332.952223777771\n",
      "Training Loss : 0.31854405999183655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -86.676008\n",
      "best mean reward -86.676008\n",
      "running time 372.742657\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -86.67600774076716\n",
      "Train_BestReturn : -86.67600774076716\n",
      "TimeSinceStart : 372.7426574230194\n",
      "Training Loss : 0.623085618019104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -70.636596\n",
      "best mean reward -70.636596\n",
      "running time 410.635717\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -70.63659564882929\n",
      "Train_BestReturn : -70.63659564882929\n",
      "TimeSinceStart : 410.6357169151306\n",
      "Training Loss : 0.20275329053401947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -47.790777\n",
      "best mean reward -47.790777\n",
      "running time 447.074527\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -47.79077726504089\n",
      "Train_BestReturn : -47.79077726504089\n",
      "TimeSinceStart : 447.0745265483856\n",
      "Training Loss : 0.31589627265930176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -16.877213\n",
      "best mean reward -16.877213\n",
      "running time 478.918514\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -16.877212814493962\n",
      "Train_BestReturn : -16.877212814493962\n",
      "TimeSinceStart : 478.9185140132904\n",
      "Training Loss : 0.7830184698104858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 7.826883\n",
      "best mean reward 7.826883\n",
      "running time 511.598639\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 7.826882993359554\n",
      "Train_BestReturn : 7.826882993359554\n",
      "TimeSinceStart : 511.59863901138306\n",
      "Training Loss : 0.1161937266588211\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 35.032218\n",
      "best mean reward 35.032218\n",
      "running time 546.007383\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 35.032217591951444\n",
      "Train_BestReturn : 35.032217591951444\n",
      "TimeSinceStart : 546.007383108139\n",
      "Training Loss : 0.6612319946289062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 60.309112\n",
      "best mean reward 60.309112\n",
      "running time 578.713274\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 60.30911212205611\n",
      "Train_BestReturn : 60.30911212205611\n",
      "TimeSinceStart : 578.7132742404938\n",
      "Training Loss : 0.146798238158226\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 62.500168\n",
      "best mean reward 62.500168\n",
      "running time 618.713666\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 62.500168220650075\n",
      "Train_BestReturn : 62.500168220650075\n",
      "TimeSinceStart : 618.7136657238007\n",
      "Training Loss : 0.12406527251005173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 55.555627\n",
      "best mean reward 62.500168\n",
      "running time 654.013373\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 55.5556268528925\n",
      "Train_BestReturn : 62.500168220650075\n",
      "TimeSinceStart : 654.013373374939\n",
      "Training Loss : 0.2676210105419159\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 48.562391\n",
      "best mean reward 62.500168\n",
      "running time 687.234502\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 48.562390884382424\n",
      "Train_BestReturn : 62.500168220650075\n",
      "TimeSinceStart : 687.2345020771027\n",
      "Training Loss : 0.46892207860946655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 52.343889\n",
      "best mean reward 62.500168\n",
      "running time 720.264324\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 52.34388895176081\n",
      "Train_BestReturn : 62.500168220650075\n",
      "TimeSinceStart : 720.2643237113953\n",
      "Training Loss : 0.15268474817276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 44.149507\n",
      "best mean reward 62.500168\n",
      "running time 756.652995\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 44.14950680020711\n",
      "Train_BestReturn : 62.500168220650075\n",
      "TimeSinceStart : 756.6529948711395\n",
      "Training Loss : 0.13569903373718262\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 65.197269\n",
      "best mean reward 65.197269\n",
      "running time 789.070618\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 65.1972692486424\n",
      "Train_BestReturn : 65.1972692486424\n",
      "TimeSinceStart : 789.0706181526184\n",
      "Training Loss : 0.07877443730831146\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 80.015347\n",
      "best mean reward 80.015347\n",
      "running time 822.107955\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 80.01534677489742\n",
      "Train_BestReturn : 80.01534677489742\n",
      "TimeSinceStart : 822.1079552173615\n",
      "Training Loss : 0.1606576144695282\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 78.131122\n",
      "best mean reward 80.015347\n",
      "running time 873.109662\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 78.13112219705326\n",
      "Train_BestReturn : 80.01534677489742\n",
      "TimeSinceStart : 873.1096622943878\n",
      "Training Loss : 0.12307540327310562\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 83.975076\n",
      "best mean reward 83.975076\n",
      "running time 906.357943\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 83.97507648123825\n",
      "Train_BestReturn : 83.97507648123825\n",
      "TimeSinceStart : 906.3579430580139\n",
      "Training Loss : 0.13279780745506287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 72.133718\n",
      "best mean reward 83.975076\n",
      "running time 940.234679\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 72.13371848140568\n",
      "Train_BestReturn : 83.97507648123825\n",
      "TimeSinceStart : 940.2346787452698\n",
      "Training Loss : 0.09884592890739441\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 93.738450\n",
      "best mean reward 93.738450\n",
      "running time 973.524228\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 93.7384503582056\n",
      "Train_BestReturn : 93.7384503582056\n",
      "TimeSinceStart : 973.5242276191711\n",
      "Training Loss : 0.09142488241195679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 113.252503\n",
      "best mean reward 113.252503\n",
      "running time 1005.237005\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 113.25250287627665\n",
      "Train_BestReturn : 113.25250287627665\n",
      "TimeSinceStart : 1005.2370047569275\n",
      "Training Loss : 0.562446653842926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 129.488062\n",
      "best mean reward 129.488062\n",
      "running time 1036.444003\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 129.48806175468368\n",
      "Train_BestReturn : 129.48806175468368\n",
      "TimeSinceStart : 1036.444002866745\n",
      "Training Loss : 0.12147580832242966\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 131.045676\n",
      "best mean reward 131.045676\n",
      "running time 1067.247838\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 131.0456758432156\n",
      "Train_BestReturn : 131.0456758432156\n",
      "TimeSinceStart : 1067.2478377819061\n",
      "Training Loss : 0.12289092689752579\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 138.527457\n",
      "best mean reward 138.527457\n",
      "running time 1099.513944\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 138.52745742873293\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1099.5139439105988\n",
      "Training Loss : 0.14131242036819458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 136.185677\n",
      "best mean reward 138.527457\n",
      "running time 1129.235385\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 136.185677456877\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1129.2353847026825\n",
      "Training Loss : 0.24837566912174225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 108.929045\n",
      "best mean reward 138.527457\n",
      "running time 1157.863561\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 108.92904467231111\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1157.8635611534119\n",
      "Training Loss : 0.40513426065444946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 125.603060\n",
      "best mean reward 138.527457\n",
      "running time 1186.207524\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 125.60306037174027\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1186.207524061203\n",
      "Training Loss : 0.20190930366516113\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 121.549208\n",
      "best mean reward 138.527457\n",
      "running time 1214.684798\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 121.54920787223944\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1214.6847982406616\n",
      "Training Loss : 0.5996345281600952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 106.355637\n",
      "best mean reward 138.527457\n",
      "running time 1244.913399\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 106.35563716100931\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1244.913399219513\n",
      "Training Loss : 0.2798343598842621\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 130.054745\n",
      "best mean reward 138.527457\n",
      "running time 1274.125895\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 130.05474544049227\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1274.1258952617645\n",
      "Training Loss : 0.7368363738059998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 128.074047\n",
      "best mean reward 138.527457\n",
      "running time 1302.756721\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 128.07404677631357\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1302.7567210197449\n",
      "Training Loss : 0.3088644742965698\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 116.344051\n",
      "best mean reward 138.527457\n",
      "running time 1339.420258\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 116.34405107706576\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1339.420257806778\n",
      "Training Loss : 0.2745462656021118\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 98.176683\n",
      "best mean reward 138.527457\n",
      "running time 1375.443471\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 98.17668274351277\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1375.4434711933136\n",
      "Training Loss : 0.6215704679489136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 103.967431\n",
      "best mean reward 138.527457\n",
      "running time 1405.993947\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 103.96743087667896\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1405.9939467906952\n",
      "Training Loss : 1.626294732093811\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 104.211161\n",
      "best mean reward 138.527457\n",
      "running time 1440.368588\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 104.21116111576463\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1440.3685879707336\n",
      "Training Loss : 0.13960742950439453\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 80.938764\n",
      "best mean reward 138.527457\n",
      "running time 1474.519942\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 80.93876390139671\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1474.5199415683746\n",
      "Training Loss : 0.44226497411727905\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 55.607196\n",
      "best mean reward 138.527457\n",
      "running time 1507.635546\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 55.607196048219585\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1507.6355459690094\n",
      "Training Loss : 0.6594359874725342\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 73.481827\n",
      "best mean reward 138.527457\n",
      "running time 1537.795091\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 73.48182684114822\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1537.7950911521912\n",
      "Training Loss : 0.1598629653453827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 90.109899\n",
      "best mean reward 138.527457\n",
      "running time 1567.921544\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 90.10989931360722\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1567.9215438365936\n",
      "Training Loss : 1.4903143644332886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 79.363285\n",
      "best mean reward 138.527457\n",
      "running time 1597.581882\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 79.36328470373\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1597.5818824768066\n",
      "Training Loss : 0.16425946354866028\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 103.886882\n",
      "best mean reward 138.527457\n",
      "running time 1626.693936\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 103.88688210857495\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1626.6939363479614\n",
      "Training Loss : 0.5682975649833679\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 76.261857\n",
      "best mean reward 138.527457\n",
      "running time 1657.362304\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 76.26185676166735\n",
      "Train_BestReturn : 138.52745742873293\n",
      "TimeSinceStart : 1657.3623039722443\n",
      "Training Loss : 0.21637892723083496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_4_LunarLander-v3_17-10-2020_14-42-59 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_4_LunarLander-v3_17-10-2020_14-42-59\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006782\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006781578063964844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -240.580732\n",
      "best mean reward -inf\n",
      "running time 33.692182\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -240.58073243364143\n",
      "TimeSinceStart : 33.692182302474976\n",
      "Training Loss : 0.40162378549575806\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -213.112021\n",
      "best mean reward -213.112021\n",
      "running time 63.807288\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -213.11202087058717\n",
      "Train_BestReturn : -213.11202087058717\n",
      "TimeSinceStart : 63.80728816986084\n",
      "Training Loss : 0.7030408382415771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -197.981869\n",
      "best mean reward -197.981869\n",
      "running time 100.255300\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -197.9818685371475\n",
      "Train_BestReturn : -197.9818685371475\n",
      "TimeSinceStart : 100.255300283432\n",
      "Training Loss : 0.5006359219551086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -177.985087\n",
      "best mean reward -177.985087\n",
      "running time 155.023341\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -177.98508725558065\n",
      "Train_BestReturn : -177.98508725558065\n",
      "TimeSinceStart : 155.02334094047546\n",
      "Training Loss : 0.4693875312805176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -167.307053\n",
      "best mean reward -167.307053\n",
      "running time 193.056123\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -167.30705311750305\n",
      "Train_BestReturn : -167.30705311750305\n",
      "TimeSinceStart : 193.05612325668335\n",
      "Training Loss : 0.4383442997932434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -160.270017\n",
      "best mean reward -160.270017\n",
      "running time 231.690581\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -160.2700172601754\n",
      "Train_BestReturn : -160.2700172601754\n",
      "TimeSinceStart : 231.6905813217163\n",
      "Training Loss : 0.42497849464416504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -150.080943\n",
      "best mean reward -150.080943\n",
      "running time 266.183075\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -150.0809430504992\n",
      "Train_BestReturn : -150.0809430504992\n",
      "TimeSinceStart : 266.18307542800903\n",
      "Training Loss : 0.2701554000377655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -144.702416\n",
      "best mean reward -144.702416\n",
      "running time 301.429589\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -144.7024155117873\n",
      "Train_BestReturn : -144.7024155117873\n",
      "TimeSinceStart : 301.42958879470825\n",
      "Training Loss : 0.25233733654022217\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -125.144127\n",
      "best mean reward -125.144127\n",
      "running time 336.828753\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -125.14412748440708\n",
      "Train_BestReturn : -125.14412748440708\n",
      "TimeSinceStart : 336.8287527561188\n",
      "Training Loss : 0.28048640489578247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -115.117825\n",
      "best mean reward -115.117825\n",
      "running time 374.077003\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -115.11782458994618\n",
      "Train_BestReturn : -115.11782458994618\n",
      "TimeSinceStart : 374.07700300216675\n",
      "Training Loss : 0.1660868227481842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -104.936203\n",
      "best mean reward -104.936203\n",
      "running time 417.609209\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -104.9362025813057\n",
      "Train_BestReturn : -104.9362025813057\n",
      "TimeSinceStart : 417.60920882225037\n",
      "Training Loss : 0.2629796266555786\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -94.329409\n",
      "best mean reward -94.329409\n",
      "running time 455.587834\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -94.32940868681611\n",
      "Train_BestReturn : -94.32940868681611\n",
      "TimeSinceStart : 455.5878338813782\n",
      "Training Loss : 0.9846382737159729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -70.431082\n",
      "best mean reward -70.431082\n",
      "running time 490.409843\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -70.43108211825738\n",
      "Train_BestReturn : -70.43108211825738\n",
      "TimeSinceStart : 490.40984296798706\n",
      "Training Loss : 0.45886099338531494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -57.872642\n",
      "best mean reward -57.872642\n",
      "running time 528.319262\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -57.87264195802496\n",
      "Train_BestReturn : -57.87264195802496\n",
      "TimeSinceStart : 528.3192622661591\n",
      "Training Loss : 0.29719841480255127\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -52.649416\n",
      "best mean reward -52.649416\n",
      "running time 570.496000\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -52.64941567355952\n",
      "Train_BestReturn : -52.64941567355952\n",
      "TimeSinceStart : 570.4960000514984\n",
      "Training Loss : 0.0774185061454773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) -37.691817\n",
      "best mean reward -37.691817\n",
      "running time 608.315460\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : -37.6918173101135\n",
      "Train_BestReturn : -37.6918173101135\n",
      "TimeSinceStart : 608.3154604434967\n",
      "Training Loss : 0.10758109390735626\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) -12.253108\n",
      "best mean reward -12.253108\n",
      "running time 642.656656\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : -12.253108269137336\n",
      "Train_BestReturn : -12.253108269137336\n",
      "TimeSinceStart : 642.6566557884216\n",
      "Training Loss : 0.03345964476466179\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 2.978642\n",
      "best mean reward 2.978642\n",
      "running time 680.634557\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 2.978641617190824\n",
      "Train_BestReturn : 2.978641617190824\n",
      "TimeSinceStart : 680.6345572471619\n",
      "Training Loss : 0.05366474762558937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 14.469730\n",
      "best mean reward 14.469730\n",
      "running time 717.343581\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 14.469729827990438\n",
      "Train_BestReturn : 14.469729827990438\n",
      "TimeSinceStart : 717.343581199646\n",
      "Training Loss : 0.03137177228927612\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 27.955347\n",
      "best mean reward 27.955347\n",
      "running time 751.927016\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 27.955347190562453\n",
      "Train_BestReturn : 27.955347190562453\n",
      "TimeSinceStart : 751.9270164966583\n",
      "Training Loss : 0.06983234733343124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 56.884777\n",
      "best mean reward 56.884777\n",
      "running time 791.931509\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 56.88477669160655\n",
      "Train_BestReturn : 56.88477669160655\n",
      "TimeSinceStart : 791.9315090179443\n",
      "Training Loss : 0.059813663363456726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 76.460637\n",
      "best mean reward 76.460637\n",
      "running time 824.546349\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 76.46063740119907\n",
      "Train_BestReturn : 76.46063740119907\n",
      "TimeSinceStart : 824.5463488101959\n",
      "Training Loss : 0.3440004587173462\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 96.642876\n",
      "best mean reward 96.642876\n",
      "running time 856.006714\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 96.64287614158322\n",
      "Train_BestReturn : 96.64287614158322\n",
      "TimeSinceStart : 856.0067138671875\n",
      "Training Loss : 0.1992059051990509\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 102.822018\n",
      "best mean reward 102.822018\n",
      "running time 886.103182\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 102.82201772657353\n",
      "Train_BestReturn : 102.82201772657353\n",
      "TimeSinceStart : 886.1031823158264\n",
      "Training Loss : 0.14937031269073486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 103.157062\n",
      "best mean reward 103.157062\n",
      "running time 917.121822\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 103.15706215152977\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 917.1218223571777\n",
      "Training Loss : 0.2811877131462097\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 80.708748\n",
      "best mean reward 103.157062\n",
      "running time 947.485889\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 80.70874813736572\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 947.4858894348145\n",
      "Training Loss : 0.31802284717559814\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 68.701451\n",
      "best mean reward 103.157062\n",
      "running time 983.704141\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 68.7014511744849\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 983.704140663147\n",
      "Training Loss : 2.6952433586120605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 61.542231\n",
      "best mean reward 103.157062\n",
      "running time 1014.508843\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 61.54223075482708\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1014.5088429450989\n",
      "Training Loss : 0.13356336951255798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 61.136022\n",
      "best mean reward 103.157062\n",
      "running time 1045.610450\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 61.136021863945324\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1045.6104500293732\n",
      "Training Loss : 0.06346186250448227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 60.488749\n",
      "best mean reward 103.157062\n",
      "running time 1075.260458\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 60.48874929542906\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1075.2604579925537\n",
      "Training Loss : 0.28909242153167725\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 65.368929\n",
      "best mean reward 103.157062\n",
      "running time 1103.630744\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 65.36892904328315\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1103.6307439804077\n",
      "Training Loss : 0.9172857403755188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 50.657034\n",
      "best mean reward 103.157062\n",
      "running time 1135.957285\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 50.657033526065845\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1135.9572846889496\n",
      "Training Loss : 0.7918508052825928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 66.383036\n",
      "best mean reward 103.157062\n",
      "running time 1167.378385\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 66.3830358398719\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1167.3783853054047\n",
      "Training Loss : 1.329586148262024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 101.534057\n",
      "best mean reward 103.157062\n",
      "running time 1198.034479\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 101.53405672253581\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1198.034479379654\n",
      "Training Loss : 2.3421876430511475\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 90.031308\n",
      "best mean reward 103.157062\n",
      "running time 1228.732437\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 90.03130821121705\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1228.7324373722076\n",
      "Training Loss : 0.371986448764801\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 97.534001\n",
      "best mean reward 103.157062\n",
      "running time 1257.711404\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 97.53400093821172\n",
      "Train_BestReturn : 103.15706215152977\n",
      "TimeSinceStart : 1257.7114043235779\n",
      "Training Loss : 0.5412963032722473\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 103.920831\n",
      "best mean reward 103.920831\n",
      "running time 1287.098354\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 103.92083055544266\n",
      "Train_BestReturn : 103.92083055544266\n",
      "TimeSinceStart : 1287.0983538627625\n",
      "Training Loss : 5.822832107543945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 97.973333\n",
      "best mean reward 103.920831\n",
      "running time 1316.119520\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 97.97333266094874\n",
      "Train_BestReturn : 103.92083055544266\n",
      "TimeSinceStart : 1316.1195197105408\n",
      "Training Loss : 0.2543194890022278\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 93.974971\n",
      "best mean reward 103.920831\n",
      "running time 1345.942238\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 93.97497056961305\n",
      "Train_BestReturn : 103.92083055544266\n",
      "TimeSinceStart : 1345.942238330841\n",
      "Training Loss : 0.6613218784332275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 107.252817\n",
      "best mean reward 107.252817\n",
      "running time 1378.053817\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 107.25281741435988\n",
      "Train_BestReturn : 107.25281741435988\n",
      "TimeSinceStart : 1378.0538170337677\n",
      "Training Loss : 0.3079652786254883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 118.825314\n",
      "best mean reward 118.825314\n",
      "running time 1407.937884\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 118.82531439805877\n",
      "Train_BestReturn : 118.82531439805877\n",
      "TimeSinceStart : 1407.9378836154938\n",
      "Training Loss : 0.2870662212371826\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 127.868997\n",
      "best mean reward 127.868997\n",
      "running time 1437.403030\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 127.86899653969651\n",
      "Train_BestReturn : 127.86899653969651\n",
      "TimeSinceStart : 1437.4030301570892\n",
      "Training Loss : 0.44186320900917053\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 115.770127\n",
      "best mean reward 127.868997\n",
      "running time 1465.420979\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 115.77012727695501\n",
      "Train_BestReturn : 127.86899653969651\n",
      "TimeSinceStart : 1465.4209785461426\n",
      "Training Loss : 0.7013302445411682\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 103.321410\n",
      "best mean reward 127.868997\n",
      "running time 1494.388697\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 103.32141028772614\n",
      "Train_BestReturn : 127.86899653969651\n",
      "TimeSinceStart : 1494.3886971473694\n",
      "Training Loss : 0.42125511169433594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 125.388347\n",
      "best mean reward 127.868997\n",
      "running time 1521.875482\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 125.38834713038561\n",
      "Train_BestReturn : 127.86899653969651\n",
      "TimeSinceStart : 1521.875482082367\n",
      "Training Loss : 0.23307591676712036\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 142.618393\n",
      "best mean reward 142.618393\n",
      "running time 1550.101321\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 142.61839293853117\n",
      "Train_BestReturn : 142.61839293853117\n",
      "TimeSinceStart : 1550.1013207435608\n",
      "Training Loss : 1.0956907272338867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 119.733182\n",
      "best mean reward 142.618393\n",
      "running time 1576.225823\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 119.73318248704553\n",
      "Train_BestReturn : 142.61839293853117\n",
      "TimeSinceStart : 1576.2258234024048\n",
      "Training Loss : 1.3344932794570923\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 104.905147\n",
      "best mean reward 142.618393\n",
      "running time 1602.430219\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 104.90514733689352\n",
      "Train_BestReturn : 142.61839293853117\n",
      "TimeSinceStart : 1602.4302189350128\n",
      "Training Loss : 1.6247072219848633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 50.081843\n",
      "best mean reward 142.618393\n",
      "running time 1629.412166\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 50.08184284182438\n",
      "Train_BestReturn : 142.61839293853117\n",
      "TimeSinceStart : 1629.4121658802032\n",
      "Training Loss : 1.5556124448776245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_5_LunarLander-v3_17-10-2020_15-10-39 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_5_LunarLander-v3_17-10-2020_15-10-39\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006593\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006592750549316406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -280.310664\n",
      "best mean reward -inf\n",
      "running time 36.283878\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -280.3106641928869\n",
      "TimeSinceStart : 36.28387761116028\n",
      "Training Loss : 0.32233238220214844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -230.100098\n",
      "best mean reward -inf\n",
      "running time 69.942812\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -230.1000983180918\n",
      "TimeSinceStart : 69.94281196594238\n",
      "Training Loss : 0.799407958984375\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -208.885016\n",
      "best mean reward -208.885016\n",
      "running time 107.454725\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -208.8850159087957\n",
      "Train_BestReturn : -208.8850159087957\n",
      "TimeSinceStart : 107.45472526550293\n",
      "Training Loss : 0.45452451705932617\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -184.957397\n",
      "best mean reward -184.957397\n",
      "running time 145.050825\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -184.95739729354264\n",
      "Train_BestReturn : -184.95739729354264\n",
      "TimeSinceStart : 145.0508246421814\n",
      "Training Loss : 0.8563187122344971\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -161.630990\n",
      "best mean reward -161.630990\n",
      "running time 196.759557\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -161.63099034899687\n",
      "Train_BestReturn : -161.63099034899687\n",
      "TimeSinceStart : 196.75955653190613\n",
      "Training Loss : 0.5749810934066772\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -153.341543\n",
      "best mean reward -153.341543\n",
      "running time 233.433521\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -153.3415428579245\n",
      "Train_BestReturn : -153.3415428579245\n",
      "TimeSinceStart : 233.43352127075195\n",
      "Training Loss : 2.0234546661376953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -129.975621\n",
      "best mean reward -129.975621\n",
      "running time 270.044270\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -129.97562131620737\n",
      "Train_BestReturn : -129.97562131620737\n",
      "TimeSinceStart : 270.0442702770233\n",
      "Training Loss : 0.8934498429298401\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -113.292216\n",
      "best mean reward -113.292216\n",
      "running time 303.283729\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -113.29221581046463\n",
      "Train_BestReturn : -113.29221581046463\n",
      "TimeSinceStart : 303.2837290763855\n",
      "Training Loss : 0.14941179752349854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -93.724255\n",
      "best mean reward -93.724255\n",
      "running time 337.788800\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -93.72425533045855\n",
      "Train_BestReturn : -93.72425533045855\n",
      "TimeSinceStart : 337.7888000011444\n",
      "Training Loss : 0.17749154567718506\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -87.032881\n",
      "best mean reward -87.032881\n",
      "running time 372.975996\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -87.0328809982695\n",
      "Train_BestReturn : -87.0328809982695\n",
      "TimeSinceStart : 372.97599625587463\n",
      "Training Loss : 0.24890947341918945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -67.744620\n",
      "best mean reward -67.744620\n",
      "running time 414.508947\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -67.7446196170625\n",
      "Train_BestReturn : -67.7446196170625\n",
      "TimeSinceStart : 414.5089473724365\n",
      "Training Loss : 0.2043442726135254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -53.730480\n",
      "best mean reward -53.730480\n",
      "running time 448.737906\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -53.73048016723319\n",
      "Train_BestReturn : -53.73048016723319\n",
      "TimeSinceStart : 448.7379062175751\n",
      "Training Loss : 0.5054291486740112\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -37.432049\n",
      "best mean reward -37.432049\n",
      "running time 485.570435\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -37.432048847037926\n",
      "Train_BestReturn : -37.432048847037926\n",
      "TimeSinceStart : 485.57043528556824\n",
      "Training Loss : 0.2768533229827881\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -18.452653\n",
      "best mean reward -18.452653\n",
      "running time 522.910932\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -18.452652603826063\n",
      "Train_BestReturn : -18.452652603826063\n",
      "TimeSinceStart : 522.9109320640564\n",
      "Training Loss : 3.0039474964141846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 6.011807\n",
      "best mean reward 6.011807\n",
      "running time 559.475839\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 6.011807399072455\n",
      "Train_BestReturn : 6.011807399072455\n",
      "TimeSinceStart : 559.4758393764496\n",
      "Training Loss : 0.11713609099388123\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 24.579109\n",
      "best mean reward 24.579109\n",
      "running time 594.896759\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 24.579109122335126\n",
      "Train_BestReturn : 24.579109122335126\n",
      "TimeSinceStart : 594.8967587947845\n",
      "Training Loss : 0.07458817958831787\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 48.113968\n",
      "best mean reward 48.113968\n",
      "running time 627.469065\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 48.113967527995385\n",
      "Train_BestReturn : 48.113967527995385\n",
      "TimeSinceStart : 627.469064950943\n",
      "Training Loss : 0.8207058906555176\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 50.375768\n",
      "best mean reward 50.375768\n",
      "running time 676.729480\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 50.37576816361905\n",
      "Train_BestReturn : 50.37576816361905\n",
      "TimeSinceStart : 676.7294800281525\n",
      "Training Loss : 0.09934677183628082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 53.137863\n",
      "best mean reward 53.137863\n",
      "running time 712.420044\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 53.137863182149076\n",
      "Train_BestReturn : 53.137863182149076\n",
      "TimeSinceStart : 712.4200444221497\n",
      "Training Loss : 0.1396687924861908\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 57.603853\n",
      "best mean reward 57.603853\n",
      "running time 748.085373\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 57.60385320919827\n",
      "Train_BestReturn : 57.60385320919827\n",
      "TimeSinceStart : 748.0853731632233\n",
      "Training Loss : 0.048745088279247284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 71.774890\n",
      "best mean reward 71.774890\n",
      "running time 778.321397\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 71.77489016004189\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 778.3213968276978\n",
      "Training Loss : 0.06819754838943481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 67.873174\n",
      "best mean reward 71.774890\n",
      "running time 810.722068\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 67.87317430689357\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 810.7220680713654\n",
      "Training Loss : 2.5931456089019775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 64.925515\n",
      "best mean reward 71.774890\n",
      "running time 842.933286\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 64.92551477085672\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 842.9332859516144\n",
      "Training Loss : 0.13959051668643951\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 38.599136\n",
      "best mean reward 71.774890\n",
      "running time 878.154541\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 38.59913646349776\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 878.1545405387878\n",
      "Training Loss : 0.06573179364204407\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 39.974438\n",
      "best mean reward 71.774890\n",
      "running time 908.942834\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 39.97443783759195\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 908.9428343772888\n",
      "Training Loss : 1.1613274812698364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 58.006081\n",
      "best mean reward 71.774890\n",
      "running time 939.875270\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 58.006080914655584\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 939.8752698898315\n",
      "Training Loss : 0.6755338907241821\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 69.632416\n",
      "best mean reward 71.774890\n",
      "running time 972.653162\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 69.63241636790399\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 972.6531624794006\n",
      "Training Loss : 0.13455457985401154\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 64.984399\n",
      "best mean reward 71.774890\n",
      "running time 1006.658149\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 64.98439878591871\n",
      "Train_BestReturn : 71.77489016004189\n",
      "TimeSinceStart : 1006.6581485271454\n",
      "Training Loss : 0.2732697129249573\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 78.215134\n",
      "best mean reward 78.215134\n",
      "running time 1039.789628\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 78.21513420242916\n",
      "Train_BestReturn : 78.21513420242916\n",
      "TimeSinceStart : 1039.7896275520325\n",
      "Training Loss : 4.375823020935059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 92.797026\n",
      "best mean reward 92.797026\n",
      "running time 1071.420621\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 92.79702598021652\n",
      "Train_BestReturn : 92.79702598021652\n",
      "TimeSinceStart : 1071.420620918274\n",
      "Training Loss : 2.019498825073242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 96.283477\n",
      "best mean reward 96.283477\n",
      "running time 1101.826389\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 96.28347712017653\n",
      "Train_BestReturn : 96.28347712017653\n",
      "TimeSinceStart : 1101.8263893127441\n",
      "Training Loss : 0.36165738105773926\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 119.915710\n",
      "best mean reward 119.915710\n",
      "running time 1130.409494\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 119.91571027930212\n",
      "Train_BestReturn : 119.91571027930212\n",
      "TimeSinceStart : 1130.4094939231873\n",
      "Training Loss : 0.14085984230041504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 137.457721\n",
      "best mean reward 137.457721\n",
      "running time 1159.812866\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 137.45772129394192\n",
      "Train_BestReturn : 137.45772129394192\n",
      "TimeSinceStart : 1159.812866449356\n",
      "Training Loss : 0.143197700381279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 138.552512\n",
      "best mean reward 138.552512\n",
      "running time 1189.141316\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 138.55251216959235\n",
      "Train_BestReturn : 138.55251216959235\n",
      "TimeSinceStart : 1189.1413156986237\n",
      "Training Loss : 0.7907847166061401\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 153.580954\n",
      "best mean reward 153.580954\n",
      "running time 1219.818805\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 153.58095377293833\n",
      "Train_BestReturn : 153.58095377293833\n",
      "TimeSinceStart : 1219.8188045024872\n",
      "Training Loss : 1.1924597024917603\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 155.845178\n",
      "best mean reward 155.845178\n",
      "running time 1249.449067\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 155.845178436125\n",
      "Train_BestReturn : 155.845178436125\n",
      "TimeSinceStart : 1249.4490666389465\n",
      "Training Loss : 2.0186212062835693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 170.262343\n",
      "best mean reward 170.262343\n",
      "running time 1282.686605\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 170.26234334554604\n",
      "Train_BestReturn : 170.26234334554604\n",
      "TimeSinceStart : 1282.6866054534912\n",
      "Training Loss : 2.593252420425415\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 179.448056\n",
      "best mean reward 179.448056\n",
      "running time 1311.466935\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 179.4480558837087\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1311.4669351577759\n",
      "Training Loss : 1.485588788986206\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 173.988945\n",
      "best mean reward 179.448056\n",
      "running time 1343.022820\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 173.98894523685772\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1343.0228204727173\n",
      "Training Loss : 0.7533301711082458\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 142.035024\n",
      "best mean reward 179.448056\n",
      "running time 1374.693846\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 142.03502434232533\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1374.693846464157\n",
      "Training Loss : 1.5861526727676392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 142.979855\n",
      "best mean reward 179.448056\n",
      "running time 1403.010637\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 142.97985468956105\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1403.010636806488\n",
      "Training Loss : 0.34462013840675354\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 146.131853\n",
      "best mean reward 179.448056\n",
      "running time 1432.005608\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 146.13185332365956\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1432.005607843399\n",
      "Training Loss : 0.19870918989181519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 128.818667\n",
      "best mean reward 179.448056\n",
      "running time 1460.719658\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 128.818666795821\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1460.7196578979492\n",
      "Training Loss : 1.3828667402267456\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 139.220195\n",
      "best mean reward 179.448056\n",
      "running time 1488.232574\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 139.22019459648337\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1488.2325735092163\n",
      "Training Loss : 1.5270696878433228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 136.301264\n",
      "best mean reward 179.448056\n",
      "running time 1515.691205\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 136.30126358233247\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1515.6912052631378\n",
      "Training Loss : 0.25289788842201233\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 131.423736\n",
      "best mean reward 179.448056\n",
      "running time 1543.307560\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 131.42373582958268\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1543.3075604438782\n",
      "Training Loss : 2.707038640975952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 143.159106\n",
      "best mean reward 179.448056\n",
      "running time 1570.431695\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 143.15910642252194\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1570.4316947460175\n",
      "Training Loss : 0.9453719854354858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 101.918200\n",
      "best mean reward 179.448056\n",
      "running time 1597.837357\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 101.91820019842785\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1597.8373568058014\n",
      "Training Loss : 0.5473776459693909\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 82.236766\n",
      "best mean reward 179.448056\n",
      "running time 1625.030628\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 82.2367661293797\n",
      "Train_BestReturn : 179.4480558837087\n",
      "TimeSinceStart : 1625.0306279659271\n",
      "Training Loss : 0.18028968572616577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_5_LunarLander-v3_17-10-2020_15-38-17 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_5_LunarLander-v3_17-10-2020_15-38-17\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006802\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.00680232048034668\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -253.321727\n",
      "best mean reward -inf\n",
      "running time 34.127585\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -253.32172729670623\n",
      "TimeSinceStart : 34.12758493423462\n",
      "Training Loss : 0.34971797466278076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -202.321932\n",
      "best mean reward -inf\n",
      "running time 62.745364\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -202.32193243094244\n",
      "TimeSinceStart : 62.74536442756653\n",
      "Training Loss : 1.2311383485794067\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -173.304345\n",
      "best mean reward -173.304345\n",
      "running time 95.941857\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -173.3043454547477\n",
      "Train_BestReturn : -173.3043454547477\n",
      "TimeSinceStart : 95.94185733795166\n",
      "Training Loss : 3.1993980407714844\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -157.193168\n",
      "best mean reward -157.193168\n",
      "running time 130.148072\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -157.1931678625918\n",
      "Train_BestReturn : -157.1931678625918\n",
      "TimeSinceStart : 130.1480724811554\n",
      "Training Loss : 0.4240884780883789\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -134.709571\n",
      "best mean reward -134.709571\n",
      "running time 181.531575\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -134.70957082369947\n",
      "Train_BestReturn : -134.70957082369947\n",
      "TimeSinceStart : 181.53157472610474\n",
      "Training Loss : 0.34594640135765076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -117.878086\n",
      "best mean reward -117.878086\n",
      "running time 217.724674\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -117.87808577700868\n",
      "Train_BestReturn : -117.87808577700868\n",
      "TimeSinceStart : 217.72467374801636\n",
      "Training Loss : 0.8708643913269043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -99.080891\n",
      "best mean reward -99.080891\n",
      "running time 254.224119\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -99.08089064058316\n",
      "Train_BestReturn : -99.08089064058316\n",
      "TimeSinceStart : 254.2241189479828\n",
      "Training Loss : 0.22524842619895935\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -76.953016\n",
      "best mean reward -76.953016\n",
      "running time 288.824617\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -76.95301618690033\n",
      "Train_BestReturn : -76.95301618690033\n",
      "TimeSinceStart : 288.8246171474457\n",
      "Training Loss : 0.3201740086078644\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -72.096122\n",
      "best mean reward -72.096122\n",
      "running time 322.041987\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -72.09612183796995\n",
      "Train_BestReturn : -72.09612183796995\n",
      "TimeSinceStart : 322.04198694229126\n",
      "Training Loss : 0.3170038163661957\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -70.163454\n",
      "best mean reward -70.163454\n",
      "running time 356.530540\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -70.16345421677356\n",
      "Train_BestReturn : -70.16345421677356\n",
      "TimeSinceStart : 356.53053998947144\n",
      "Training Loss : 0.6444247961044312\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -57.309135\n",
      "best mean reward -57.309135\n",
      "running time 396.130921\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -57.30913483612357\n",
      "Train_BestReturn : -57.30913483612357\n",
      "TimeSinceStart : 396.1309208869934\n",
      "Training Loss : 0.15869209170341492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -29.845514\n",
      "best mean reward -29.845514\n",
      "running time 445.234654\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -29.845514298654543\n",
      "Train_BestReturn : -29.845514298654543\n",
      "TimeSinceStart : 445.23465394973755\n",
      "Training Loss : 0.21885056793689728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -15.643856\n",
      "best mean reward -15.643856\n",
      "running time 485.306370\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -15.643856155239833\n",
      "Train_BestReturn : -15.643856155239833\n",
      "TimeSinceStart : 485.3063700199127\n",
      "Training Loss : 0.10302110016345978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 4.577159\n",
      "best mean reward 4.577159\n",
      "running time 526.258159\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 4.577158925715983\n",
      "Train_BestReturn : 4.577158925715983\n",
      "TimeSinceStart : 526.2581589221954\n",
      "Training Loss : 0.09937374293804169\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 18.652040\n",
      "best mean reward 18.652040\n",
      "running time 562.192496\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 18.652039605781866\n",
      "Train_BestReturn : 18.652039605781866\n",
      "TimeSinceStart : 562.1924960613251\n",
      "Training Loss : 0.06322482228279114\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 30.735992\n",
      "best mean reward 30.735992\n",
      "running time 596.012030\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 30.735991919427082\n",
      "Train_BestReturn : 30.735991919427082\n",
      "TimeSinceStart : 596.0120301246643\n",
      "Training Loss : 0.12637034058570862\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 37.535722\n",
      "best mean reward 37.535722\n",
      "running time 632.565609\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 37.53572152640657\n",
      "Train_BestReturn : 37.53572152640657\n",
      "TimeSinceStart : 632.5656085014343\n",
      "Training Loss : 0.05229302868247032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 46.555641\n",
      "best mean reward 46.555641\n",
      "running time 665.548033\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 46.55564103840756\n",
      "Train_BestReturn : 46.55564103840756\n",
      "TimeSinceStart : 665.5480334758759\n",
      "Training Loss : 0.19756242632865906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 42.714156\n",
      "best mean reward 46.555641\n",
      "running time 717.759014\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 42.714156192431716\n",
      "Train_BestReturn : 46.55564103840756\n",
      "TimeSinceStart : 717.7590138912201\n",
      "Training Loss : 0.11587594449520111\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 35.694656\n",
      "best mean reward 46.555641\n",
      "running time 754.070348\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 35.6946557883331\n",
      "Train_BestReturn : 46.55564103840756\n",
      "TimeSinceStart : 754.0703482627869\n",
      "Training Loss : 0.07891324162483215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 48.131119\n",
      "best mean reward 48.131119\n",
      "running time 784.816465\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 48.131118609845736\n",
      "Train_BestReturn : 48.131118609845736\n",
      "TimeSinceStart : 784.8164649009705\n",
      "Training Loss : 0.6993802785873413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 63.616205\n",
      "best mean reward 63.616205\n",
      "running time 814.229789\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 63.616204853318116\n",
      "Train_BestReturn : 63.616204853318116\n",
      "TimeSinceStart : 814.2297885417938\n",
      "Training Loss : 0.18922770023345947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 82.217734\n",
      "best mean reward 82.217734\n",
      "running time 846.265850\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 82.21773360634174\n",
      "Train_BestReturn : 82.21773360634174\n",
      "TimeSinceStart : 846.2658503055573\n",
      "Training Loss : 1.0012484788894653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 85.780914\n",
      "best mean reward 85.780914\n",
      "running time 878.299384\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 85.78091381877097\n",
      "Train_BestReturn : 85.78091381877097\n",
      "TimeSinceStart : 878.2993836402893\n",
      "Training Loss : 0.20687668025493622\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 90.972296\n",
      "best mean reward 90.972296\n",
      "running time 911.619583\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 90.97229614434889\n",
      "Train_BestReturn : 90.97229614434889\n",
      "TimeSinceStart : 911.6195826530457\n",
      "Training Loss : 0.15836311876773834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 96.651980\n",
      "best mean reward 96.651980\n",
      "running time 944.916646\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 96.65197957982178\n",
      "Train_BestReturn : 96.65197957982178\n",
      "TimeSinceStart : 944.916645526886\n",
      "Training Loss : 4.17280387878418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 98.429231\n",
      "best mean reward 98.429231\n",
      "running time 975.306330\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 98.42923054533904\n",
      "Train_BestReturn : 98.42923054533904\n",
      "TimeSinceStart : 975.3063304424286\n",
      "Training Loss : 0.15417379140853882\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 98.830316\n",
      "best mean reward 98.830316\n",
      "running time 1006.521809\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 98.83031648343518\n",
      "Train_BestReturn : 98.83031648343518\n",
      "TimeSinceStart : 1006.5218088626862\n",
      "Training Loss : 0.4604610204696655\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 105.854683\n",
      "best mean reward 105.854683\n",
      "running time 1036.390226\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 105.85468264255616\n",
      "Train_BestReturn : 105.85468264255616\n",
      "TimeSinceStart : 1036.39022564888\n",
      "Training Loss : 0.493582546710968\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 133.106926\n",
      "best mean reward 133.106926\n",
      "running time 1065.780608\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 133.10692636777765\n",
      "Train_BestReturn : 133.10692636777765\n",
      "TimeSinceStart : 1065.780607700348\n",
      "Training Loss : 0.11261692643165588\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 144.615302\n",
      "best mean reward 144.615302\n",
      "running time 1097.623162\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 144.61530246523336\n",
      "Train_BestReturn : 144.61530246523336\n",
      "TimeSinceStart : 1097.6231615543365\n",
      "Training Loss : 0.1422504186630249\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 153.585536\n",
      "best mean reward 153.585536\n",
      "running time 1126.962577\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 153.58553631699107\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1126.962577342987\n",
      "Training Loss : 0.1396683007478714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 145.873515\n",
      "best mean reward 153.585536\n",
      "running time 1154.469921\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 145.8735152589566\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1154.4699211120605\n",
      "Training Loss : 0.6850761771202087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 128.624500\n",
      "best mean reward 153.585536\n",
      "running time 1181.491136\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 128.62450004429522\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1181.4911360740662\n",
      "Training Loss : 0.164004385471344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 129.266444\n",
      "best mean reward 153.585536\n",
      "running time 1208.672736\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 129.26644425798935\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1208.6727364063263\n",
      "Training Loss : 0.07888659834861755\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 128.057871\n",
      "best mean reward 153.585536\n",
      "running time 1238.424554\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 128.05787137184979\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1238.4245538711548\n",
      "Training Loss : 0.9194584488868713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 118.835903\n",
      "best mean reward 153.585536\n",
      "running time 1265.139829\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 118.83590335854613\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1265.1398289203644\n",
      "Training Loss : 0.668825626373291\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 123.364591\n",
      "best mean reward 153.585536\n",
      "running time 1293.224306\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 123.36459057892958\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1293.224306344986\n",
      "Training Loss : 0.28185614943504333\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 115.687426\n",
      "best mean reward 153.585536\n",
      "running time 1323.677564\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 115.68742612577498\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1323.677564382553\n",
      "Training Loss : 0.18031442165374756\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 99.639104\n",
      "best mean reward 153.585536\n",
      "running time 1355.000120\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 99.63910421361261\n",
      "Train_BestReturn : 153.58553631699107\n",
      "TimeSinceStart : 1355.0001196861267\n",
      "Training Loss : 0.18248608708381653\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 155.944878\n",
      "best mean reward 155.944878\n",
      "running time 1385.345858\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 155.94487826663683\n",
      "Train_BestReturn : 155.94487826663683\n",
      "TimeSinceStart : 1385.3458576202393\n",
      "Training Loss : 3.9698493480682373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 176.071659\n",
      "best mean reward 176.071659\n",
      "running time 1417.027934\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 176.07165910629286\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1417.0279340744019\n",
      "Training Loss : 1.0706968307495117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 139.722710\n",
      "best mean reward 176.071659\n",
      "running time 1447.148502\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 139.72270955063033\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1447.1485018730164\n",
      "Training Loss : 0.24419967830181122\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 93.238303\n",
      "best mean reward 176.071659\n",
      "running time 1473.998100\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 93.23830320086529\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1473.9980998039246\n",
      "Training Loss : 1.4746038913726807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 45.311230\n",
      "best mean reward 176.071659\n",
      "running time 1500.098787\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 45.31123017801499\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1500.098786830902\n",
      "Training Loss : 1.2619719505310059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 21.622920\n",
      "best mean reward 176.071659\n",
      "running time 1528.787331\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 21.622919617716146\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1528.7873306274414\n",
      "Training Loss : 0.6722865700721741\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) -40.358223\n",
      "best mean reward 176.071659\n",
      "running time 1557.729536\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : -40.35822295391536\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1557.7295358181\n",
      "Training Loss : 0.6344767808914185\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) -89.099634\n",
      "best mean reward 176.071659\n",
      "running time 1583.728279\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : -89.09963440413911\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1583.7282791137695\n",
      "Training Loss : 6.107780933380127\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) -101.177550\n",
      "best mean reward 176.071659\n",
      "running time 1609.840595\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : -101.17754979361551\n",
      "Train_BestReturn : 176.07165910629286\n",
      "TimeSinceStart : 1609.8405950069427\n",
      "Training Loss : 0.13777604699134827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_6_LunarLander-v3_17-10-2020_16-05-39 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_6_LunarLander-v3_17-10-2020_16-05-39\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006517\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006516695022583008\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -263.836540\n",
      "best mean reward -inf\n",
      "running time 35.997465\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -263.83654012171456\n",
      "TimeSinceStart : 35.99746513366699\n",
      "Training Loss : 0.3461913466453552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -202.483688\n",
      "best mean reward -inf\n",
      "running time 66.634528\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -202.4836881589519\n",
      "TimeSinceStart : 66.63452816009521\n",
      "Training Loss : 0.42114177346229553\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -179.219161\n",
      "best mean reward -179.219161\n",
      "running time 106.429971\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -179.21916125283417\n",
      "Train_BestReturn : -179.21916125283417\n",
      "TimeSinceStart : 106.42997074127197\n",
      "Training Loss : 0.4344223439693451\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -155.485883\n",
      "best mean reward -155.485883\n",
      "running time 147.601832\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -155.48588324686355\n",
      "Train_BestReturn : -155.48588324686355\n",
      "TimeSinceStart : 147.60183215141296\n",
      "Training Loss : 1.1206763982772827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -126.409598\n",
      "best mean reward -126.409598\n",
      "running time 204.514100\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -126.4095978030632\n",
      "Train_BestReturn : -126.4095978030632\n",
      "TimeSinceStart : 204.5140995979309\n",
      "Training Loss : 0.5202037692070007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -100.211972\n",
      "best mean reward -100.211972\n",
      "running time 241.908081\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -100.21197247119716\n",
      "Train_BestReturn : -100.21197247119716\n",
      "TimeSinceStart : 241.90808057785034\n",
      "Training Loss : 0.16613653302192688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -64.644114\n",
      "best mean reward -64.644114\n",
      "running time 278.629759\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -64.64411399765632\n",
      "Train_BestReturn : -64.64411399765632\n",
      "TimeSinceStart : 278.6297585964203\n",
      "Training Loss : 0.5375560522079468\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -51.408173\n",
      "best mean reward -51.408173\n",
      "running time 316.902262\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -51.408172541705724\n",
      "Train_BestReturn : -51.408172541705724\n",
      "TimeSinceStart : 316.9022617340088\n",
      "Training Loss : 0.2840597629547119\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -46.091075\n",
      "best mean reward -46.091075\n",
      "running time 354.005595\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -46.091075054069435\n",
      "Train_BestReturn : -46.091075054069435\n",
      "TimeSinceStart : 354.0055947303772\n",
      "Training Loss : 0.3039461672306061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -43.249224\n",
      "best mean reward -43.249224\n",
      "running time 393.623142\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -43.249224156250854\n",
      "Train_BestReturn : -43.249224156250854\n",
      "TimeSinceStart : 393.6231417655945\n",
      "Training Loss : 0.27245429158210754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -26.973306\n",
      "best mean reward -26.973306\n",
      "running time 437.922714\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -26.973305794092948\n",
      "Train_BestReturn : -26.973305794092948\n",
      "TimeSinceStart : 437.92271423339844\n",
      "Training Loss : 0.21980921924114227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -3.717978\n",
      "best mean reward -3.717978\n",
      "running time 475.351832\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -3.717977691005262\n",
      "Train_BestReturn : -3.717977691005262\n",
      "TimeSinceStart : 475.35183215141296\n",
      "Training Loss : 0.15432646870613098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 11.193578\n",
      "best mean reward 11.193578\n",
      "running time 510.897876\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 11.193577915895485\n",
      "Train_BestReturn : 11.193577915895485\n",
      "TimeSinceStart : 510.89787578582764\n",
      "Training Loss : 0.20002447068691254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 29.567850\n",
      "best mean reward 29.567850\n",
      "running time 544.701430\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 29.567850309493842\n",
      "Train_BestReturn : 29.567850309493842\n",
      "TimeSinceStart : 544.7014303207397\n",
      "Training Loss : 0.329857736825943\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 54.794628\n",
      "best mean reward 54.794628\n",
      "running time 578.833519\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 54.79462837255305\n",
      "Train_BestReturn : 54.79462837255305\n",
      "TimeSinceStart : 578.8335185050964\n",
      "Training Loss : 0.11100827902555466\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 68.197507\n",
      "best mean reward 68.197507\n",
      "running time 613.301069\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 68.19750689504673\n",
      "Train_BestReturn : 68.19750689504673\n",
      "TimeSinceStart : 613.3010692596436\n",
      "Training Loss : 0.5922534465789795\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 69.699712\n",
      "best mean reward 69.699712\n",
      "running time 648.745007\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 69.69971215285497\n",
      "Train_BestReturn : 69.69971215285497\n",
      "TimeSinceStart : 648.7450070381165\n",
      "Training Loss : 1.1572633981704712\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 78.072859\n",
      "best mean reward 78.072859\n",
      "running time 690.151432\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 78.07285923331001\n",
      "Train_BestReturn : 78.07285923331001\n",
      "TimeSinceStart : 690.1514322757721\n",
      "Training Loss : 0.7837312817573547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 81.070139\n",
      "best mean reward 81.070139\n",
      "running time 723.704824\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 81.07013857267808\n",
      "Train_BestReturn : 81.07013857267808\n",
      "TimeSinceStart : 723.7048242092133\n",
      "Training Loss : 0.07988616824150085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 83.259037\n",
      "best mean reward 83.259037\n",
      "running time 759.353645\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 83.25903652509734\n",
      "Train_BestReturn : 83.25903652509734\n",
      "TimeSinceStart : 759.3536448478699\n",
      "Training Loss : 0.08548468351364136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 94.369741\n",
      "best mean reward 94.369741\n",
      "running time 796.357511\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 94.36974127177815\n",
      "Train_BestReturn : 94.36974127177815\n",
      "TimeSinceStart : 796.3575105667114\n",
      "Training Loss : 0.34593072533607483\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 97.699488\n",
      "best mean reward 97.699488\n",
      "running time 831.617539\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 97.69948770040185\n",
      "Train_BestReturn : 97.69948770040185\n",
      "TimeSinceStart : 831.6175391674042\n",
      "Training Loss : 0.14277514815330505\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 98.018358\n",
      "best mean reward 98.018358\n",
      "running time 866.989162\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 98.01835840511434\n",
      "Train_BestReturn : 98.01835840511434\n",
      "TimeSinceStart : 866.9891624450684\n",
      "Training Loss : 3.3210537433624268\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 111.597073\n",
      "best mean reward 111.597073\n",
      "running time 900.684963\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 111.59707321520207\n",
      "Train_BestReturn : 111.59707321520207\n",
      "TimeSinceStart : 900.6849632263184\n",
      "Training Loss : 0.12202893197536469\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 113.267496\n",
      "best mean reward 113.267496\n",
      "running time 940.570274\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 113.26749607730713\n",
      "Train_BestReturn : 113.26749607730713\n",
      "TimeSinceStart : 940.5702743530273\n",
      "Training Loss : 0.3491450250148773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 128.181100\n",
      "best mean reward 128.181100\n",
      "running time 973.138443\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 128.1810995407308\n",
      "Train_BestReturn : 128.1810995407308\n",
      "TimeSinceStart : 973.1384427547455\n",
      "Training Loss : 0.9076929688453674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 128.840091\n",
      "best mean reward 128.840091\n",
      "running time 1007.305199\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 128.84009145805936\n",
      "Train_BestReturn : 128.84009145805936\n",
      "TimeSinceStart : 1007.3051986694336\n",
      "Training Loss : 0.7062638401985168\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 123.749305\n",
      "best mean reward 128.840091\n",
      "running time 1041.019474\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 123.74930534534406\n",
      "Train_BestReturn : 128.84009145805936\n",
      "TimeSinceStart : 1041.0194735527039\n",
      "Training Loss : 0.1349702626466751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 100.083907\n",
      "best mean reward 128.840091\n",
      "running time 1072.684885\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 100.08390727974397\n",
      "Train_BestReturn : 128.84009145805936\n",
      "TimeSinceStart : 1072.6848847866058\n",
      "Training Loss : 0.5642826557159424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 101.482183\n",
      "best mean reward 128.840091\n",
      "running time 1107.942285\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 101.48218326713543\n",
      "Train_BestReturn : 128.84009145805936\n",
      "TimeSinceStart : 1107.9422850608826\n",
      "Training Loss : 0.18301883339881897\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 123.126999\n",
      "best mean reward 128.840091\n",
      "running time 1138.695117\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 123.12699889335784\n",
      "Train_BestReturn : 128.84009145805936\n",
      "TimeSinceStart : 1138.6951167583466\n",
      "Training Loss : 0.7986015677452087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 142.137294\n",
      "best mean reward 142.137294\n",
      "running time 1169.692978\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 142.13729431667687\n",
      "Train_BestReturn : 142.13729431667687\n",
      "TimeSinceStart : 1169.6929779052734\n",
      "Training Loss : 0.4692460298538208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 146.671557\n",
      "best mean reward 146.671557\n",
      "running time 1204.414142\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 146.67155749204235\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1204.4141418933868\n",
      "Training Loss : 0.24226266145706177\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 137.854439\n",
      "best mean reward 146.671557\n",
      "running time 1241.858780\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 137.85443908313573\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1241.858779668808\n",
      "Training Loss : 0.12014856934547424\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 138.790732\n",
      "best mean reward 146.671557\n",
      "running time 1277.870499\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 138.79073202701773\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1277.8704988956451\n",
      "Training Loss : 0.26510709524154663\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 122.398303\n",
      "best mean reward 146.671557\n",
      "running time 1309.540535\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 122.39830257930245\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1309.540534734726\n",
      "Training Loss : 0.7288417816162109\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 101.381744\n",
      "best mean reward 146.671557\n",
      "running time 1340.711542\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 101.38174409318643\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1340.711541891098\n",
      "Training Loss : 0.3554280996322632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 89.753452\n",
      "best mean reward 146.671557\n",
      "running time 1371.702740\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 89.75345192300745\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1371.702740430832\n",
      "Training Loss : 2.9660985469818115\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 89.753371\n",
      "best mean reward 146.671557\n",
      "running time 1402.257091\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 89.75337082567161\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1402.2570912837982\n",
      "Training Loss : 1.088491439819336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 110.193464\n",
      "best mean reward 146.671557\n",
      "running time 1432.592346\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 110.19346401237738\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1432.5923464298248\n",
      "Training Loss : 0.20754316449165344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 133.145414\n",
      "best mean reward 146.671557\n",
      "running time 1463.507811\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 133.14541359231228\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1463.507811307907\n",
      "Training Loss : 0.4643796682357788\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 138.959527\n",
      "best mean reward 146.671557\n",
      "running time 1494.378850\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 138.95952698085043\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1494.3788504600525\n",
      "Training Loss : 0.47305142879486084\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 103.832306\n",
      "best mean reward 146.671557\n",
      "running time 1526.914747\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 103.83230608551325\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1526.9147465229034\n",
      "Training Loss : 0.12149520963430405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 75.502616\n",
      "best mean reward 146.671557\n",
      "running time 1557.676437\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 75.50261567025338\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1557.6764369010925\n",
      "Training Loss : 3.652029514312744\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 29.914848\n",
      "best mean reward 146.671557\n",
      "running time 1587.927473\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 29.9148483194009\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1587.9274728298187\n",
      "Training Loss : 0.22800149023532867\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 82.386192\n",
      "best mean reward 146.671557\n",
      "running time 1617.802161\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 82.38619194684262\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1617.8021614551544\n",
      "Training Loss : 4.690451622009277\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 40.505906\n",
      "best mean reward 146.671557\n",
      "running time 1646.456524\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 40.50590562495474\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1646.4565241336823\n",
      "Training Loss : 0.6797274947166443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 58.945833\n",
      "best mean reward 146.671557\n",
      "running time 1676.456294\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 58.94583347402424\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1676.4562938213348\n",
      "Training Loss : 0.8149582147598267\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 46.044541\n",
      "best mean reward 146.671557\n",
      "running time 1708.439688\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 46.044540647913855\n",
      "Train_BestReturn : 146.67155749204235\n",
      "TimeSinceStart : 1708.4396879673004\n",
      "Training Loss : 5.873178958892822\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_6_LunarLander-v3_17-10-2020_16-34-41 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_6_LunarLander-v3_17-10-2020_16-34-41\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006726\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.00672602653503418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -239.880056\n",
      "best mean reward -inf\n",
      "running time 38.936655\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -239.8800562512922\n",
      "TimeSinceStart : 38.93665528297424\n",
      "Training Loss : 3.7260992527008057\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -196.072157\n",
      "best mean reward -196.072157\n",
      "running time 69.082393\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -196.0721574886977\n",
      "Train_BestReturn : -196.0721574886977\n",
      "TimeSinceStart : 69.08239340782166\n",
      "Training Loss : 0.46638023853302\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -173.387528\n",
      "best mean reward -173.387528\n",
      "running time 104.846789\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -173.38752844099193\n",
      "Train_BestReturn : -173.38752844099193\n",
      "TimeSinceStart : 104.84678912162781\n",
      "Training Loss : 0.4364835023880005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -156.561511\n",
      "best mean reward -156.561511\n",
      "running time 156.831340\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -156.56151081111534\n",
      "Train_BestReturn : -156.56151081111534\n",
      "TimeSinceStart : 156.8313398361206\n",
      "Training Loss : 0.33106595277786255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -148.083309\n",
      "best mean reward -148.083309\n",
      "running time 191.961742\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -148.0833094987096\n",
      "Train_BestReturn : -148.0833094987096\n",
      "TimeSinceStart : 191.96174240112305\n",
      "Training Loss : 0.6232320666313171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -123.249304\n",
      "best mean reward -123.249304\n",
      "running time 225.827266\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -123.24930374579264\n",
      "Train_BestReturn : -123.24930374579264\n",
      "TimeSinceStart : 225.82726645469666\n",
      "Training Loss : 0.6798598766326904\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -95.383050\n",
      "best mean reward -95.383050\n",
      "running time 260.025218\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -95.38304952968751\n",
      "Train_BestReturn : -95.38304952968751\n",
      "TimeSinceStart : 260.0252175331116\n",
      "Training Loss : 0.3454241454601288\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -84.070792\n",
      "best mean reward -84.070792\n",
      "running time 294.741955\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -84.07079223877254\n",
      "Train_BestReturn : -84.07079223877254\n",
      "TimeSinceStart : 294.7419548034668\n",
      "Training Loss : 0.2604786157608032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -75.727565\n",
      "best mean reward -75.727565\n",
      "running time 327.898013\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -75.72756503632024\n",
      "Train_BestReturn : -75.72756503632024\n",
      "TimeSinceStart : 327.89801263809204\n",
      "Training Loss : 0.21145369112491608\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -79.036627\n",
      "best mean reward -75.727565\n",
      "running time 362.457600\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -79.03662661270485\n",
      "Train_BestReturn : -75.72756503632024\n",
      "TimeSinceStart : 362.45760011672974\n",
      "Training Loss : 0.1582896113395691\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -66.264448\n",
      "best mean reward -66.264448\n",
      "running time 410.908566\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -66.26444808450036\n",
      "Train_BestReturn : -66.26444808450036\n",
      "TimeSinceStart : 410.90856552124023\n",
      "Training Loss : 0.1862279772758484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -50.556433\n",
      "best mean reward -50.556433\n",
      "running time 445.717222\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -50.55643279962316\n",
      "Train_BestReturn : -50.55643279962316\n",
      "TimeSinceStart : 445.71722173690796\n",
      "Training Loss : 0.10019265115261078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -48.534627\n",
      "best mean reward -48.534627\n",
      "running time 480.179783\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -48.534627185498586\n",
      "Train_BestReturn : -48.534627185498586\n",
      "TimeSinceStart : 480.17978262901306\n",
      "Training Loss : 0.08182297646999359\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -44.614336\n",
      "best mean reward -44.614336\n",
      "running time 515.727090\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -44.61433556250436\n",
      "Train_BestReturn : -44.61433556250436\n",
      "TimeSinceStart : 515.7270901203156\n",
      "Training Loss : 0.13701769709587097\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -28.268160\n",
      "best mean reward -28.268160\n",
      "running time 550.346013\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -28.2681597623943\n",
      "Train_BestReturn : -28.2681597623943\n",
      "TimeSinceStart : 550.3460130691528\n",
      "Training Loss : 0.095058374106884\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) -4.095891\n",
      "best mean reward -4.095891\n",
      "running time 582.752121\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : -4.09589096068167\n",
      "Train_BestReturn : -4.09589096068167\n",
      "TimeSinceStart : 582.7521207332611\n",
      "Training Loss : 0.4633733332157135\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 5.362201\n",
      "best mean reward 5.362201\n",
      "running time 617.546886\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 5.36220064095981\n",
      "Train_BestReturn : 5.36220064095981\n",
      "TimeSinceStart : 617.5468857288361\n",
      "Training Loss : 0.106673464179039\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 21.798555\n",
      "best mean reward 21.798555\n",
      "running time 648.873157\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 21.798555035811876\n",
      "Train_BestReturn : 21.798555035811876\n",
      "TimeSinceStart : 648.8731570243835\n",
      "Training Loss : 0.1295410543680191\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 43.408127\n",
      "best mean reward 43.408127\n",
      "running time 684.664087\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 43.408126981518464\n",
      "Train_BestReturn : 43.408126981518464\n",
      "TimeSinceStart : 684.6640868186951\n",
      "Training Loss : 0.09271043539047241\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 57.714321\n",
      "best mean reward 57.714321\n",
      "running time 715.974689\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 57.71432097354162\n",
      "Train_BestReturn : 57.71432097354162\n",
      "TimeSinceStart : 715.9746890068054\n",
      "Training Loss : 0.19603298604488373\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 67.940233\n",
      "best mean reward 67.940233\n",
      "running time 746.505259\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 67.9402331416023\n",
      "Train_BestReturn : 67.9402331416023\n",
      "TimeSinceStart : 746.5052587985992\n",
      "Training Loss : 2.0623619556427\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 76.780141\n",
      "best mean reward 76.780141\n",
      "running time 777.011797\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 76.7801408906662\n",
      "Train_BestReturn : 76.7801408906662\n",
      "TimeSinceStart : 777.0117974281311\n",
      "Training Loss : 0.12583838403224945\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 74.804223\n",
      "best mean reward 76.780141\n",
      "running time 807.831254\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 74.80422285750949\n",
      "Train_BestReturn : 76.7801408906662\n",
      "TimeSinceStart : 807.8312542438507\n",
      "Training Loss : 0.1516311764717102\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 79.402896\n",
      "best mean reward 79.402896\n",
      "running time 840.648985\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 79.40289636815122\n",
      "Train_BestReturn : 79.40289636815122\n",
      "TimeSinceStart : 840.6489849090576\n",
      "Training Loss : 0.06804513931274414\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 78.113388\n",
      "best mean reward 79.402896\n",
      "running time 871.836142\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 78.11338791158552\n",
      "Train_BestReturn : 79.40289636815122\n",
      "TimeSinceStart : 871.8361415863037\n",
      "Training Loss : 1.8382389545440674\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 71.452773\n",
      "best mean reward 79.402896\n",
      "running time 910.497719\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 71.45277291510244\n",
      "Train_BestReturn : 79.40289636815122\n",
      "TimeSinceStart : 910.4977192878723\n",
      "Training Loss : 0.06806100904941559\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 74.761115\n",
      "best mean reward 79.402896\n",
      "running time 939.645218\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 74.76111538274802\n",
      "Train_BestReturn : 79.40289636815122\n",
      "TimeSinceStart : 939.6452178955078\n",
      "Training Loss : 0.2751874327659607\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 90.503410\n",
      "best mean reward 90.503410\n",
      "running time 968.380688\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 90.50341020148315\n",
      "Train_BestReturn : 90.50341020148315\n",
      "TimeSinceStart : 968.3806884288788\n",
      "Training Loss : 0.9510519504547119\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 121.139399\n",
      "best mean reward 121.139399\n",
      "running time 995.275798\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 121.13939894927145\n",
      "Train_BestReturn : 121.13939894927145\n",
      "TimeSinceStart : 995.2757980823517\n",
      "Training Loss : 0.10785959661006927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 127.431246\n",
      "best mean reward 127.431246\n",
      "running time 1024.680130\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 127.43124582028791\n",
      "Train_BestReturn : 127.43124582028791\n",
      "TimeSinceStart : 1024.68013048172\n",
      "Training Loss : 1.1144897937774658\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 128.306764\n",
      "best mean reward 128.306764\n",
      "running time 1054.282809\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 128.30676408680682\n",
      "Train_BestReturn : 128.30676408680682\n",
      "TimeSinceStart : 1054.2828092575073\n",
      "Training Loss : 0.10812832415103912\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 137.489440\n",
      "best mean reward 137.489440\n",
      "running time 1080.898235\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 137.48944011215949\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1080.8982350826263\n",
      "Training Loss : 0.10756789147853851\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 113.264369\n",
      "best mean reward 137.489440\n",
      "running time 1107.859878\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 113.26436940002966\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1107.8598775863647\n",
      "Training Loss : 2.6712305545806885\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 97.323557\n",
      "best mean reward 137.489440\n",
      "running time 1135.024944\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 97.32355744479086\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1135.0249435901642\n",
      "Training Loss : 0.14514832198619843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 114.229603\n",
      "best mean reward 137.489440\n",
      "running time 1163.349527\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 114.22960293377201\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1163.3495268821716\n",
      "Training Loss : 0.1649983823299408\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 114.189021\n",
      "best mean reward 137.489440\n",
      "running time 1199.711807\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 114.18902055347722\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1199.711807012558\n",
      "Training Loss : 0.16277551651000977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 123.231832\n",
      "best mean reward 137.489440\n",
      "running time 1227.589517\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 123.23183209014329\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1227.5895173549652\n",
      "Training Loss : 0.1054958701133728\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 125.124230\n",
      "best mean reward 137.489440\n",
      "running time 1255.992145\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 125.12423013625376\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1255.992145061493\n",
      "Training Loss : 1.0142468214035034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 118.977816\n",
      "best mean reward 137.489440\n",
      "running time 1285.555525\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 118.97781582536898\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1285.5555250644684\n",
      "Training Loss : 0.11409692466259003\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 135.646501\n",
      "best mean reward 137.489440\n",
      "running time 1315.422288\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 135.64650081895385\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1315.4222881793976\n",
      "Training Loss : 0.11298410594463348\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 120.277760\n",
      "best mean reward 137.489440\n",
      "running time 1344.048348\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 120.27776009708822\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1344.048347711563\n",
      "Training Loss : 0.6215236186981201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 136.024568\n",
      "best mean reward 137.489440\n",
      "running time 1371.827047\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 136.02456750318922\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1371.8270473480225\n",
      "Training Loss : 0.08055716753005981\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 129.497929\n",
      "best mean reward 137.489440\n",
      "running time 1399.200452\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 129.49792861127082\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1399.200451850891\n",
      "Training Loss : 0.15631937980651855\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 106.959161\n",
      "best mean reward 137.489440\n",
      "running time 1428.250569\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 106.9591606751143\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1428.2505691051483\n",
      "Training Loss : 0.1340724229812622\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 98.217682\n",
      "best mean reward 137.489440\n",
      "running time 1455.124671\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 98.21768160924387\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1455.124671459198\n",
      "Training Loss : 0.5331435203552246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 90.142699\n",
      "best mean reward 137.489440\n",
      "running time 1482.864021\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 90.14269914397897\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1482.8640205860138\n",
      "Training Loss : 0.1222185343503952\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 95.556709\n",
      "best mean reward 137.489440\n",
      "running time 1509.374986\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 95.55670945520944\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1509.374986410141\n",
      "Training Loss : 0.27750352025032043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 109.131538\n",
      "best mean reward 137.489440\n",
      "running time 1536.914007\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 109.13153759214201\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1536.9140067100525\n",
      "Training Loss : 0.22766780853271484\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 90.923042\n",
      "best mean reward 137.489440\n",
      "running time 1564.251361\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 90.92304200536405\n",
      "Train_BestReturn : 137.48944011215949\n",
      "TimeSinceStart : 1564.251361131668\n",
      "Training Loss : 0.9249432682991028\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_7_LunarLander-v3_17-10-2020_17-01-17 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_7_LunarLander-v3_17-10-2020_17-01-17\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006987\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0069866180419921875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -271.570212\n",
      "best mean reward -inf\n",
      "running time 36.706827\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -271.57021189406936\n",
      "TimeSinceStart : 36.70682716369629\n",
      "Training Loss : 3.6989617347717285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -217.052808\n",
      "best mean reward -217.052808\n",
      "running time 67.237728\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -217.05280815153804\n",
      "Train_BestReturn : -217.05280815153804\n",
      "TimeSinceStart : 67.23772764205933\n",
      "Training Loss : 0.8206982612609863\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -196.291916\n",
      "best mean reward -196.291916\n",
      "running time 106.022370\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -196.29191569622407\n",
      "Train_BestReturn : -196.29191569622407\n",
      "TimeSinceStart : 106.0223696231842\n",
      "Training Loss : 0.8772880434989929\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -179.597593\n",
      "best mean reward -179.597593\n",
      "running time 162.863310\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -179.5975928960084\n",
      "Train_BestReturn : -179.5975928960084\n",
      "TimeSinceStart : 162.86331009864807\n",
      "Training Loss : 0.7867158055305481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -159.151734\n",
      "best mean reward -159.151734\n",
      "running time 203.814958\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -159.15173415049838\n",
      "Train_BestReturn : -159.15173415049838\n",
      "TimeSinceStart : 203.81495785713196\n",
      "Training Loss : 0.7933980822563171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -145.449188\n",
      "best mean reward -145.449188\n",
      "running time 244.051434\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -145.44918824852746\n",
      "Train_BestReturn : -145.44918824852746\n",
      "TimeSinceStart : 244.05143404006958\n",
      "Training Loss : 1.5625872611999512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -118.737310\n",
      "best mean reward -118.737310\n",
      "running time 280.590917\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -118.73731001506557\n",
      "Train_BestReturn : -118.73731001506557\n",
      "TimeSinceStart : 280.59091687202454\n",
      "Training Loss : 0.3525308668613434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -97.030654\n",
      "best mean reward -97.030654\n",
      "running time 317.169107\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -97.03065441562738\n",
      "Train_BestReturn : -97.03065441562738\n",
      "TimeSinceStart : 317.1691074371338\n",
      "Training Loss : 0.6822446584701538\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -87.609490\n",
      "best mean reward -87.609490\n",
      "running time 354.487876\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -87.60948994473371\n",
      "Train_BestReturn : -87.60948994473371\n",
      "TimeSinceStart : 354.4878761768341\n",
      "Training Loss : 0.23293986916542053\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -64.810353\n",
      "best mean reward -64.810353\n",
      "running time 408.569443\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -64.81035291831262\n",
      "Train_BestReturn : -64.81035291831262\n",
      "TimeSinceStart : 408.5694434642792\n",
      "Training Loss : 0.4724266529083252\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -37.389463\n",
      "best mean reward -37.389463\n",
      "running time 445.620567\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -37.389462946544064\n",
      "Train_BestReturn : -37.389462946544064\n",
      "TimeSinceStart : 445.62056708335876\n",
      "Training Loss : 0.3159463703632355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -21.373679\n",
      "best mean reward -21.373679\n",
      "running time 482.452456\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -21.37367868273512\n",
      "Train_BestReturn : -21.37367868273512\n",
      "TimeSinceStart : 482.4524555206299\n",
      "Training Loss : 0.5294188261032104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -13.658423\n",
      "best mean reward -13.658423\n",
      "running time 519.030973\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -13.658423075007592\n",
      "Train_BestReturn : -13.658423075007592\n",
      "TimeSinceStart : 519.0309731960297\n",
      "Training Loss : 0.8339176177978516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 15.750145\n",
      "best mean reward 15.750145\n",
      "running time 554.491235\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 15.750145064401813\n",
      "Train_BestReturn : 15.750145064401813\n",
      "TimeSinceStart : 554.4912354946136\n",
      "Training Loss : 0.32898151874542236\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 40.573696\n",
      "best mean reward 40.573696\n",
      "running time 588.557540\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 40.573695961779315\n",
      "Train_BestReturn : 40.573695961779315\n",
      "TimeSinceStart : 588.5575399398804\n",
      "Training Loss : 0.2841836214065552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 42.591466\n",
      "best mean reward 42.591466\n",
      "running time 624.844049\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 42.591466270504505\n",
      "Train_BestReturn : 42.591466270504505\n",
      "TimeSinceStart : 624.8440489768982\n",
      "Training Loss : 0.08006070554256439\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 66.516625\n",
      "best mean reward 66.516625\n",
      "running time 671.980431\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 66.51662521287537\n",
      "Train_BestReturn : 66.51662521287537\n",
      "TimeSinceStart : 671.9804310798645\n",
      "Training Loss : 0.43372753262519836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 64.899348\n",
      "best mean reward 66.516625\n",
      "running time 712.536266\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 64.89934820216291\n",
      "Train_BestReturn : 66.51662521287537\n",
      "TimeSinceStart : 712.5362660884857\n",
      "Training Loss : 0.10796010494232178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 64.745434\n",
      "best mean reward 66.516625\n",
      "running time 753.124471\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 64.74543426061727\n",
      "Train_BestReturn : 66.51662521287537\n",
      "TimeSinceStart : 753.1244714260101\n",
      "Training Loss : 0.15837961435317993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 74.905583\n",
      "best mean reward 74.905583\n",
      "running time 791.195458\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 74.90558291538277\n",
      "Train_BestReturn : 74.90558291538277\n",
      "TimeSinceStart : 791.1954584121704\n",
      "Training Loss : 0.08276107907295227\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 68.664805\n",
      "best mean reward 74.905583\n",
      "running time 830.479514\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 68.66480527231577\n",
      "Train_BestReturn : 74.90558291538277\n",
      "TimeSinceStart : 830.4795143604279\n",
      "Training Loss : 0.11999838054180145\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 66.378210\n",
      "best mean reward 74.905583\n",
      "running time 866.411822\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 66.3782095677195\n",
      "Train_BestReturn : 74.90558291538277\n",
      "TimeSinceStart : 866.411821603775\n",
      "Training Loss : 0.10225286334753036\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 76.477829\n",
      "best mean reward 76.477829\n",
      "running time 901.901633\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 76.4778291139391\n",
      "Train_BestReturn : 76.4778291139391\n",
      "TimeSinceStart : 901.9016325473785\n",
      "Training Loss : 2.400186777114868\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 78.813410\n",
      "best mean reward 78.813410\n",
      "running time 952.150401\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 78.81341022476957\n",
      "Train_BestReturn : 78.81341022476957\n",
      "TimeSinceStart : 952.1504011154175\n",
      "Training Loss : 0.24412387609481812\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 100.433348\n",
      "best mean reward 100.433348\n",
      "running time 986.602271\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 100.43334773581468\n",
      "Train_BestReturn : 100.43334773581468\n",
      "TimeSinceStart : 986.6022706031799\n",
      "Training Loss : 1.0746610164642334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 103.923542\n",
      "best mean reward 103.923542\n",
      "running time 1023.103397\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 103.9235416736353\n",
      "Train_BestReturn : 103.9235416736353\n",
      "TimeSinceStart : 1023.1033973693848\n",
      "Training Loss : 0.34114861488342285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 111.521421\n",
      "best mean reward 111.521421\n",
      "running time 1056.232708\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 111.52142091857141\n",
      "Train_BestReturn : 111.52142091857141\n",
      "TimeSinceStart : 1056.2327077388763\n",
      "Training Loss : 0.858157217502594\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 115.643098\n",
      "best mean reward 115.643098\n",
      "running time 1093.505106\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 115.64309760895995\n",
      "Train_BestReturn : 115.64309760895995\n",
      "TimeSinceStart : 1093.5051062107086\n",
      "Training Loss : 0.1696527600288391\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 109.372268\n",
      "best mean reward 115.643098\n",
      "running time 1135.181126\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 109.3722680628743\n",
      "Train_BestReturn : 115.64309760895995\n",
      "TimeSinceStart : 1135.1811258792877\n",
      "Training Loss : 0.23159973323345184\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 111.070898\n",
      "best mean reward 115.643098\n",
      "running time 1175.272647\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 111.07089807763063\n",
      "Train_BestReturn : 115.64309760895995\n",
      "TimeSinceStart : 1175.2726466655731\n",
      "Training Loss : 0.27364200353622437\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 122.780786\n",
      "best mean reward 122.780786\n",
      "running time 1209.852330\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 122.78078634239256\n",
      "Train_BestReturn : 122.78078634239256\n",
      "TimeSinceStart : 1209.8523299694061\n",
      "Training Loss : 0.6344826817512512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 135.185552\n",
      "best mean reward 135.185552\n",
      "running time 1250.526272\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 135.18555171485272\n",
      "Train_BestReturn : 135.18555171485272\n",
      "TimeSinceStart : 1250.526272058487\n",
      "Training Loss : 0.13438184559345245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 140.346575\n",
      "best mean reward 140.346575\n",
      "running time 1284.233629\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 140.3465748979797\n",
      "Train_BestReturn : 140.3465748979797\n",
      "TimeSinceStart : 1284.233628988266\n",
      "Training Loss : 0.36782318353652954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 145.302367\n",
      "best mean reward 145.302367\n",
      "running time 1316.811556\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 145.30236746936197\n",
      "Train_BestReturn : 145.30236746936197\n",
      "TimeSinceStart : 1316.811556339264\n",
      "Training Loss : 0.19147524237632751\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 167.334164\n",
      "best mean reward 167.334164\n",
      "running time 1349.372366\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 167.33416392059323\n",
      "Train_BestReturn : 167.33416392059323\n",
      "TimeSinceStart : 1349.3723661899567\n",
      "Training Loss : 0.12836411595344543\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 161.296959\n",
      "best mean reward 167.334164\n",
      "running time 1380.791302\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 161.29695913434577\n",
      "Train_BestReturn : 167.33416392059323\n",
      "TimeSinceStart : 1380.7913024425507\n",
      "Training Loss : 0.879607617855072\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 158.936212\n",
      "best mean reward 167.334164\n",
      "running time 1412.165759\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 158.93621226169262\n",
      "Train_BestReturn : 167.33416392059323\n",
      "TimeSinceStart : 1412.1657588481903\n",
      "Training Loss : 0.1654711216688156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 176.531066\n",
      "best mean reward 176.531066\n",
      "running time 1445.716830\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 176.53106562124083\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1445.7168300151825\n",
      "Training Loss : 0.699215292930603\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 176.057963\n",
      "best mean reward 176.531066\n",
      "running time 1475.748344\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 176.0579633863786\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1475.7483439445496\n",
      "Training Loss : 3.5876801013946533\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 163.807180\n",
      "best mean reward 176.531066\n",
      "running time 1505.433136\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 163.80717980114008\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1505.4331357479095\n",
      "Training Loss : 0.20572499930858612\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 171.336902\n",
      "best mean reward 176.531066\n",
      "running time 1536.811880\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 171.33690165607618\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1536.8118798732758\n",
      "Training Loss : 6.5889997482299805\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 174.658552\n",
      "best mean reward 176.531066\n",
      "running time 1567.748693\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 174.6585519854022\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1567.7486929893494\n",
      "Training Loss : 2.307943105697632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 157.811027\n",
      "best mean reward 176.531066\n",
      "running time 1597.557267\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 157.81102667539753\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1597.5572674274445\n",
      "Training Loss : 2.9634451866149902\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 133.781768\n",
      "best mean reward 176.531066\n",
      "running time 1629.332775\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 133.78176804811076\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1629.3327748775482\n",
      "Training Loss : 0.05338750779628754\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 131.669949\n",
      "best mean reward 176.531066\n",
      "running time 1659.649330\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 131.66994939112325\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1659.6493299007416\n",
      "Training Loss : 0.1370391994714737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 70.631664\n",
      "best mean reward 176.531066\n",
      "running time 1689.334869\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 70.63166421875486\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1689.3348689079285\n",
      "Training Loss : 5.786623001098633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 50.815229\n",
      "best mean reward 176.531066\n",
      "running time 1720.890873\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 50.81522935936515\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1720.8908727169037\n",
      "Training Loss : 4.987591743469238\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 10.509640\n",
      "best mean reward 176.531066\n",
      "running time 1750.059913\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 10.509639777199183\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1750.0599126815796\n",
      "Training Loss : 0.1612243354320526\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 4.383148\n",
      "best mean reward 176.531066\n",
      "running time 1780.720044\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 4.383148057220605\n",
      "Train_BestReturn : 176.53106562124083\n",
      "TimeSinceStart : 1780.7200436592102\n",
      "Training Loss : 2.0794425010681152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_7_LunarLander-v3_17-10-2020_17-31-33 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_7_LunarLander-v3_17-10-2020_17-31-33\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.007382\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0073816776275634766\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -269.292148\n",
      "best mean reward -inf\n",
      "running time 36.432614\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -269.2921479060446\n",
      "TimeSinceStart : 36.43261408805847\n",
      "Training Loss : 3.9727272987365723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -220.669169\n",
      "best mean reward -220.669169\n",
      "running time 65.769837\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -220.66916904794402\n",
      "Train_BestReturn : -220.66916904794402\n",
      "TimeSinceStart : 65.76983666419983\n",
      "Training Loss : 0.5065169930458069\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -189.101490\n",
      "best mean reward -189.101490\n",
      "running time 102.127116\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -189.10149006578015\n",
      "Train_BestReturn : -189.10149006578015\n",
      "TimeSinceStart : 102.12711596488953\n",
      "Training Loss : 3.747302532196045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -176.189052\n",
      "best mean reward -176.189052\n",
      "running time 156.639284\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -176.18905172283885\n",
      "Train_BestReturn : -176.18905172283885\n",
      "TimeSinceStart : 156.6392843723297\n",
      "Training Loss : 3.585723400115967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -163.754352\n",
      "best mean reward -163.754352\n",
      "running time 192.101284\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -163.75435224550674\n",
      "Train_BestReturn : -163.75435224550674\n",
      "TimeSinceStart : 192.10128378868103\n",
      "Training Loss : 0.25060978531837463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -134.298569\n",
      "best mean reward -134.298569\n",
      "running time 227.705865\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -134.29856937151976\n",
      "Train_BestReturn : -134.29856937151976\n",
      "TimeSinceStart : 227.70586490631104\n",
      "Training Loss : 1.4062680006027222\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -111.009123\n",
      "best mean reward -111.009123\n",
      "running time 262.240002\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -111.00912296852758\n",
      "Train_BestReturn : -111.00912296852758\n",
      "TimeSinceStart : 262.24000239372253\n",
      "Training Loss : 0.3885074555873871\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -94.725824\n",
      "best mean reward -94.725824\n",
      "running time 295.058453\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -94.72582379182386\n",
      "Train_BestReturn : -94.72582379182386\n",
      "TimeSinceStart : 295.05845284461975\n",
      "Training Loss : 0.2555729150772095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -74.599873\n",
      "best mean reward -74.599873\n",
      "running time 333.117590\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -74.59987251577986\n",
      "Train_BestReturn : -74.59987251577986\n",
      "TimeSinceStart : 333.1175899505615\n",
      "Training Loss : 0.2252773642539978\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -58.325693\n",
      "best mean reward -58.325693\n",
      "running time 373.923394\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -58.325693009732774\n",
      "Train_BestReturn : -58.325693009732774\n",
      "TimeSinceStart : 373.9233944416046\n",
      "Training Loss : 0.18262626230716705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -41.564574\n",
      "best mean reward -41.564574\n",
      "running time 409.425570\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -41.56457400410377\n",
      "Train_BestReturn : -41.56457400410377\n",
      "TimeSinceStart : 409.42556977272034\n",
      "Training Loss : 0.36529040336608887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -32.817778\n",
      "best mean reward -32.817778\n",
      "running time 446.581909\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -32.81777845179345\n",
      "Train_BestReturn : -32.81777845179345\n",
      "TimeSinceStart : 446.58190870285034\n",
      "Training Loss : 1.2064235210418701\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -26.929946\n",
      "best mean reward -26.929946\n",
      "running time 487.867837\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -26.929946495524014\n",
      "Train_BestReturn : -26.929946495524014\n",
      "TimeSinceStart : 487.86783719062805\n",
      "Training Loss : 0.829420268535614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -16.698724\n",
      "best mean reward -16.698724\n",
      "running time 528.962324\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -16.69872365703552\n",
      "Train_BestReturn : -16.69872365703552\n",
      "TimeSinceStart : 528.962324142456\n",
      "Training Loss : 4.008630752563477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -5.882821\n",
      "best mean reward -5.882821\n",
      "running time 567.739290\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -5.882820831420587\n",
      "Train_BestReturn : -5.882820831420587\n",
      "TimeSinceStart : 567.7392904758453\n",
      "Training Loss : 0.11400695145130157\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 0.845280\n",
      "best mean reward 0.845280\n",
      "running time 611.947876\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 0.8452798257884885\n",
      "Train_BestReturn : 0.8452798257884885\n",
      "TimeSinceStart : 611.9478764533997\n",
      "Training Loss : 0.4538326561450958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 7.213997\n",
      "best mean reward 7.213997\n",
      "running time 650.427407\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 7.213997038191543\n",
      "Train_BestReturn : 7.213997038191543\n",
      "TimeSinceStart : 650.4274067878723\n",
      "Training Loss : 0.08094756305217743\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 18.843091\n",
      "best mean reward 18.843091\n",
      "running time 688.976867\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 18.84309132935957\n",
      "Train_BestReturn : 18.84309132935957\n",
      "TimeSinceStart : 688.9768671989441\n",
      "Training Loss : 0.7629625797271729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 16.981533\n",
      "best mean reward 18.843091\n",
      "running time 731.786531\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 16.98153328010311\n",
      "Train_BestReturn : 18.84309132935957\n",
      "TimeSinceStart : 731.7865314483643\n",
      "Training Loss : 0.1212637722492218\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 21.200612\n",
      "best mean reward 21.200612\n",
      "running time 767.142215\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 21.20061193935517\n",
      "Train_BestReturn : 21.20061193935517\n",
      "TimeSinceStart : 767.1422152519226\n",
      "Training Loss : 0.2266739010810852\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 30.781268\n",
      "best mean reward 30.781268\n",
      "running time 800.439893\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 30.78126796523047\n",
      "Train_BestReturn : 30.78126796523047\n",
      "TimeSinceStart : 800.439893245697\n",
      "Training Loss : 0.1600794494152069\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 42.954867\n",
      "best mean reward 42.954867\n",
      "running time 836.726509\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 42.954866922055295\n",
      "Train_BestReturn : 42.954866922055295\n",
      "TimeSinceStart : 836.7265086174011\n",
      "Training Loss : 0.17918509244918823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 62.836743\n",
      "best mean reward 62.836743\n",
      "running time 869.713736\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 62.83674286018391\n",
      "Train_BestReturn : 62.83674286018391\n",
      "TimeSinceStart : 869.7137360572815\n",
      "Training Loss : 0.20291106402873993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 78.097690\n",
      "best mean reward 78.097690\n",
      "running time 900.993480\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 78.09768960750824\n",
      "Train_BestReturn : 78.09768960750824\n",
      "TimeSinceStart : 900.9934804439545\n",
      "Training Loss : 3.321730375289917\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 86.117889\n",
      "best mean reward 86.117889\n",
      "running time 932.695242\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 86.11788858788239\n",
      "Train_BestReturn : 86.11788858788239\n",
      "TimeSinceStart : 932.6952419281006\n",
      "Training Loss : 1.2800092697143555\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 99.017778\n",
      "best mean reward 99.017778\n",
      "running time 960.714205\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 99.01777839783728\n",
      "Train_BestReturn : 99.01777839783728\n",
      "TimeSinceStart : 960.714204788208\n",
      "Training Loss : 0.13243882358074188\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 120.335009\n",
      "best mean reward 120.335009\n",
      "running time 992.622640\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 120.33500872451972\n",
      "Train_BestReturn : 120.33500872451972\n",
      "TimeSinceStart : 992.6226403713226\n",
      "Training Loss : 0.2043011486530304\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 121.036146\n",
      "best mean reward 121.036146\n",
      "running time 1023.135555\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 121.03614572815688\n",
      "Train_BestReturn : 121.03614572815688\n",
      "TimeSinceStart : 1023.1355545520782\n",
      "Training Loss : 0.7690123319625854\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 123.703260\n",
      "best mean reward 123.703260\n",
      "running time 1055.157536\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 123.7032598213535\n",
      "Train_BestReturn : 123.7032598213535\n",
      "TimeSinceStart : 1055.1575360298157\n",
      "Training Loss : 0.8259429931640625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 114.598645\n",
      "best mean reward 123.703260\n",
      "running time 1087.689639\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 114.59864505394604\n",
      "Train_BestReturn : 123.7032598213535\n",
      "TimeSinceStart : 1087.6896390914917\n",
      "Training Loss : 2.4187636375427246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 118.861037\n",
      "best mean reward 123.703260\n",
      "running time 1119.234000\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 118.86103743170817\n",
      "Train_BestReturn : 123.7032598213535\n",
      "TimeSinceStart : 1119.2339997291565\n",
      "Training Loss : 0.2406827211380005\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 141.987212\n",
      "best mean reward 141.987212\n",
      "running time 1146.205562\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 141.9872120116168\n",
      "Train_BestReturn : 141.9872120116168\n",
      "TimeSinceStart : 1146.2055621147156\n",
      "Training Loss : 1.4016247987747192\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 146.096058\n",
      "best mean reward 146.096058\n",
      "running time 1173.468401\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 146.09605792566705\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1173.4684007167816\n",
      "Training Loss : 2.1942873001098633\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 144.580127\n",
      "best mean reward 146.096058\n",
      "running time 1202.608698\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 144.58012744592267\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1202.608698129654\n",
      "Training Loss : 3.7228810787200928\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 135.197109\n",
      "best mean reward 146.096058\n",
      "running time 1231.744289\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 135.19710893947038\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1231.7442893981934\n",
      "Training Loss : 2.035489082336426\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 116.292352\n",
      "best mean reward 146.096058\n",
      "running time 1258.521064\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 116.29235212947637\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1258.5210635662079\n",
      "Training Loss : 0.6839472651481628\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 125.716964\n",
      "best mean reward 146.096058\n",
      "running time 1286.299095\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 125.71696438827803\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1286.29909491539\n",
      "Training Loss : 0.22238203883171082\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 108.097335\n",
      "best mean reward 146.096058\n",
      "running time 1315.067157\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 108.09733538544806\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1315.0671565532684\n",
      "Training Loss : 1.008116602897644\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 99.401177\n",
      "best mean reward 146.096058\n",
      "running time 1345.065015\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 99.40117660039837\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1345.0650148391724\n",
      "Training Loss : 0.28763115406036377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 43.949436\n",
      "best mean reward 146.096058\n",
      "running time 1373.463792\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 43.949436007361946\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1373.4637923240662\n",
      "Training Loss : 0.23204681277275085\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 40.527914\n",
      "best mean reward 146.096058\n",
      "running time 1401.219608\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 40.5279141480199\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1401.2196080684662\n",
      "Training Loss : 1.2206288576126099\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 58.784328\n",
      "best mean reward 146.096058\n",
      "running time 1430.181617\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 58.784328383655314\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1430.1816170215607\n",
      "Training Loss : 0.11434375494718552\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 78.505756\n",
      "best mean reward 146.096058\n",
      "running time 1457.481983\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 78.50575649249328\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1457.4819831848145\n",
      "Training Loss : 8.65200138092041\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 85.503118\n",
      "best mean reward 146.096058\n",
      "running time 1484.631312\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 85.50311764583228\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1484.6313118934631\n",
      "Training Loss : 0.5791298151016235\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 89.774287\n",
      "best mean reward 146.096058\n",
      "running time 1511.453565\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 89.77428731030484\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1511.453565120697\n",
      "Training Loss : 0.5989083051681519\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 87.045174\n",
      "best mean reward 146.096058\n",
      "running time 1538.909256\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 87.04517397005537\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1538.9092564582825\n",
      "Training Loss : 1.6931556463241577\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 91.614399\n",
      "best mean reward 146.096058\n",
      "running time 1567.375803\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 91.61439861900928\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1567.375803232193\n",
      "Training Loss : 0.34572839736938477\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 92.293947\n",
      "best mean reward 146.096058\n",
      "running time 1597.056592\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 92.29394734119245\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1597.056592464447\n",
      "Training Loss : 0.6780033111572266\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 100.694382\n",
      "best mean reward 146.096058\n",
      "running time 1628.097264\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 100.6943822494308\n",
      "Train_BestReturn : 146.09605792566705\n",
      "TimeSinceStart : 1628.097264289856\n",
      "Training Loss : 0.1816421002149582\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_8_LunarLander-v3_17-10-2020_17-59-13 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_8_LunarLander-v3_17-10-2020_17-59-13\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006600\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.0065996646881103516\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -239.691554\n",
      "best mean reward -inf\n",
      "running time 32.979043\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -239.6915536835006\n",
      "TimeSinceStart : 32.97904324531555\n",
      "Training Loss : 0.2825758457183838\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -187.416737\n",
      "best mean reward -187.416737\n",
      "running time 65.914434\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -187.4167368593627\n",
      "Train_BestReturn : -187.4167368593627\n",
      "TimeSinceStart : 65.91443371772766\n",
      "Training Loss : 4.4350738525390625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -171.373747\n",
      "best mean reward -171.373747\n",
      "running time 105.132158\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -171.3737469686608\n",
      "Train_BestReturn : -171.3737469686608\n",
      "TimeSinceStart : 105.13215756416321\n",
      "Training Loss : 1.08955979347229\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -153.803751\n",
      "best mean reward -153.803751\n",
      "running time 163.997091\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -153.80375086272068\n",
      "Train_BestReturn : -153.80375086272068\n",
      "TimeSinceStart : 163.99709105491638\n",
      "Training Loss : 0.6020019054412842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -147.126705\n",
      "best mean reward -147.126705\n",
      "running time 203.356313\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -147.12670502224097\n",
      "Train_BestReturn : -147.12670502224097\n",
      "TimeSinceStart : 203.35631346702576\n",
      "Training Loss : 0.33116406202316284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -135.865036\n",
      "best mean reward -135.865036\n",
      "running time 243.443823\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -135.86503573384832\n",
      "Train_BestReturn : -135.86503573384832\n",
      "TimeSinceStart : 243.44382309913635\n",
      "Training Loss : 0.35113680362701416\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -114.669166\n",
      "best mean reward -114.669166\n",
      "running time 282.165280\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -114.66916556645205\n",
      "Train_BestReturn : -114.66916556645205\n",
      "TimeSinceStart : 282.1652801036835\n",
      "Training Loss : 0.7600048780441284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -107.258989\n",
      "best mean reward -107.258989\n",
      "running time 321.799335\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -107.2589889104924\n",
      "Train_BestReturn : -107.2589889104924\n",
      "TimeSinceStart : 321.7993354797363\n",
      "Training Loss : 0.2273607701063156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -86.628906\n",
      "best mean reward -86.628906\n",
      "running time 360.664723\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -86.62890570255337\n",
      "Train_BestReturn : -86.62890570255337\n",
      "TimeSinceStart : 360.66472268104553\n",
      "Training Loss : 0.22767172753810883\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -61.793140\n",
      "best mean reward -61.793140\n",
      "running time 407.039338\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -61.793140345941275\n",
      "Train_BestReturn : -61.793140345941275\n",
      "TimeSinceStart : 407.039338350296\n",
      "Training Loss : 0.2089107632637024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -47.822906\n",
      "best mean reward -47.822906\n",
      "running time 443.249858\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -47.82290558198523\n",
      "Train_BestReturn : -47.82290558198523\n",
      "TimeSinceStart : 443.24985790252686\n",
      "Training Loss : 0.23140427470207214\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -17.831786\n",
      "best mean reward -17.831786\n",
      "running time 480.458688\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -17.831785557126256\n",
      "Train_BestReturn : -17.831785557126256\n",
      "TimeSinceStart : 480.4586880207062\n",
      "Training Loss : 0.37360209226608276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 1.017933\n",
      "best mean reward 1.017933\n",
      "running time 518.890077\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 1.0179329266237889\n",
      "Train_BestReturn : 1.0179329266237889\n",
      "TimeSinceStart : 518.8900766372681\n",
      "Training Loss : 0.7639432549476624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 13.728487\n",
      "best mean reward 13.728487\n",
      "running time 556.033159\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 13.72848721114434\n",
      "Train_BestReturn : 13.72848721114434\n",
      "TimeSinceStart : 556.0331585407257\n",
      "Training Loss : 0.14452773332595825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 30.901054\n",
      "best mean reward 30.901054\n",
      "running time 591.684684\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 30.901053513863644\n",
      "Train_BestReturn : 30.901053513863644\n",
      "TimeSinceStart : 591.6846835613251\n",
      "Training Loss : 0.400836318731308\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 33.717090\n",
      "best mean reward 33.717090\n",
      "running time 628.018560\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 33.71709010895937\n",
      "Train_BestReturn : 33.71709010895937\n",
      "TimeSinceStart : 628.0185599327087\n",
      "Training Loss : 0.2035943865776062\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 48.561765\n",
      "best mean reward 48.561765\n",
      "running time 663.714897\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 48.56176533655438\n",
      "Train_BestReturn : 48.56176533655438\n",
      "TimeSinceStart : 663.7148966789246\n",
      "Training Loss : 0.053807005286216736\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 68.914043\n",
      "best mean reward 68.914043\n",
      "running time 702.481876\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 68.91404279676877\n",
      "Train_BestReturn : 68.91404279676877\n",
      "TimeSinceStart : 702.4818756580353\n",
      "Training Loss : 0.08655218034982681\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 76.311010\n",
      "best mean reward 76.311010\n",
      "running time 737.181408\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 76.31100967179273\n",
      "Train_BestReturn : 76.31100967179273\n",
      "TimeSinceStart : 737.1814081668854\n",
      "Training Loss : 0.14734673500061035\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 84.680325\n",
      "best mean reward 84.680325\n",
      "running time 774.127409\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 84.68032483153061\n",
      "Train_BestReturn : 84.68032483153061\n",
      "TimeSinceStart : 774.1274085044861\n",
      "Training Loss : 0.5637937784194946\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 93.736523\n",
      "best mean reward 93.736523\n",
      "running time 807.200681\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 93.73652274231877\n",
      "Train_BestReturn : 93.73652274231877\n",
      "TimeSinceStart : 807.2006812095642\n",
      "Training Loss : 0.05483768880367279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 89.781186\n",
      "best mean reward 93.736523\n",
      "running time 845.432206\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 89.78118573183619\n",
      "Train_BestReturn : 93.73652274231877\n",
      "TimeSinceStart : 845.4322056770325\n",
      "Training Loss : 0.15733343362808228\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 89.890202\n",
      "best mean reward 93.736523\n",
      "running time 878.752839\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 89.89020158342626\n",
      "Train_BestReturn : 93.73652274231877\n",
      "TimeSinceStart : 878.7528388500214\n",
      "Training Loss : 0.151030495762825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 90.520967\n",
      "best mean reward 93.736523\n",
      "running time 915.091319\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 90.52096656546888\n",
      "Train_BestReturn : 93.73652274231877\n",
      "TimeSinceStart : 915.0913186073303\n",
      "Training Loss : 0.276791512966156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 89.450840\n",
      "best mean reward 93.736523\n",
      "running time 950.107481\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 89.45084015609629\n",
      "Train_BestReturn : 93.73652274231877\n",
      "TimeSinceStart : 950.1074805259705\n",
      "Training Loss : 0.16338452696800232\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 100.346812\n",
      "best mean reward 100.346812\n",
      "running time 987.187081\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 100.34681177478745\n",
      "Train_BestReturn : 100.34681177478745\n",
      "TimeSinceStart : 987.1870806217194\n",
      "Training Loss : 0.08460161089897156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 115.655113\n",
      "best mean reward 115.655113\n",
      "running time 1022.822306\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 115.65511332081724\n",
      "Train_BestReturn : 115.65511332081724\n",
      "TimeSinceStart : 1022.8223061561584\n",
      "Training Loss : 0.975547730922699\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 124.867018\n",
      "best mean reward 124.867018\n",
      "running time 1054.817128\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 124.86701765430253\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1054.8171277046204\n",
      "Training Loss : 0.38679295778274536\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 118.406656\n",
      "best mean reward 124.867018\n",
      "running time 1086.123773\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 118.40665646542732\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1086.1237726211548\n",
      "Training Loss : 0.21277011930942535\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 113.234593\n",
      "best mean reward 124.867018\n",
      "running time 1118.283186\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 113.23459322942257\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1118.2831861972809\n",
      "Training Loss : 1.1624130010604858\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 115.020590\n",
      "best mean reward 124.867018\n",
      "running time 1150.401544\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 115.02059033786777\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1150.4015443325043\n",
      "Training Loss : 1.4805331230163574\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 98.638015\n",
      "best mean reward 124.867018\n",
      "running time 1180.903767\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 98.63801468660002\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1180.9037671089172\n",
      "Training Loss : 0.274079293012619\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 105.251401\n",
      "best mean reward 124.867018\n",
      "running time 1211.108386\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 105.25140081357291\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1211.1083855628967\n",
      "Training Loss : 0.22530195116996765\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 107.770434\n",
      "best mean reward 124.867018\n",
      "running time 1240.216037\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 107.77043398444465\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1240.2160367965698\n",
      "Training Loss : 0.2540352940559387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 75.033074\n",
      "best mean reward 124.867018\n",
      "running time 1270.295227\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 75.0330739487975\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1270.2952270507812\n",
      "Training Loss : 2.9724607467651367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 52.364954\n",
      "best mean reward 124.867018\n",
      "running time 1301.784246\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 52.36495430531106\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1301.784245967865\n",
      "Training Loss : 0.9867066144943237\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 68.773660\n",
      "best mean reward 124.867018\n",
      "running time 1331.858142\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 68.77365987343853\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1331.858141899109\n",
      "Training Loss : 0.18346810340881348\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 96.780001\n",
      "best mean reward 124.867018\n",
      "running time 1363.066700\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 96.78000149136427\n",
      "Train_BestReturn : 124.86701765430253\n",
      "TimeSinceStart : 1363.0666997432709\n",
      "Training Loss : 2.054194927215576\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 130.685619\n",
      "best mean reward 130.685619\n",
      "running time 1392.153687\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 130.68561886878064\n",
      "Train_BestReturn : 130.68561886878064\n",
      "TimeSinceStart : 1392.1536874771118\n",
      "Training Loss : 1.3854289054870605\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 157.051974\n",
      "best mean reward 157.051974\n",
      "running time 1422.986248\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 157.05197412578923\n",
      "Train_BestReturn : 157.05197412578923\n",
      "TimeSinceStart : 1422.9862475395203\n",
      "Training Loss : 3.1679625511169434\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 146.373571\n",
      "best mean reward 157.051974\n",
      "running time 1452.563153\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 146.37357096705895\n",
      "Train_BestReturn : 157.05197412578923\n",
      "TimeSinceStart : 1452.5631530284882\n",
      "Training Loss : 0.20529823005199432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 139.322125\n",
      "best mean reward 157.051974\n",
      "running time 1480.831982\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 139.32212526444036\n",
      "Train_BestReturn : 157.05197412578923\n",
      "TimeSinceStart : 1480.8319823741913\n",
      "Training Loss : 3.481936454772949\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 149.755772\n",
      "best mean reward 157.051974\n",
      "running time 1510.013870\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 149.7557723301769\n",
      "Train_BestReturn : 157.05197412578923\n",
      "TimeSinceStart : 1510.013869524002\n",
      "Training Loss : 2.5883238315582275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 183.320580\n",
      "best mean reward 183.320580\n",
      "running time 1538.674198\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 183.32058023273493\n",
      "Train_BestReturn : 183.32058023273493\n",
      "TimeSinceStart : 1538.6741981506348\n",
      "Training Loss : 3.742330551147461\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 187.758833\n",
      "best mean reward 187.758833\n",
      "running time 1568.997898\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 187.75883300596234\n",
      "Train_BestReturn : 187.75883300596234\n",
      "TimeSinceStart : 1568.9978983402252\n",
      "Training Loss : 5.87452507019043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 178.564723\n",
      "best mean reward 187.758833\n",
      "running time 1598.617976\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 178.56472314553582\n",
      "Train_BestReturn : 187.75883300596234\n",
      "TimeSinceStart : 1598.6179757118225\n",
      "Training Loss : 3.047053813934326\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 176.315734\n",
      "best mean reward 187.758833\n",
      "running time 1626.814737\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 176.31573374672035\n",
      "Train_BestReturn : 187.75883300596234\n",
      "TimeSinceStart : 1626.8147366046906\n",
      "Training Loss : 0.5143492817878723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 197.321707\n",
      "best mean reward 197.321707\n",
      "running time 1654.553359\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 197.3217066470462\n",
      "Train_BestReturn : 197.3217066470462\n",
      "TimeSinceStart : 1654.55335855484\n",
      "Training Loss : 2.4564902782440186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 197.519482\n",
      "best mean reward 197.519482\n",
      "running time 1682.645696\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 197.51948203988545\n",
      "Train_BestReturn : 197.51948203988545\n",
      "TimeSinceStart : 1682.645695924759\n",
      "Training Loss : 2.5880658626556396\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_hw3_dqn.py\", line 4, in <module>\n",
      "    from cs285.infrastructure.rl_trainer import RL_Trainer\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/infrastructure/rl_trainer.py\", line 10, in <module>\n",
      "    import torch\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/torch/__init__.py\", line 189, in <module>\n",
      "    from torch._C import *\n",
      "RuntimeError: KeyboardInterrupt: \n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_9_LunarLander-v3_17-10-2020_18-27-50 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_9_LunarLander-v3_17-10-2020_18-27-50\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006963\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.00696253776550293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"./run_hw3_dqn.py\", line 94, in <module>\n",
      "    main()\n",
      "  File \"./run_hw3_dqn.py\", line 90, in main\n",
      "    trainer.run_training_loop()\n",
      "  File \"./run_hw3_dqn.py\", line 36, in run_training_loop\n",
      "    eval_policy = self.rl_trainer.agent.actor,\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/infrastructure/rl_trainer.py\", line 151, in run_training_loop\n",
      "    self.agent.step_env()\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/agents/dqn_agent.py\", line 71, in step_env\n",
      "    self.last_obs, reward, done, info = self.env.step(action)\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\", line 32, in step\n",
      "    done = self._after_step(observation, reward, done, info)\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/gym/wrappers/monitor.py\", line 171, in _after_step\n",
      "    self.video_recorder.capture_frame()\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/gym/wrappers/monitoring/video_recorder.py\", line 101, in capture_frame\n",
      "    frame = self.env.render(mode=render_mode)\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/gym/core.py\", line 233, in render\n",
      "    return self.env.render(mode, **kwargs)\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/gym/envs/registration.py\", line 165, in render\n",
      "    return env._render(mode, close=False)\n",
      "  File \"/home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/envs/box2d/lunar_lander.py\", line 416, in _render\n",
      "    return self.viewer.render(return_rgb_array = mode=='rgb_array')\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\", line 114, in render\n",
      "    image_data = buffer.get_image_data()\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/pyglet/image/__init__.py\", line 2047, in get_image_data\n",
      "    self.gl_format, GL_UNSIGNED_BYTE, buffer)\n",
      "  File \"/home/tomas/.local/lib/python3.6/site-packages/pyglet/gl/lib.py\", line 87, in errcheck\n",
      "    def errcheck(result, func, arguments):\n",
      "KeyboardInterrupt\n",
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_9_LunarLander-v3_17-10-2020_18-27-54 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_9_LunarLander-v3_17-10-2020_18-27-54\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "def run_lunar_ddqn(s):\n",
    "    !python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_doubledqn_{s} \\\n",
    "    --double_q --seed {s}\n",
    "def run_lunar_dqn(s):\n",
    "    !python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_dqn_{s} --seed {s}\n",
    "for i in range(4,10):\n",
    "    run_lunar_ddqn(i)\n",
    "    run_lunar_dqn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_1_LunarLander-v3_15-10-2020_16-03-36 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_1_LunarLander-v3_15-10-2020_16-03-36\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.007295\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.007295370101928711\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -235.854732\n",
      "best mean reward -inf\n",
      "running time 32.264398\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -235.85473179250428\n",
      "TimeSinceStart : 32.26439833641052\n",
      "Training Loss : 0.7206376791000366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -193.374702\n",
      "best mean reward -193.374702\n",
      "running time 60.973800\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -193.37470235131903\n",
      "Train_BestReturn : -193.37470235131903\n",
      "TimeSinceStart : 60.97380018234253\n",
      "Training Loss : 0.6584873795509338\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -172.660721\n",
      "best mean reward -172.660721\n",
      "running time 93.767328\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -172.660720680267\n",
      "Train_BestReturn : -172.660720680267\n",
      "TimeSinceStart : 93.76732802391052\n",
      "Training Loss : 0.6219433546066284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -160.034526\n",
      "best mean reward -160.034526\n",
      "running time 147.037922\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -160.03452647093957\n",
      "Train_BestReturn : -160.03452647093957\n",
      "TimeSinceStart : 147.037921667099\n",
      "Training Loss : 0.3703054189682007\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -139.557512\n",
      "best mean reward -139.557512\n",
      "running time 185.255169\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -139.55751204121495\n",
      "Train_BestReturn : -139.55751204121495\n",
      "TimeSinceStart : 185.25516891479492\n",
      "Training Loss : 1.4919482469558716\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -127.558094\n",
      "best mean reward -127.558094\n",
      "running time 219.299366\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -127.55809354865835\n",
      "Train_BestReturn : -127.55809354865835\n",
      "TimeSinceStart : 219.29936575889587\n",
      "Training Loss : 0.26447317004203796\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -85.721759\n",
      "best mean reward -85.721759\n",
      "running time 250.993678\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -85.72175908298749\n",
      "Train_BestReturn : -85.72175908298749\n",
      "TimeSinceStart : 250.99367761611938\n",
      "Training Loss : 0.2009587287902832\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -64.178752\n",
      "best mean reward -64.178752\n",
      "running time 285.039701\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -64.17875174465475\n",
      "Train_BestReturn : -64.17875174465475\n",
      "TimeSinceStart : 285.0397005081177\n",
      "Training Loss : 0.21404790878295898\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -61.863571\n",
      "best mean reward -61.863571\n",
      "running time 320.453170\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -61.86357126308473\n",
      "Train_BestReturn : -61.86357126308473\n",
      "TimeSinceStart : 320.45316982269287\n",
      "Training Loss : 0.20664867758750916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -64.845301\n",
      "best mean reward -61.863571\n",
      "running time 354.518018\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -64.84530083195347\n",
      "Train_BestReturn : -61.86357126308473\n",
      "TimeSinceStart : 354.5180184841156\n",
      "Training Loss : 0.12372255325317383\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -49.962881\n",
      "best mean reward -49.962881\n",
      "running time 402.286682\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -49.962881302886515\n",
      "Train_BestReturn : -49.962881302886515\n",
      "TimeSinceStart : 402.28668236732483\n",
      "Training Loss : 0.38645002245903015\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -25.369659\n",
      "best mean reward -25.369659\n",
      "running time 434.819974\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -25.369659372404644\n",
      "Train_BestReturn : -25.369659372404644\n",
      "TimeSinceStart : 434.81997418403625\n",
      "Training Loss : 0.045637309551239014\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -8.820966\n",
      "best mean reward -8.820966\n",
      "running time 469.826668\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -8.820965703011147\n",
      "Train_BestReturn : -8.820965703011147\n",
      "TimeSinceStart : 469.8266680240631\n",
      "Training Loss : 0.6997844576835632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -3.320876\n",
      "best mean reward -3.320876\n",
      "running time 503.765343\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -3.320875549950255\n",
      "Train_BestReturn : -3.320875549950255\n",
      "TimeSinceStart : 503.7653429508209\n",
      "Training Loss : 0.1218775063753128\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 3.464659\n",
      "best mean reward 3.464659\n",
      "running time 538.078149\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 3.4646588478722964\n",
      "Train_BestReturn : 3.4646588478722964\n",
      "TimeSinceStart : 538.0781486034393\n",
      "Training Loss : 0.13908466696739197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 17.509937\n",
      "best mean reward 17.509937\n",
      "running time 573.276363\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 17.509937440269674\n",
      "Train_BestReturn : 17.509937440269674\n",
      "TimeSinceStart : 573.2763628959656\n",
      "Training Loss : 0.1017022430896759\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 28.446772\n",
      "best mean reward 28.446772\n",
      "running time 605.945714\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 28.44677208795932\n",
      "Train_BestReturn : 28.44677208795932\n",
      "TimeSinceStart : 605.9457144737244\n",
      "Training Loss : 0.16293102502822876\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 31.287665\n",
      "best mean reward 31.287665\n",
      "running time 649.648576\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 31.287664810130767\n",
      "Train_BestReturn : 31.287664810130767\n",
      "TimeSinceStart : 649.6485760211945\n",
      "Training Loss : 0.2644323706626892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 33.349458\n",
      "best mean reward 33.349458\n",
      "running time 682.894198\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 33.34945787725149\n",
      "Train_BestReturn : 33.34945787725149\n",
      "TimeSinceStart : 682.894198179245\n",
      "Training Loss : 0.14152972400188446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 39.173729\n",
      "best mean reward 39.173729\n",
      "running time 714.216491\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 39.173728950498464\n",
      "Train_BestReturn : 39.173728950498464\n",
      "TimeSinceStart : 714.216490983963\n",
      "Training Loss : 0.1624743938446045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 55.762408\n",
      "best mean reward 55.762408\n",
      "running time 745.661592\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 55.76240760996333\n",
      "Train_BestReturn : 55.76240760996333\n",
      "TimeSinceStart : 745.6615920066833\n",
      "Training Loss : 0.07096904516220093\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 72.422370\n",
      "best mean reward 72.422370\n",
      "running time 777.947154\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 72.42237003358606\n",
      "Train_BestReturn : 72.42237003358606\n",
      "TimeSinceStart : 777.947154045105\n",
      "Training Loss : 0.0718507245182991\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 61.739932\n",
      "best mean reward 72.422370\n",
      "running time 813.241368\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 61.739931711146326\n",
      "Train_BestReturn : 72.42237003358606\n",
      "TimeSinceStart : 813.241367816925\n",
      "Training Loss : 1.6080994606018066\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 84.571808\n",
      "best mean reward 84.571808\n",
      "running time 844.479291\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 84.57180834317671\n",
      "Train_BestReturn : 84.57180834317671\n",
      "TimeSinceStart : 844.4792907238007\n",
      "Training Loss : 0.8853445649147034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 90.316391\n",
      "best mean reward 90.316391\n",
      "running time 879.114080\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 90.31639145261948\n",
      "Train_BestReturn : 90.31639145261948\n",
      "TimeSinceStart : 879.1140801906586\n",
      "Training Loss : 0.3165161907672882\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 94.297380\n",
      "best mean reward 94.297380\n",
      "running time 908.170666\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 94.29737988020551\n",
      "Train_BestReturn : 94.29737988020551\n",
      "TimeSinceStart : 908.1706655025482\n",
      "Training Loss : 0.2078574299812317\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 85.231719\n",
      "best mean reward 94.297380\n",
      "running time 936.668008\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 85.23171920660506\n",
      "Train_BestReturn : 94.29737988020551\n",
      "TimeSinceStart : 936.6680083274841\n",
      "Training Loss : 0.18031203746795654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 94.784419\n",
      "best mean reward 94.784419\n",
      "running time 967.473104\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 94.78441913594571\n",
      "Train_BestReturn : 94.78441913594571\n",
      "TimeSinceStart : 967.4731040000916\n",
      "Training Loss : 0.10135506093502045\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 92.344997\n",
      "best mean reward 94.784419\n",
      "running time 996.675699\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 92.34499708245038\n",
      "Train_BestReturn : 94.78441913594571\n",
      "TimeSinceStart : 996.6756994724274\n",
      "Training Loss : 0.22330057621002197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 99.668751\n",
      "best mean reward 99.668751\n",
      "running time 1024.523174\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 99.66875060544511\n",
      "Train_BestReturn : 99.66875060544511\n",
      "TimeSinceStart : 1024.5231742858887\n",
      "Training Loss : 0.13128000497817993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 106.095296\n",
      "best mean reward 106.095296\n",
      "running time 1053.324396\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 106.095295849252\n",
      "Train_BestReturn : 106.095295849252\n",
      "TimeSinceStart : 1053.3243958950043\n",
      "Training Loss : 0.7907900214195251\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 116.858239\n",
      "best mean reward 116.858239\n",
      "running time 1079.891960\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 116.85823877348626\n",
      "Train_BestReturn : 116.85823877348626\n",
      "TimeSinceStart : 1079.8919603824615\n",
      "Training Loss : 0.2080133855342865\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 120.995991\n",
      "best mean reward 120.995991\n",
      "running time 1106.497268\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 120.99599062846855\n",
      "Train_BestReturn : 120.99599062846855\n",
      "TimeSinceStart : 1106.4972677230835\n",
      "Training Loss : 0.16844946146011353\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 125.538708\n",
      "best mean reward 125.538708\n",
      "running time 1132.806412\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 125.53870753949333\n",
      "Train_BestReturn : 125.53870753949333\n",
      "TimeSinceStart : 1132.8064119815826\n",
      "Training Loss : 1.1165480613708496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 122.954679\n",
      "best mean reward 125.538708\n",
      "running time 1160.126980\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 122.95467928240825\n",
      "Train_BestReturn : 125.53870753949333\n",
      "TimeSinceStart : 1160.1269798278809\n",
      "Training Loss : 0.20175181329250336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 124.250887\n",
      "best mean reward 125.538708\n",
      "running time 1189.281934\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 124.2508873262667\n",
      "Train_BestReturn : 125.53870753949333\n",
      "TimeSinceStart : 1189.2819337844849\n",
      "Training Loss : 0.6449941992759705\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 133.804458\n",
      "best mean reward 133.804458\n",
      "running time 1217.095857\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 133.8044582351928\n",
      "Train_BestReturn : 133.8044582351928\n",
      "TimeSinceStart : 1217.0958573818207\n",
      "Training Loss : 0.21191668510437012\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 145.574758\n",
      "best mean reward 145.574758\n",
      "running time 1245.531086\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 145.57475803732143\n",
      "Train_BestReturn : 145.57475803732143\n",
      "TimeSinceStart : 1245.5310862064362\n",
      "Training Loss : 0.5600973963737488\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 144.297358\n",
      "best mean reward 145.574758\n",
      "running time 1272.545080\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 144.2973576442673\n",
      "Train_BestReturn : 145.57475803732143\n",
      "TimeSinceStart : 1272.545080423355\n",
      "Training Loss : 0.175935298204422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 125.351908\n",
      "best mean reward 145.574758\n",
      "running time 1300.531380\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 125.35190774922447\n",
      "Train_BestReturn : 145.57475803732143\n",
      "TimeSinceStart : 1300.531379699707\n",
      "Training Loss : 2.5688211917877197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 145.736198\n",
      "best mean reward 145.736198\n",
      "running time 1327.415766\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 145.73619812414947\n",
      "Train_BestReturn : 145.73619812414947\n",
      "TimeSinceStart : 1327.415765762329\n",
      "Training Loss : 0.29587483406066895\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 155.381668\n",
      "best mean reward 155.381668\n",
      "running time 1353.342416\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 155.3816684737149\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1353.34241604805\n",
      "Training Loss : 0.0910554826259613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 149.687133\n",
      "best mean reward 155.381668\n",
      "running time 1380.407786\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 149.68713300385417\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1380.4077863693237\n",
      "Training Loss : 0.20234368741512299\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 146.660069\n",
      "best mean reward 155.381668\n",
      "running time 1407.468982\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 146.66006879690386\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1407.468981742859\n",
      "Training Loss : 0.7648055553436279\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 141.713897\n",
      "best mean reward 155.381668\n",
      "running time 1435.368899\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 141.7138970744174\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1435.368899345398\n",
      "Training Loss : 1.065333604812622\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 131.560155\n",
      "best mean reward 155.381668\n",
      "running time 1462.062937\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 131.56015503349045\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1462.0629365444183\n",
      "Training Loss : 0.4432219862937927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 145.161047\n",
      "best mean reward 155.381668\n",
      "running time 1488.648477\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 145.16104712054775\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1488.648476600647\n",
      "Training Loss : 0.28752392530441284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 152.717302\n",
      "best mean reward 155.381668\n",
      "running time 1514.854293\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 152.71730209457718\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1514.8542931079865\n",
      "Training Loss : 1.6049439907073975\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 131.827114\n",
      "best mean reward 155.381668\n",
      "running time 1542.119384\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 131.82711391534482\n",
      "Train_BestReturn : 155.3816684737149\n",
      "TimeSinceStart : 1542.1193842887878\n",
      "Training Loss : 0.1966186761856079\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_dqn_1 --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_2_LunarLander-v3_15-10-2020_16-29-49 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_2_LunarLander-v3_15-10-2020_16-29-49\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006542\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006541728973388672\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -240.222291\n",
      "best mean reward -inf\n",
      "running time 36.575685\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -240.2222906837436\n",
      "TimeSinceStart : 36.575684785842896\n",
      "Training Loss : 0.5123874545097351\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -172.602225\n",
      "best mean reward -172.602225\n",
      "running time 63.338480\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -172.60222477507025\n",
      "Train_BestReturn : -172.60222477507025\n",
      "TimeSinceStart : 63.33847951889038\n",
      "Training Loss : 0.3120874762535095\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -147.115344\n",
      "best mean reward -147.115344\n",
      "running time 97.388189\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -147.11534352845035\n",
      "Train_BestReturn : -147.11534352845035\n",
      "TimeSinceStart : 97.38818883895874\n",
      "Training Loss : 0.45403391122817993\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -130.379345\n",
      "best mean reward -130.379345\n",
      "running time 147.787686\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -130.37934475389326\n",
      "Train_BestReturn : -130.37934475389326\n",
      "TimeSinceStart : 147.78768610954285\n",
      "Training Loss : 0.5888557434082031\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -122.945911\n",
      "best mean reward -122.945911\n",
      "running time 185.491831\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -122.94591055671596\n",
      "Train_BestReturn : -122.94591055671596\n",
      "TimeSinceStart : 185.49183106422424\n",
      "Training Loss : 2.008836269378662\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -95.572614\n",
      "best mean reward -95.572614\n",
      "running time 217.955722\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -95.57261372169171\n",
      "Train_BestReturn : -95.57261372169171\n",
      "TimeSinceStart : 217.95572209358215\n",
      "Training Loss : 0.2615731358528137\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -77.890289\n",
      "best mean reward -77.890289\n",
      "running time 252.174981\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -77.89028852504805\n",
      "Train_BestReturn : -77.89028852504805\n",
      "TimeSinceStart : 252.17498064041138\n",
      "Training Loss : 0.7309415936470032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -63.140429\n",
      "best mean reward -63.140429\n",
      "running time 286.620717\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -63.14042917888774\n",
      "Train_BestReturn : -63.14042917888774\n",
      "TimeSinceStart : 286.62071657180786\n",
      "Training Loss : 0.315406858921051\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -59.209726\n",
      "best mean reward -59.209726\n",
      "running time 320.681449\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -59.20972592283127\n",
      "Train_BestReturn : -59.20972592283127\n",
      "TimeSinceStart : 320.681449174881\n",
      "Training Loss : 0.1941177248954773\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -48.719657\n",
      "best mean reward -48.719657\n",
      "running time 354.907908\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -48.71965737560061\n",
      "Train_BestReturn : -48.71965737560061\n",
      "TimeSinceStart : 354.90790820121765\n",
      "Training Loss : 0.3472517430782318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -38.352000\n",
      "best mean reward -38.352000\n",
      "running time 400.171230\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -38.35200025331443\n",
      "Train_BestReturn : -38.35200025331443\n",
      "TimeSinceStart : 400.17122983932495\n",
      "Training Loss : 0.69109708070755\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -9.114435\n",
      "best mean reward -9.114435\n",
      "running time 434.049339\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -9.11443493769829\n",
      "Train_BestReturn : -9.11443493769829\n",
      "TimeSinceStart : 434.04933857917786\n",
      "Training Loss : 0.21554461121559143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 5.296888\n",
      "best mean reward 5.296888\n",
      "running time 467.871036\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 5.296888318408824\n",
      "Train_BestReturn : 5.296888318408824\n",
      "TimeSinceStart : 467.8710355758667\n",
      "Training Loss : 0.21078315377235413\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 20.585035\n",
      "best mean reward 20.585035\n",
      "running time 503.177883\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 20.585034714140978\n",
      "Train_BestReturn : 20.585034714140978\n",
      "TimeSinceStart : 503.1778829097748\n",
      "Training Loss : 0.3509983420372009\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 48.418430\n",
      "best mean reward 48.418430\n",
      "running time 537.421458\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 48.41842996324445\n",
      "Train_BestReturn : 48.41842996324445\n",
      "TimeSinceStart : 537.4214577674866\n",
      "Training Loss : 3.044907808303833\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 54.819540\n",
      "best mean reward 54.819540\n",
      "running time 572.353979\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 54.81953986410907\n",
      "Train_BestReturn : 54.81953986410907\n",
      "TimeSinceStart : 572.3539786338806\n",
      "Training Loss : 0.21826285123825073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 66.998051\n",
      "best mean reward 66.998051\n",
      "running time 606.879148\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 66.9980505389097\n",
      "Train_BestReturn : 66.9980505389097\n",
      "TimeSinceStart : 606.8791484832764\n",
      "Training Loss : 0.10209517925977707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 70.006525\n",
      "best mean reward 70.006525\n",
      "running time 643.308228\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 70.00652546108525\n",
      "Train_BestReturn : 70.00652546108525\n",
      "TimeSinceStart : 643.3082280158997\n",
      "Training Loss : 0.08148475736379623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 75.779049\n",
      "best mean reward 75.779049\n",
      "running time 685.977143\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 75.7790491136097\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 685.9771428108215\n",
      "Training Loss : 0.3864858150482178\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 63.790800\n",
      "best mean reward 75.779049\n",
      "running time 720.765528\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 63.79079981493144\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 720.7655279636383\n",
      "Training Loss : 1.41102933883667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 63.480137\n",
      "best mean reward 75.779049\n",
      "running time 755.989977\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 63.48013722932368\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 755.9899771213531\n",
      "Training Loss : 0.12807385623455048\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 66.556197\n",
      "best mean reward 75.779049\n",
      "running time 788.937671\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 66.55619714736032\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 788.9376707077026\n",
      "Training Loss : 1.484749674797058\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 74.176340\n",
      "best mean reward 75.779049\n",
      "running time 826.039888\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 74.17633966083969\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 826.039888381958\n",
      "Training Loss : 0.14540612697601318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 69.961550\n",
      "best mean reward 75.779049\n",
      "running time 864.362578\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 69.96154988275812\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 864.3625779151917\n",
      "Training Loss : 0.0673249289393425\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 58.248199\n",
      "best mean reward 75.779049\n",
      "running time 903.152197\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 58.24819925344253\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 903.1521973609924\n",
      "Training Loss : 0.09368681907653809\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 73.358437\n",
      "best mean reward 75.779049\n",
      "running time 937.924092\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 73.3584372599164\n",
      "Train_BestReturn : 75.7790491136097\n",
      "TimeSinceStart : 937.9240920543671\n",
      "Training Loss : 0.1592077910900116\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 79.884428\n",
      "best mean reward 79.884428\n",
      "running time 970.475793\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 79.88442818030632\n",
      "Train_BestReturn : 79.88442818030632\n",
      "TimeSinceStart : 970.4757933616638\n",
      "Training Loss : 0.16780266165733337\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 95.272902\n",
      "best mean reward 95.272902\n",
      "running time 1007.647793\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 95.27290198994153\n",
      "Train_BestReturn : 95.27290198994153\n",
      "TimeSinceStart : 1007.6477928161621\n",
      "Training Loss : 0.12037629634141922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 122.099653\n",
      "best mean reward 122.099653\n",
      "running time 1039.049527\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 122.09965282388004\n",
      "Train_BestReturn : 122.09965282388004\n",
      "TimeSinceStart : 1039.0495266914368\n",
      "Training Loss : 0.09482519328594208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 141.483052\n",
      "best mean reward 141.483052\n",
      "running time 1068.617228\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 141.48305159536005\n",
      "Train_BestReturn : 141.48305159536005\n",
      "TimeSinceStart : 1068.6172280311584\n",
      "Training Loss : 0.0775144100189209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 150.706152\n",
      "best mean reward 150.706152\n",
      "running time 1096.952011\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 150.7061524136665\n",
      "Train_BestReturn : 150.7061524136665\n",
      "TimeSinceStart : 1096.9520111083984\n",
      "Training Loss : 0.12526467442512512\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 159.678647\n",
      "best mean reward 159.678647\n",
      "running time 1127.372489\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 159.6786466716124\n",
      "Train_BestReturn : 159.6786466716124\n",
      "TimeSinceStart : 1127.3724892139435\n",
      "Training Loss : 0.07788598537445068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 141.033658\n",
      "best mean reward 159.678647\n",
      "running time 1158.882659\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 141.0336575864584\n",
      "Train_BestReturn : 159.6786466716124\n",
      "TimeSinceStart : 1158.8826594352722\n",
      "Training Loss : 0.1806376427412033\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 153.514529\n",
      "best mean reward 159.678647\n",
      "running time 1185.517349\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 153.51452878781362\n",
      "Train_BestReturn : 159.6786466716124\n",
      "TimeSinceStart : 1185.5173494815826\n",
      "Training Loss : 0.09010885655879974\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 173.801161\n",
      "best mean reward 173.801161\n",
      "running time 1214.763739\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 173.80116131949686\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1214.7637388706207\n",
      "Training Loss : 0.6644197106361389\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 169.088883\n",
      "best mean reward 173.801161\n",
      "running time 1240.348237\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 169.08888339723464\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1240.3482365608215\n",
      "Training Loss : 0.23281151056289673\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 157.878863\n",
      "best mean reward 173.801161\n",
      "running time 1265.820605\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 157.8788630498173\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1265.8206052780151\n",
      "Training Loss : 1.2434860467910767\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 149.650286\n",
      "best mean reward 173.801161\n",
      "running time 1291.118960\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 149.65028588108314\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1291.1189601421356\n",
      "Training Loss : 0.11416366696357727\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 154.280256\n",
      "best mean reward 173.801161\n",
      "running time 1316.533187\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 154.28025600315678\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1316.533186674118\n",
      "Training Loss : 0.09358204156160355\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 164.318886\n",
      "best mean reward 173.801161\n",
      "running time 1345.196868\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 164.31888577781856\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1345.1968681812286\n",
      "Training Loss : 0.15723782777786255\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 160.049636\n",
      "best mean reward 173.801161\n",
      "running time 1370.433706\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 160.04963630757456\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1370.4337055683136\n",
      "Training Loss : 0.04879656434059143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 138.154798\n",
      "best mean reward 173.801161\n",
      "running time 1395.445404\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 138.15479755331046\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1395.4454035758972\n",
      "Training Loss : 2.6554555892944336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 148.854235\n",
      "best mean reward 173.801161\n",
      "running time 1420.437325\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 148.8542346816846\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1420.4373252391815\n",
      "Training Loss : 0.20515084266662598\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 120.388140\n",
      "best mean reward 173.801161\n",
      "running time 1445.197113\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 120.38813960452293\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1445.197113275528\n",
      "Training Loss : 0.31680792570114136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 100.619176\n",
      "best mean reward 173.801161\n",
      "running time 1469.895223\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 100.61917619043587\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1469.8952231407166\n",
      "Training Loss : 0.42594999074935913\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 79.764689\n",
      "best mean reward 173.801161\n",
      "running time 1494.498998\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 79.76468932350966\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1494.498997926712\n",
      "Training Loss : 0.45097869634628296\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 46.901347\n",
      "best mean reward 173.801161\n",
      "running time 1518.980916\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 46.90134679331534\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1518.9809155464172\n",
      "Training Loss : 0.40746009349823\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 5.778744\n",
      "best mean reward 173.801161\n",
      "running time 1543.421839\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 5.7787436013229705\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1543.4218385219574\n",
      "Training Loss : 0.10247619450092316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) -58.067388\n",
      "best mean reward 173.801161\n",
      "running time 1568.030899\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : -58.06738761885698\n",
      "Train_BestReturn : 173.80116131949686\n",
      "TimeSinceStart : 1568.030898809433\n",
      "Training Loss : 0.31245946884155273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_dqn_2 --seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_3_LunarLander-v3_15-10-2020_16-56-25 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_dqn_3_LunarLander-v3_15-10-2020_16-56-25\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006959\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006958723068237305\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -211.576450\n",
      "best mean reward -inf\n",
      "running time 32.402255\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -211.57644969738854\n",
      "TimeSinceStart : 32.40225529670715\n",
      "Training Loss : 1.578321099281311\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -189.079355\n",
      "best mean reward -189.079355\n",
      "running time 60.506169\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -189.079355366949\n",
      "Train_BestReturn : -189.079355366949\n",
      "TimeSinceStart : 60.50616908073425\n",
      "Training Loss : 0.4042821526527405\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -166.093940\n",
      "best mean reward -166.093940\n",
      "running time 95.936256\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -166.09393981087692\n",
      "Train_BestReturn : -166.09393981087692\n",
      "TimeSinceStart : 95.9362564086914\n",
      "Training Loss : 0.8343470692634583\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -159.653037\n",
      "best mean reward -159.653037\n",
      "running time 145.492267\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -159.65303743056975\n",
      "Train_BestReturn : -159.65303743056975\n",
      "TimeSinceStart : 145.49226713180542\n",
      "Training Loss : 0.4906376004219055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -141.295998\n",
      "best mean reward -141.295998\n",
      "running time 183.929832\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -141.29599753176254\n",
      "Train_BestReturn : -141.29599753176254\n",
      "TimeSinceStart : 183.92983150482178\n",
      "Training Loss : 0.6567500233650208\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -130.298406\n",
      "best mean reward -130.298406\n",
      "running time 216.317109\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -130.29840645451637\n",
      "Train_BestReturn : -130.29840645451637\n",
      "TimeSinceStart : 216.3171088695526\n",
      "Training Loss : 6.769768714904785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -114.755048\n",
      "best mean reward -114.755048\n",
      "running time 246.769853\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -114.75504756561227\n",
      "Train_BestReturn : -114.75504756561227\n",
      "TimeSinceStart : 246.7698528766632\n",
      "Training Loss : 1.1980071067810059\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -90.192801\n",
      "best mean reward -90.192801\n",
      "running time 277.706867\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -90.19280099420031\n",
      "Train_BestReturn : -90.19280099420031\n",
      "TimeSinceStart : 277.7068667411804\n",
      "Training Loss : 0.13081754744052887\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -73.233380\n",
      "best mean reward -73.233380\n",
      "running time 312.608968\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -73.23338021642277\n",
      "Train_BestReturn : -73.23338021642277\n",
      "TimeSinceStart : 312.6089675426483\n",
      "Training Loss : 0.2948611378669739\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -78.365617\n",
      "best mean reward -73.233380\n",
      "running time 342.907016\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -78.36561672706922\n",
      "Train_BestReturn : -73.23338021642277\n",
      "TimeSinceStart : 342.9070158004761\n",
      "Training Loss : 0.4105357825756073\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -63.128863\n",
      "best mean reward -63.128863\n",
      "running time 374.087157\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -63.12886347715116\n",
      "Train_BestReturn : -63.12886347715116\n",
      "TimeSinceStart : 374.08715653419495\n",
      "Training Loss : 5.714624881744385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -19.148915\n",
      "best mean reward -19.148915\n",
      "running time 405.928063\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -19.14891532743637\n",
      "Train_BestReturn : -19.14891532743637\n",
      "TimeSinceStart : 405.9280626773834\n",
      "Training Loss : 0.3365997076034546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -2.814810\n",
      "best mean reward -2.814810\n",
      "running time 439.593265\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -2.8148099721673936\n",
      "Train_BestReturn : -2.8148099721673936\n",
      "TimeSinceStart : 439.5932648181915\n",
      "Training Loss : 0.4325239658355713\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 17.607842\n",
      "best mean reward 17.607842\n",
      "running time 476.394482\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 17.607841762414477\n",
      "Train_BestReturn : 17.607841762414477\n",
      "TimeSinceStart : 476.3944823741913\n",
      "Training Loss : 0.45162028074264526\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 45.285366\n",
      "best mean reward 45.285366\n",
      "running time 519.980739\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 45.28536564172466\n",
      "Train_BestReturn : 45.28536564172466\n",
      "TimeSinceStart : 519.9807391166687\n",
      "Training Loss : 1.6755067110061646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 59.958392\n",
      "best mean reward 59.958392\n",
      "running time 550.924832\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 59.9583920224895\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 550.9248316287994\n",
      "Training Loss : 3.494859218597412\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 53.234581\n",
      "best mean reward 59.958392\n",
      "running time 585.463870\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 53.23458109914434\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 585.4638698101044\n",
      "Training Loss : 0.5479288697242737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 48.638100\n",
      "best mean reward 59.958392\n",
      "running time 621.073594\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 48.63810002769492\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 621.0735943317413\n",
      "Training Loss : 0.28352728486061096\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 57.762130\n",
      "best mean reward 59.958392\n",
      "running time 655.369446\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 57.76212978486284\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 655.3694462776184\n",
      "Training Loss : 0.22730734944343567\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 56.797316\n",
      "best mean reward 59.958392\n",
      "running time 689.808095\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 56.797315975522096\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 689.8080947399139\n",
      "Training Loss : 0.8402894139289856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 49.548578\n",
      "best mean reward 59.958392\n",
      "running time 722.421836\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 49.54857786239772\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 722.4218356609344\n",
      "Training Loss : 0.2751687169075012\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 50.300429\n",
      "best mean reward 59.958392\n",
      "running time 757.269784\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 50.30042926978158\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 757.2697837352753\n",
      "Training Loss : 0.27059057354927063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 53.600085\n",
      "best mean reward 59.958392\n",
      "running time 793.523922\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 53.60008534775062\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 793.5239224433899\n",
      "Training Loss : 0.15726546943187714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 45.557675\n",
      "best mean reward 59.958392\n",
      "running time 843.981648\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 45.5576752740777\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 843.9816477298737\n",
      "Training Loss : 0.10951761901378632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 32.671705\n",
      "best mean reward 59.958392\n",
      "running time 881.513378\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 32.67170507381289\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 881.5133776664734\n",
      "Training Loss : 0.13757169246673584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 52.097724\n",
      "best mean reward 59.958392\n",
      "running time 916.616930\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 52.09772424011564\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 916.6169304847717\n",
      "Training Loss : 0.07822369784116745\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 58.680950\n",
      "best mean reward 59.958392\n",
      "running time 955.681858\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 58.680950363366385\n",
      "Train_BestReturn : 59.9583920224895\n",
      "TimeSinceStart : 955.6818580627441\n",
      "Training Loss : 0.2237822264432907\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 88.403658\n",
      "best mean reward 88.403658\n",
      "running time 988.449509\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 88.40365836465807\n",
      "Train_BestReturn : 88.40365836465807\n",
      "TimeSinceStart : 988.4495089054108\n",
      "Training Loss : 0.15560564398765564\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 122.058429\n",
      "best mean reward 122.058429\n",
      "running time 1021.030010\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 122.05842877507985\n",
      "Train_BestReturn : 122.05842877507985\n",
      "TimeSinceStart : 1021.0300102233887\n",
      "Training Loss : 0.2008378803730011\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 143.329227\n",
      "best mean reward 143.329227\n",
      "running time 1052.007117\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 143.32922674336498\n",
      "Train_BestReturn : 143.32922674336498\n",
      "TimeSinceStart : 1052.0071172714233\n",
      "Training Loss : 0.18184876441955566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 156.380251\n",
      "best mean reward 156.380251\n",
      "running time 1088.295553\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 156.38025119450938\n",
      "Train_BestReturn : 156.38025119450938\n",
      "TimeSinceStart : 1088.2955527305603\n",
      "Training Loss : 0.6490781903266907\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 165.940168\n",
      "best mean reward 165.940168\n",
      "running time 1120.425466\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 165.94016839401567\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1120.425466299057\n",
      "Training Loss : 0.5619028806686401\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 155.048805\n",
      "best mean reward 165.940168\n",
      "running time 1149.846738\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 155.04880549787495\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1149.8467376232147\n",
      "Training Loss : 0.188810795545578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 162.627522\n",
      "best mean reward 165.940168\n",
      "running time 1177.892659\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 162.6275219241836\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1177.8926594257355\n",
      "Training Loss : 0.7847743034362793\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 159.214504\n",
      "best mean reward 165.940168\n",
      "running time 1207.110996\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 159.21450404622436\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1207.1109957695007\n",
      "Training Loss : 0.3479735851287842\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 135.124834\n",
      "best mean reward 165.940168\n",
      "running time 1239.658628\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 135.12483441555875\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1239.6586275100708\n",
      "Training Loss : 0.5782660245895386\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 125.873690\n",
      "best mean reward 165.940168\n",
      "running time 1268.377568\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 125.87369005027523\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1268.377567768097\n",
      "Training Loss : 0.09538097679615021\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 118.301357\n",
      "best mean reward 165.940168\n",
      "running time 1298.600959\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 118.30135720886068\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1298.6009585857391\n",
      "Training Loss : 0.165054053068161\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 33.126923\n",
      "best mean reward 165.940168\n",
      "running time 1328.497983\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 33.126922936731525\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1328.4979827404022\n",
      "Training Loss : 0.09652189165353775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 16.677968\n",
      "best mean reward 165.940168\n",
      "running time 1356.120808\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 16.677968225569945\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1356.1208081245422\n",
      "Training Loss : 17.70090675354004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 26.877656\n",
      "best mean reward 165.940168\n",
      "running time 1383.196819\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 26.87765593480517\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1383.1968190670013\n",
      "Training Loss : 0.18635088205337524\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 71.052782\n",
      "best mean reward 165.940168\n",
      "running time 1409.229305\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 71.05278244573991\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1409.2293045520782\n",
      "Training Loss : 1.6230343580245972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 92.034076\n",
      "best mean reward 165.940168\n",
      "running time 1437.043452\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 92.0340758477451\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1437.0434515476227\n",
      "Training Loss : 2.6115198135375977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 82.601300\n",
      "best mean reward 165.940168\n",
      "running time 1464.335385\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 82.60130001564659\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1464.3353850841522\n",
      "Training Loss : 0.4564017653465271\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 65.063521\n",
      "best mean reward 165.940168\n",
      "running time 1491.111075\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 65.06352117804218\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1491.1110746860504\n",
      "Training Loss : 1.4185492992401123\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 23.297057\n",
      "best mean reward 165.940168\n",
      "running time 1518.243109\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 23.297057065211543\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1518.2431087493896\n",
      "Training Loss : 3.2994682788848877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 34.203247\n",
      "best mean reward 165.940168\n",
      "running time 1544.113106\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 34.20324690092322\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1544.113106250763\n",
      "Training Loss : 5.459181785583496\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 13.330708\n",
      "best mean reward 165.940168\n",
      "running time 1573.928309\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 13.330707680848514\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1573.928308725357\n",
      "Training Loss : 1.9742579460144043\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 30.315754\n",
      "best mean reward 165.940168\n",
      "running time 1606.664450\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 30.315754266182804\n",
      "Train_BestReturn : 165.94016839401567\n",
      "TimeSinceStart : 1606.6644496917725\n",
      "Training Loss : 2.304473876953125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_dqn_3 --seed 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_1_LunarLander-v3_15-10-2020_17-23-45 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_1_LunarLander-v3_15-10-2020_17-23-45\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006936\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006936311721801758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -227.084135\n",
      "best mean reward -inf\n",
      "running time 33.078266\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -227.0841346071823\n",
      "TimeSinceStart : 33.07826566696167\n",
      "Training Loss : 0.32455864548683167\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -176.289126\n",
      "best mean reward -176.289126\n",
      "running time 60.670531\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -176.28912583298992\n",
      "Train_BestReturn : -176.28912583298992\n",
      "TimeSinceStart : 60.670530796051025\n",
      "Training Loss : 1.2445228099822998\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -171.611435\n",
      "best mean reward -171.611435\n",
      "running time 112.482385\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -171.61143450534993\n",
      "Train_BestReturn : -171.61143450534993\n",
      "TimeSinceStart : 112.48238468170166\n",
      "Training Loss : 0.6033408641815186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -159.063530\n",
      "best mean reward -159.063530\n",
      "running time 148.887533\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -159.0635302506925\n",
      "Train_BestReturn : -159.0635302506925\n",
      "TimeSinceStart : 148.88753271102905\n",
      "Training Loss : 0.7158759832382202\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -144.239458\n",
      "best mean reward -144.239458\n",
      "running time 185.326436\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -144.23945764814272\n",
      "Train_BestReturn : -144.23945764814272\n",
      "TimeSinceStart : 185.3264355659485\n",
      "Training Loss : 0.4189292788505554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -131.573035\n",
      "best mean reward -131.573035\n",
      "running time 221.111164\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -131.57303472685797\n",
      "Train_BestReturn : -131.57303472685797\n",
      "TimeSinceStart : 221.11116433143616\n",
      "Training Loss : 0.3368450999259949\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -111.494302\n",
      "best mean reward -111.494302\n",
      "running time 257.896887\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -111.4943016218481\n",
      "Train_BestReturn : -111.4943016218481\n",
      "TimeSinceStart : 257.8968873023987\n",
      "Training Loss : 0.24426338076591492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -90.538540\n",
      "best mean reward -90.538540\n",
      "running time 291.839339\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -90.53854040411977\n",
      "Train_BestReturn : -90.53854040411977\n",
      "TimeSinceStart : 291.8393385410309\n",
      "Training Loss : 0.3434527814388275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -72.580911\n",
      "best mean reward -72.580911\n",
      "running time 325.269482\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -72.5809111392109\n",
      "Train_BestReturn : -72.5809111392109\n",
      "TimeSinceStart : 325.2694823741913\n",
      "Training Loss : 0.4446491003036499\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -67.668873\n",
      "best mean reward -67.668873\n",
      "running time 361.198912\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -67.66887267726973\n",
      "Train_BestReturn : -67.66887267726973\n",
      "TimeSinceStart : 361.19891238212585\n",
      "Training Loss : 0.1507001668214798\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -46.760752\n",
      "best mean reward -46.760752\n",
      "running time 394.011357\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -46.76075189717446\n",
      "Train_BestReturn : -46.76075189717446\n",
      "TimeSinceStart : 394.0113573074341\n",
      "Training Loss : 0.08020986616611481\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -25.861129\n",
      "best mean reward -25.861129\n",
      "running time 427.164012\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -25.86112875025925\n",
      "Train_BestReturn : -25.86112875025925\n",
      "TimeSinceStart : 427.1640124320984\n",
      "Training Loss : 0.1994272917509079\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -8.529078\n",
      "best mean reward -8.529078\n",
      "running time 460.255016\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -8.529077985354744\n",
      "Train_BestReturn : -8.529077985354744\n",
      "TimeSinceStart : 460.25501585006714\n",
      "Training Loss : 0.25125735998153687\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 14.004939\n",
      "best mean reward 14.004939\n",
      "running time 491.461435\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 14.004938727067946\n",
      "Train_BestReturn : 14.004938727067946\n",
      "TimeSinceStart : 491.4614350795746\n",
      "Training Loss : 0.2020125389099121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 27.511871\n",
      "best mean reward 27.511871\n",
      "running time 524.954734\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 27.511870761444833\n",
      "Train_BestReturn : 27.511870761444833\n",
      "TimeSinceStart : 524.9547340869904\n",
      "Training Loss : 0.43861210346221924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 51.407157\n",
      "best mean reward 51.407157\n",
      "running time 556.829505\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 51.407157460341494\n",
      "Train_BestReturn : 51.407157460341494\n",
      "TimeSinceStart : 556.829505443573\n",
      "Training Loss : 0.5553913116455078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 67.438793\n",
      "best mean reward 67.438793\n",
      "running time 605.181779\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 67.43879260621182\n",
      "Train_BestReturn : 67.43879260621182\n",
      "TimeSinceStart : 605.181779384613\n",
      "Training Loss : 0.2382211685180664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 67.722935\n",
      "best mean reward 67.722935\n",
      "running time 637.548784\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 67.72293545899468\n",
      "Train_BestReturn : 67.72293545899468\n",
      "TimeSinceStart : 637.5487837791443\n",
      "Training Loss : 0.3369983434677124\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 65.281803\n",
      "best mean reward 67.722935\n",
      "running time 669.780750\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 65.28180289580453\n",
      "Train_BestReturn : 67.72293545899468\n",
      "TimeSinceStart : 669.7807502746582\n",
      "Training Loss : 0.8383952379226685\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 77.055975\n",
      "best mean reward 77.055975\n",
      "running time 701.740462\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 77.05597518130338\n",
      "Train_BestReturn : 77.05597518130338\n",
      "TimeSinceStart : 701.7404615879059\n",
      "Training Loss : 0.2726545035839081\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 73.977858\n",
      "best mean reward 77.055975\n",
      "running time 735.674910\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 73.97785828875661\n",
      "Train_BestReturn : 77.05597518130338\n",
      "TimeSinceStart : 735.6749095916748\n",
      "Training Loss : 0.546870768070221\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 72.984209\n",
      "best mean reward 77.055975\n",
      "running time 767.347933\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 72.98420944819162\n",
      "Train_BestReturn : 77.05597518130338\n",
      "TimeSinceStart : 767.3479328155518\n",
      "Training Loss : 0.3268756866455078\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 91.368641\n",
      "best mean reward 91.368641\n",
      "running time 798.474311\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 91.36864097799712\n",
      "Train_BestReturn : 91.36864097799712\n",
      "TimeSinceStart : 798.4743106365204\n",
      "Training Loss : 0.27825409173965454\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 96.639570\n",
      "best mean reward 96.639570\n",
      "running time 832.379398\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 96.63956993598309\n",
      "Train_BestReturn : 96.63956993598309\n",
      "TimeSinceStart : 832.3793976306915\n",
      "Training Loss : 0.13269323110580444\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 105.293323\n",
      "best mean reward 105.293323\n",
      "running time 863.965640\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 105.29332318866125\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 863.9656403064728\n",
      "Training Loss : 0.6380674839019775\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 99.854832\n",
      "best mean reward 105.293323\n",
      "running time 897.021765\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 99.85483248650422\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 897.0217645168304\n",
      "Training Loss : 1.3988685607910156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 78.526041\n",
      "best mean reward 105.293323\n",
      "running time 930.289927\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 78.52604066372166\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 930.289927482605\n",
      "Training Loss : 0.42586496472358704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 76.715175\n",
      "best mean reward 105.293323\n",
      "running time 963.082216\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 76.71517498752623\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 963.0822162628174\n",
      "Training Loss : 0.11928632855415344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 65.343078\n",
      "best mean reward 105.293323\n",
      "running time 996.178888\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 65.34307838710023\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 996.1788880825043\n",
      "Training Loss : 0.1929057538509369\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 41.553614\n",
      "best mean reward 105.293323\n",
      "running time 1028.759900\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 41.55361385384484\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1028.7599000930786\n",
      "Training Loss : 0.11672259867191315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 38.539680\n",
      "best mean reward 105.293323\n",
      "running time 1062.563837\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 38.53968008319041\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1062.5638370513916\n",
      "Training Loss : 0.17256471514701843\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 55.530974\n",
      "best mean reward 105.293323\n",
      "running time 1096.882963\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 55.53097368692379\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1096.8829629421234\n",
      "Training Loss : 0.0723557323217392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 45.340131\n",
      "best mean reward 105.293323\n",
      "running time 1131.070372\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 45.340130860807356\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1131.0703716278076\n",
      "Training Loss : 0.13002216815948486\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 57.074465\n",
      "best mean reward 105.293323\n",
      "running time 1165.321261\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 57.074464572271836\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1165.3212614059448\n",
      "Training Loss : 0.1812325268983841\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 84.329790\n",
      "best mean reward 105.293323\n",
      "running time 1215.533442\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 84.3297903920265\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1215.5334424972534\n",
      "Training Loss : 0.1298016458749771\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 92.860926\n",
      "best mean reward 105.293323\n",
      "running time 1251.286702\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 92.86092566987634\n",
      "Train_BestReturn : 105.29332318866125\n",
      "TimeSinceStart : 1251.2867019176483\n",
      "Training Loss : 0.14284677803516388\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 113.198614\n",
      "best mean reward 113.198614\n",
      "running time 1283.866194\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 113.19861361673466\n",
      "Train_BestReturn : 113.19861361673466\n",
      "TimeSinceStart : 1283.866194486618\n",
      "Training Loss : 0.14481954276561737\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 129.946436\n",
      "best mean reward 129.946436\n",
      "running time 1317.586283\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 129.94643556129935\n",
      "Train_BestReturn : 129.94643556129935\n",
      "TimeSinceStart : 1317.5862827301025\n",
      "Training Loss : 0.10042412579059601\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 137.063128\n",
      "best mean reward 137.063128\n",
      "running time 1348.467024\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 137.0631284914808\n",
      "Train_BestReturn : 137.0631284914808\n",
      "TimeSinceStart : 1348.4670240879059\n",
      "Training Loss : 3.8917107582092285\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 148.002997\n",
      "best mean reward 148.002997\n",
      "running time 1380.101827\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 148.00299704410682\n",
      "Train_BestReturn : 148.00299704410682\n",
      "TimeSinceStart : 1380.1018266677856\n",
      "Training Loss : 0.16980460286140442\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 166.260091\n",
      "best mean reward 166.260091\n",
      "running time 1408.319717\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 166.26009141670372\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1408.3197174072266\n",
      "Training Loss : 3.3171863555908203\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 162.561259\n",
      "best mean reward 166.260091\n",
      "running time 1438.012375\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 162.56125938261695\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1438.012374639511\n",
      "Training Loss : 0.23639428615570068\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 141.817604\n",
      "best mean reward 166.260091\n",
      "running time 1468.828695\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 141.81760433774235\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1468.8286945819855\n",
      "Training Loss : 6.778857707977295\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 126.150487\n",
      "best mean reward 166.260091\n",
      "running time 1496.021141\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 126.15048717472841\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1496.021140575409\n",
      "Training Loss : 0.24214795231819153\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 137.679024\n",
      "best mean reward 166.260091\n",
      "running time 1523.012933\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 137.67902399496361\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1523.0129325389862\n",
      "Training Loss : 0.14718376100063324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 157.126147\n",
      "best mean reward 166.260091\n",
      "running time 1550.041213\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 157.12614682005088\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1550.0412130355835\n",
      "Training Loss : 0.17564982175827026\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 134.137290\n",
      "best mean reward 166.260091\n",
      "running time 1577.624210\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 134.1372902106821\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1577.6242098808289\n",
      "Training Loss : 0.7083450555801392\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 106.063134\n",
      "best mean reward 166.260091\n",
      "running time 1604.212982\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 106.06313360790875\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1604.2129819393158\n",
      "Training Loss : 0.12486499547958374\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 66.876607\n",
      "best mean reward 166.260091\n",
      "running time 1630.687490\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 66.87660739085571\n",
      "Train_BestReturn : 166.26009141670372\n",
      "TimeSinceStart : 1630.6874902248383\n",
      "Training Loss : 0.08061710745096207\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_doubledqn_1 --double_q --seed 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_2_LunarLander-v3_15-10-2020_17-51-26 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_2_LunarLander-v3_15-10-2020_17-51-26\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.008200\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.008199930191040039\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -241.346799\n",
      "best mean reward -inf\n",
      "running time 140.162368\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -241.34679943545115\n",
      "TimeSinceStart : 140.16236782073975\n",
      "Training Loss : 0.5201081037521362\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -167.864120\n",
      "best mean reward -167.864120\n",
      "running time 168.934080\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -167.8641198125494\n",
      "Train_BestReturn : -167.8641198125494\n",
      "TimeSinceStart : 168.93408036231995\n",
      "Training Loss : 2.5301458835601807\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -153.451169\n",
      "best mean reward -153.451169\n",
      "running time 287.426270\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -153.45116859676088\n",
      "Train_BestReturn : -153.45116859676088\n",
      "TimeSinceStart : 287.42627024650574\n",
      "Training Loss : 0.40331947803497314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -143.069430\n",
      "best mean reward -143.069430\n",
      "running time 322.701328\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -143.06942981773952\n",
      "Train_BestReturn : -143.06942981773952\n",
      "TimeSinceStart : 322.7013280391693\n",
      "Training Loss : 0.39224478602409363\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -125.985782\n",
      "best mean reward -125.985782\n",
      "running time 361.210787\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -125.98578190436434\n",
      "Train_BestReturn : -125.98578190436434\n",
      "TimeSinceStart : 361.2107870578766\n",
      "Training Loss : 0.5273435115814209\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -109.008019\n",
      "best mean reward -109.008019\n",
      "running time 395.392047\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -109.00801874430792\n",
      "Train_BestReturn : -109.00801874430792\n",
      "TimeSinceStart : 395.3920474052429\n",
      "Training Loss : 0.8234105110168457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -103.039633\n",
      "best mean reward -103.039633\n",
      "running time 431.674745\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -103.03963274021689\n",
      "Train_BestReturn : -103.03963274021689\n",
      "TimeSinceStart : 431.6747450828552\n",
      "Training Loss : 0.6143113970756531\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -93.268182\n",
      "best mean reward -93.268182\n",
      "running time 466.455689\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -93.26818152463218\n",
      "Train_BestReturn : -93.26818152463218\n",
      "TimeSinceStart : 466.45568919181824\n",
      "Training Loss : 0.147718608379364\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -78.794031\n",
      "best mean reward -78.794031\n",
      "running time 500.514042\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -78.79403088452442\n",
      "Train_BestReturn : -78.79403088452442\n",
      "TimeSinceStart : 500.51404213905334\n",
      "Training Loss : 0.18032407760620117\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -69.981843\n",
      "best mean reward -69.981843\n",
      "running time 542.520687\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -69.9818428168839\n",
      "Train_BestReturn : -69.9818428168839\n",
      "TimeSinceStart : 542.5206873416901\n",
      "Training Loss : 0.1932763010263443\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -58.099311\n",
      "best mean reward -58.099311\n",
      "running time 577.762797\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -58.0993109963291\n",
      "Train_BestReturn : -58.0993109963291\n",
      "TimeSinceStart : 577.7627973556519\n",
      "Training Loss : 0.14604371786117554\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -41.184060\n",
      "best mean reward -41.184060\n",
      "running time 612.935708\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -41.18405992119197\n",
      "Train_BestReturn : -41.18405992119197\n",
      "TimeSinceStart : 612.9357075691223\n",
      "Training Loss : 0.42410245537757874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -27.767933\n",
      "best mean reward -27.767933\n",
      "running time 648.432001\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -27.76793296529932\n",
      "Train_BestReturn : -27.76793296529932\n",
      "TimeSinceStart : 648.432000875473\n",
      "Training Loss : 0.9175013303756714\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -16.038047\n",
      "best mean reward -16.038047\n",
      "running time 683.337142\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -16.03804743032863\n",
      "Train_BestReturn : -16.03804743032863\n",
      "TimeSinceStart : 683.3371424674988\n",
      "Training Loss : 0.20127229392528534\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 0.313716\n",
      "best mean reward 0.313716\n",
      "running time 719.006063\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 0.31371554775682325\n",
      "Train_BestReturn : 0.31371554775682325\n",
      "TimeSinceStart : 719.0060634613037\n",
      "Training Loss : 0.16652712225914001\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 9.561631\n",
      "best mean reward 9.561631\n",
      "running time 752.547998\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 9.561631374259818\n",
      "Train_BestReturn : 9.561631374259818\n",
      "TimeSinceStart : 752.5479979515076\n",
      "Training Loss : 0.09039542824029922\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 16.037019\n",
      "best mean reward 16.037019\n",
      "running time 787.403415\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 16.037018529026074\n",
      "Train_BestReturn : 16.037018529026074\n",
      "TimeSinceStart : 787.4034147262573\n",
      "Training Loss : 0.1003153845667839\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 31.938506\n",
      "best mean reward 31.938506\n",
      "running time 821.146435\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 31.93850642995436\n",
      "Train_BestReturn : 31.93850642995436\n",
      "TimeSinceStart : 821.1464352607727\n",
      "Training Loss : 0.2973588705062866\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 46.886998\n",
      "best mean reward 46.886998\n",
      "running time 860.380415\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 46.886997948847565\n",
      "Train_BestReturn : 46.886997948847565\n",
      "TimeSinceStart : 860.3804149627686\n",
      "Training Loss : 0.08927321434020996\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 51.695581\n",
      "best mean reward 51.695581\n",
      "running time 895.512807\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 51.69558090143454\n",
      "Train_BestReturn : 51.69558090143454\n",
      "TimeSinceStart : 895.5128071308136\n",
      "Training Loss : 0.14654366672039032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 58.873601\n",
      "best mean reward 58.873601\n",
      "running time 927.677492\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 58.873600512597584\n",
      "Train_BestReturn : 58.873600512597584\n",
      "TimeSinceStart : 927.677491903305\n",
      "Training Loss : 0.21996887028217316\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 59.508173\n",
      "best mean reward 59.508173\n",
      "running time 959.163545\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 59.50817321137687\n",
      "Train_BestReturn : 59.50817321137687\n",
      "TimeSinceStart : 959.1635453701019\n",
      "Training Loss : 0.06037355214357376\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 57.901430\n",
      "best mean reward 59.508173\n",
      "running time 991.340780\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 57.90143005284257\n",
      "Train_BestReturn : 59.50817321137687\n",
      "TimeSinceStart : 991.3407797813416\n",
      "Training Loss : 0.06070576608181\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 49.673782\n",
      "best mean reward 59.508173\n",
      "running time 1124.724629\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 49.673781611958056\n",
      "Train_BestReturn : 59.50817321137687\n",
      "TimeSinceStart : 1124.7246294021606\n",
      "Training Loss : 0.6147978901863098\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 55.462041\n",
      "best mean reward 59.508173\n",
      "running time 1155.919583\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 55.46204050294644\n",
      "Train_BestReturn : 59.50817321137687\n",
      "TimeSinceStart : 1155.919583082199\n",
      "Training Loss : 0.20711536705493927\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 63.317130\n",
      "best mean reward 63.317130\n",
      "running time 1186.916800\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 63.317130243071375\n",
      "Train_BestReturn : 63.317130243071375\n",
      "TimeSinceStart : 1186.9168004989624\n",
      "Training Loss : 0.46768683195114136\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 64.509036\n",
      "best mean reward 64.509036\n",
      "running time 1218.082113\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 64.50903614441182\n",
      "Train_BestReturn : 64.50903614441182\n",
      "TimeSinceStart : 1218.082112789154\n",
      "Training Loss : 0.695073664188385\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 83.746295\n",
      "best mean reward 83.746295\n",
      "running time 1247.898956\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 83.7462947053304\n",
      "Train_BestReturn : 83.7462947053304\n",
      "TimeSinceStart : 1247.8989555835724\n",
      "Training Loss : 0.24104130268096924\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 86.220241\n",
      "best mean reward 86.220241\n",
      "running time 1279.125083\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 86.22024054669365\n",
      "Train_BestReturn : 86.22024054669365\n",
      "TimeSinceStart : 1279.125083208084\n",
      "Training Loss : 0.26172757148742676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 83.737537\n",
      "best mean reward 86.220241\n",
      "running time 2306.653125\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 83.73753668731102\n",
      "Train_BestReturn : 86.22024054669365\n",
      "TimeSinceStart : 2306.6531252861023\n",
      "Training Loss : 0.15057286620140076\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 75.992682\n",
      "best mean reward 86.220241\n",
      "running time 2336.648488\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 75.99268226899854\n",
      "Train_BestReturn : 86.22024054669365\n",
      "TimeSinceStart : 2336.64848780632\n",
      "Training Loss : 1.0188469886779785\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 100.203044\n",
      "best mean reward 100.203044\n",
      "running time 2364.271444\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 100.20304448678866\n",
      "Train_BestReturn : 100.20304448678866\n",
      "TimeSinceStart : 2364.271443605423\n",
      "Training Loss : 0.2652590870857239\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 109.619024\n",
      "best mean reward 109.619024\n",
      "running time 2395.605798\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 109.61902447804752\n",
      "Train_BestReturn : 109.61902447804752\n",
      "TimeSinceStart : 2395.605797767639\n",
      "Training Loss : 0.1662491410970688\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 98.276480\n",
      "best mean reward 109.619024\n",
      "running time 2429.899853\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 98.27647958445952\n",
      "Train_BestReturn : 109.61902447804752\n",
      "TimeSinceStart : 2429.899852991104\n",
      "Training Loss : 0.42292600870132446\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 88.228798\n",
      "best mean reward 109.619024\n",
      "running time 2466.646283\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 88.22879788833578\n",
      "Train_BestReturn : 109.61902447804752\n",
      "TimeSinceStart : 2466.6462829113007\n",
      "Training Loss : 2.303269386291504\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 93.226751\n",
      "best mean reward 109.619024\n",
      "running time 2605.207447\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 93.22675087325682\n",
      "Train_BestReturn : 109.61902447804752\n",
      "TimeSinceStart : 2605.207447052002\n",
      "Training Loss : 0.25762996077537537\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 108.832762\n",
      "best mean reward 109.619024\n",
      "running time 2634.838845\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 108.83276190757262\n",
      "Train_BestReturn : 109.61902447804752\n",
      "TimeSinceStart : 2634.8388454914093\n",
      "Training Loss : 0.5535068511962891\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 117.915083\n",
      "best mean reward 117.915083\n",
      "running time 2665.909215\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 117.9150829043251\n",
      "Train_BestReturn : 117.9150829043251\n",
      "TimeSinceStart : 2665.9092149734497\n",
      "Training Loss : 0.07716499269008636\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 126.133384\n",
      "best mean reward 126.133384\n",
      "running time 2695.194824\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 126.13338356671511\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2695.1948235034943\n",
      "Training Loss : 0.12319217622280121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 74.524593\n",
      "best mean reward 126.133384\n",
      "running time 2723.600863\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 74.5245925152627\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2723.6008627414703\n",
      "Training Loss : 0.2366190254688263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 60.077937\n",
      "best mean reward 126.133384\n",
      "running time 2751.842651\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 60.07793664902167\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2751.8426513671875\n",
      "Training Loss : 0.23437687754631042\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 51.682303\n",
      "best mean reward 126.133384\n",
      "running time 2779.628859\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 51.682303307730805\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2779.628858566284\n",
      "Training Loss : 0.3500182032585144\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 27.211182\n",
      "best mean reward 126.133384\n",
      "running time 2807.558326\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 27.211182312342856\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2807.5583260059357\n",
      "Training Loss : 1.3004642724990845\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 68.167067\n",
      "best mean reward 126.133384\n",
      "running time 2835.023254\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 68.16706709771309\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2835.023253917694\n",
      "Training Loss : 0.23172950744628906\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 35.229875\n",
      "best mean reward 126.133384\n",
      "running time 2862.260790\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 35.22987465071681\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2862.260790348053\n",
      "Training Loss : 0.4240751266479492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 31.894355\n",
      "best mean reward 126.133384\n",
      "running time 2889.223181\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 31.894354774691507\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2889.2231814861298\n",
      "Training Loss : 12.224329948425293\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 17.808788\n",
      "best mean reward 126.133384\n",
      "running time 2916.842978\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 17.808788406773186\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2916.8429775238037\n",
      "Training Loss : 0.6612032651901245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 25.671396\n",
      "best mean reward 126.133384\n",
      "running time 2943.757645\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 25.67139595651772\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2943.757645368576\n",
      "Training Loss : 0.5082295536994934\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 14.419507\n",
      "best mean reward 126.133384\n",
      "running time 2970.725701\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 14.419506778819306\n",
      "Train_BestReturn : 126.13338356671511\n",
      "TimeSinceStart : 2970.725700855255\n",
      "Training Loss : 0.617897093296051\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_doubledqn_2 --double_q --seed 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_3_LunarLander-v3_15-10-2020_18-41-26 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q2_doubledqn_3_LunarLander-v3_15-10-2020_18-41-26\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.006689\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.006688594818115234\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -223.077219\n",
      "best mean reward -inf\n",
      "running time 724.686358\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -223.0772188886426\n",
      "TimeSinceStart : 724.6863579750061\n",
      "Training Loss : 0.5867453813552856\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -180.053929\n",
      "best mean reward -180.053929\n",
      "running time 753.901227\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -180.05392938090688\n",
      "Train_BestReturn : -180.05392938090688\n",
      "TimeSinceStart : 753.9012265205383\n",
      "Training Loss : 0.641108512878418\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -162.966887\n",
      "best mean reward -162.966887\n",
      "running time 787.304268\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -162.96688704510728\n",
      "Train_BestReturn : -162.96688704510728\n",
      "TimeSinceStart : 787.3042681217194\n",
      "Training Loss : 0.6295766830444336\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -152.306005\n",
      "best mean reward -152.306005\n",
      "running time 1563.761093\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -152.30600502861776\n",
      "Train_BestReturn : -152.30600502861776\n",
      "TimeSinceStart : 1563.7610929012299\n",
      "Training Loss : 0.5778341293334961\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -133.955397\n",
      "best mean reward -133.955397\n",
      "running time 1598.180506\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -133.95539742213504\n",
      "Train_BestReturn : -133.95539742213504\n",
      "TimeSinceStart : 1598.180505990982\n",
      "Training Loss : 0.6388716101646423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -128.452100\n",
      "best mean reward -128.452100\n",
      "running time 1633.408767\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -128.45209999722488\n",
      "Train_BestReturn : -128.45209999722488\n",
      "TimeSinceStart : 1633.408766746521\n",
      "Training Loss : 0.30162346363067627\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -102.754219\n",
      "best mean reward -102.754219\n",
      "running time 1667.844710\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -102.75421897455824\n",
      "Train_BestReturn : -102.75421897455824\n",
      "TimeSinceStart : 1667.844710111618\n",
      "Training Loss : 0.24843396246433258\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -85.574947\n",
      "best mean reward -85.574947\n",
      "running time 1700.653391\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -85.57494704936619\n",
      "Train_BestReturn : -85.57494704936619\n",
      "TimeSinceStart : 1700.6533913612366\n",
      "Training Loss : 0.4557158946990967\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -73.117196\n",
      "best mean reward -73.117196\n",
      "running time 1735.345364\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -73.11719550839854\n",
      "Train_BestReturn : -73.11719550839854\n",
      "TimeSinceStart : 1735.345363855362\n",
      "Training Loss : 0.2869962751865387\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -57.338410\n",
      "best mean reward -57.338410\n",
      "running time 1797.789159\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -57.338409637023624\n",
      "Train_BestReturn : -57.338409637023624\n",
      "TimeSinceStart : 1797.7891585826874\n",
      "Training Loss : 0.16555145382881165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -27.183140\n",
      "best mean reward -27.183140\n",
      "running time 2080.950308\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -27.183140259356865\n",
      "Train_BestReturn : -27.183140259356865\n",
      "TimeSinceStart : 2080.9503083229065\n",
      "Training Loss : 0.48742079734802246\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) 6.414216\n",
      "best mean reward 6.414216\n",
      "running time 2113.885007\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : 6.414215533158255\n",
      "Train_BestReturn : 6.414215533158255\n",
      "TimeSinceStart : 2113.8850071430206\n",
      "Training Loss : 0.41417235136032104\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 21.436927\n",
      "best mean reward 21.436927\n",
      "running time 2147.099103\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 21.436927027924057\n",
      "Train_BestReturn : 21.436927027924057\n",
      "TimeSinceStart : 2147.0991027355194\n",
      "Training Loss : 0.11012230813503265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 35.122369\n",
      "best mean reward 35.122369\n",
      "running time 2182.319038\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 35.1223692960062\n",
      "Train_BestReturn : 35.1223692960062\n",
      "TimeSinceStart : 2182.319037914276\n",
      "Training Loss : 0.13879692554473877\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 49.806023\n",
      "best mean reward 49.806023\n",
      "running time 2215.566037\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 49.80602317330409\n",
      "Train_BestReturn : 49.80602317330409\n",
      "TimeSinceStart : 2215.5660371780396\n",
      "Training Loss : 0.07626798748970032\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 60.860626\n",
      "best mean reward 60.860626\n",
      "running time 2246.751625\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 60.8606256857181\n",
      "Train_BestReturn : 60.8606256857181\n",
      "TimeSinceStart : 2246.751624584198\n",
      "Training Loss : 0.602881133556366\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 64.000440\n",
      "best mean reward 64.000440\n",
      "running time 2452.773559\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 64.00043986789363\n",
      "Train_BestReturn : 64.00043986789363\n",
      "TimeSinceStart : 2452.7735590934753\n",
      "Training Loss : 0.10231909155845642\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 69.736069\n",
      "best mean reward 69.736069\n",
      "running time 2485.237479\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 69.73606895711305\n",
      "Train_BestReturn : 69.73606895711305\n",
      "TimeSinceStart : 2485.2374794483185\n",
      "Training Loss : 0.529769778251648\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 83.353652\n",
      "best mean reward 83.353652\n",
      "running time 2517.854518\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 83.35365168877297\n",
      "Train_BestReturn : 83.35365168877297\n",
      "TimeSinceStart : 2517.8545179367065\n",
      "Training Loss : 0.27691295742988586\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 85.994717\n",
      "best mean reward 85.994717\n",
      "running time 2550.033843\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 85.99471740716197\n",
      "Train_BestReturn : 85.99471740716197\n",
      "TimeSinceStart : 2550.0338430404663\n",
      "Training Loss : 2.0256714820861816\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 77.417114\n",
      "best mean reward 85.994717\n",
      "running time 2585.865983\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 77.4171135394681\n",
      "Train_BestReturn : 85.99471740716197\n",
      "TimeSinceStart : 2585.86598277092\n",
      "Training Loss : 2.6161599159240723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 77.326014\n",
      "best mean reward 85.994717\n",
      "running time 2618.857319\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 77.3260135911051\n",
      "Train_BestReturn : 85.99471740716197\n",
      "TimeSinceStart : 2618.857319355011\n",
      "Training Loss : 0.21209707856178284\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 73.800010\n",
      "best mean reward 85.994717\n",
      "running time 2651.336131\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 73.80001041904858\n",
      "Train_BestReturn : 85.99471740716197\n",
      "TimeSinceStart : 2651.336130619049\n",
      "Training Loss : 0.12031900882720947\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 89.084385\n",
      "best mean reward 89.084385\n",
      "running time 2681.442237\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 89.08438513329752\n",
      "Train_BestReturn : 89.08438513329752\n",
      "TimeSinceStart : 2681.4422369003296\n",
      "Training Loss : 0.4545848071575165\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 106.651870\n",
      "best mean reward 106.651870\n",
      "running time 2984.078437\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 106.65187041256621\n",
      "Train_BestReturn : 106.65187041256621\n",
      "TimeSinceStart : 2984.078436613083\n",
      "Training Loss : 0.8689162731170654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 116.458590\n",
      "best mean reward 116.458590\n",
      "running time 3015.093008\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 116.45858955202735\n",
      "Train_BestReturn : 116.45858955202735\n",
      "TimeSinceStart : 3015.0930075645447\n",
      "Training Loss : 0.17521725594997406\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 126.132947\n",
      "best mean reward 126.132947\n",
      "running time 3044.246223\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 126.13294679841361\n",
      "Train_BestReturn : 126.13294679841361\n",
      "TimeSinceStart : 3044.2462232112885\n",
      "Training Loss : 0.22725477814674377\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 130.665290\n",
      "best mean reward 130.665290\n",
      "running time 3072.823617\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 130.66529028783455\n",
      "Train_BestReturn : 130.66529028783455\n",
      "TimeSinceStart : 3072.8236169815063\n",
      "Training Loss : 0.22710932791233063\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 132.982121\n",
      "best mean reward 132.982121\n",
      "running time 3101.494648\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 132.98212131048678\n",
      "Train_BestReturn : 132.98212131048678\n",
      "TimeSinceStart : 3101.494647502899\n",
      "Training Loss : 0.11237703263759613\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 132.377791\n",
      "best mean reward 132.982121\n",
      "running time 3130.715762\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 132.377791184164\n",
      "Train_BestReturn : 132.98212131048678\n",
      "TimeSinceStart : 3130.7157616615295\n",
      "Training Loss : 0.12716838717460632\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 141.526022\n",
      "best mean reward 141.526022\n",
      "running time 3592.026592\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 141.52602155124075\n",
      "Train_BestReturn : 141.52602155124075\n",
      "TimeSinceStart : 3592.02659201622\n",
      "Training Loss : 0.5254722833633423\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 153.067923\n",
      "best mean reward 153.067923\n",
      "running time 3619.433338\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 153.06792272767404\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3619.433338403702\n",
      "Training Loss : 0.2105468511581421\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 149.262390\n",
      "best mean reward 153.067923\n",
      "running time 3646.964006\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 149.2623900786799\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3646.9640061855316\n",
      "Training Loss : 0.8546762466430664\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 138.274859\n",
      "best mean reward 153.067923\n",
      "running time 3674.480853\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 138.2748591691163\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3674.480853319168\n",
      "Training Loss : 0.2401401847600937\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 132.243362\n",
      "best mean reward 153.067923\n",
      "running time 3910.594090\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 132.2433618170376\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3910.5940902233124\n",
      "Training Loss : 0.3342319428920746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 112.870281\n",
      "best mean reward 153.067923\n",
      "running time 3938.902421\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 112.87028146065964\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3938.9024205207825\n",
      "Training Loss : 2.530811071395874\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 116.878871\n",
      "best mean reward 153.067923\n",
      "running time 3967.597247\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 116.87887146920872\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3967.597246646881\n",
      "Training Loss : 0.16895440220832825\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 110.807122\n",
      "best mean reward 153.067923\n",
      "running time 3995.329957\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 110.80712180431365\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 3995.3299567699432\n",
      "Training Loss : 1.0622748136520386\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 68.058908\n",
      "best mean reward 153.067923\n",
      "running time 4022.546863\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 68.05890799874054\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4022.546863079071\n",
      "Training Loss : 0.3951806128025055\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 76.339678\n",
      "best mean reward 153.067923\n",
      "running time 4049.834544\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 76.33967800912453\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4049.834543943405\n",
      "Training Loss : 0.3435192108154297\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 28.009936\n",
      "best mean reward 153.067923\n",
      "running time 4077.051451\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 28.009936366590072\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4077.05145072937\n",
      "Training Loss : 1.0880956649780273\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) -24.672097\n",
      "best mean reward 153.067923\n",
      "running time 4104.526059\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : -24.672097003751496\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4104.526058912277\n",
      "Training Loss : 0.3946540057659149\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) -53.446478\n",
      "best mean reward 153.067923\n",
      "running time 4131.759568\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : -53.44647797149998\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4131.759567975998\n",
      "Training Loss : 0.13843736052513123\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) -54.801275\n",
      "best mean reward 153.067923\n",
      "running time 4158.792538\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : -54.80127526879222\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4158.792537689209\n",
      "Training Loss : 0.25315871834754944\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) -19.715563\n",
      "best mean reward 153.067923\n",
      "running time 4185.777457\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : -19.71556294839386\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4185.777456998825\n",
      "Training Loss : 1.0984218120574951\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) -18.436913\n",
      "best mean reward 153.067923\n",
      "running time 4212.563719\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : -18.43691324302223\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4212.5637192726135\n",
      "Training Loss : 11.193909645080566\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) -13.786636\n",
      "best mean reward 153.067923\n",
      "running time 4239.578331\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : -13.786635823995118\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4239.578330516815\n",
      "Training Loss : 0.3671085238456726\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 16.213439\n",
      "best mean reward 153.067923\n",
      "running time 4266.323258\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 16.213439110787977\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4266.323257684708\n",
      "Training Loss : 1.1446512937545776\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 15.001296\n",
      "best mean reward 153.067923\n",
      "running time 4293.624300\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 15.001295652156408\n",
      "Train_BestReturn : 153.06792272767404\n",
      "TimeSinceStart : 4293.624300479889\n",
      "Training Loss : 0.34498268365859985\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q2_doubledqn_3 --double_q --seed 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q3_hparam1_LunarLander-v3_17-10-2020_18-37-41 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q3_hparam1_LunarLander-v3_17-10-2020_18-37-41\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.008007\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.008007287979125977\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -259.782416\n",
      "best mean reward -inf\n",
      "running time 34.604395\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -259.7824160236244\n",
      "TimeSinceStart : 34.604395389556885\n",
      "Training Loss : 0.7642713189125061\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -211.505310\n",
      "best mean reward -inf\n",
      "running time 64.778460\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -211.50530950518473\n",
      "TimeSinceStart : 64.77846002578735\n",
      "Training Loss : 0.5119469165802002\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -188.965388\n",
      "best mean reward -188.965388\n",
      "running time 99.056767\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -188.965388154684\n",
      "Train_BestReturn : -188.965388154684\n",
      "TimeSinceStart : 99.0567672252655\n",
      "Training Loss : 0.2949095368385315\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -176.686045\n",
      "best mean reward -176.686045\n",
      "running time 136.875064\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -176.68604544330472\n",
      "Train_BestReturn : -176.68604544330472\n",
      "TimeSinceStart : 136.87506437301636\n",
      "Training Loss : 9.55926513671875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -158.084898\n",
      "best mean reward -158.084898\n",
      "running time 191.186056\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -158.08489770129208\n",
      "Train_BestReturn : -158.08489770129208\n",
      "TimeSinceStart : 191.18605589866638\n",
      "Training Loss : 0.271543025970459\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -136.798093\n",
      "best mean reward -136.798093\n",
      "running time 225.616513\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -136.79809345101697\n",
      "Train_BestReturn : -136.79809345101697\n",
      "TimeSinceStart : 225.6165132522583\n",
      "Training Loss : 0.2710339426994324\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -111.454015\n",
      "best mean reward -111.454015\n",
      "running time 258.326560\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -111.45401533459122\n",
      "Train_BestReturn : -111.45401533459122\n",
      "TimeSinceStart : 258.32656025886536\n",
      "Training Loss : 0.24404388666152954\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -72.795096\n",
      "best mean reward -72.795096\n",
      "running time 290.609805\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -72.79509559125208\n",
      "Train_BestReturn : -72.79509559125208\n",
      "TimeSinceStart : 290.6098048686981\n",
      "Training Loss : 0.40091603994369507\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -58.779153\n",
      "best mean reward -58.779153\n",
      "running time 322.516863\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -58.77915294779219\n",
      "Train_BestReturn : -58.77915294779219\n",
      "TimeSinceStart : 322.5168628692627\n",
      "Training Loss : 0.7814165353775024\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -58.513817\n",
      "best mean reward -58.513817\n",
      "running time 354.937987\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -58.51381705482365\n",
      "Train_BestReturn : -58.51381705482365\n",
      "TimeSinceStart : 354.9379873275757\n",
      "Training Loss : 0.4173111915588379\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -41.754447\n",
      "best mean reward -41.754447\n",
      "running time 396.423184\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -41.754447374653864\n",
      "Train_BestReturn : -41.754447374653864\n",
      "TimeSinceStart : 396.4231836795807\n",
      "Training Loss : 0.24879387021064758\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -19.205351\n",
      "best mean reward -19.205351\n",
      "running time 429.363090\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -19.205351218448733\n",
      "Train_BestReturn : -19.205351218448733\n",
      "TimeSinceStart : 429.3630895614624\n",
      "Training Loss : 0.23344281315803528\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -7.296139\n",
      "best mean reward -7.296139\n",
      "running time 462.154588\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -7.296139053533219\n",
      "Train_BestReturn : -7.296139053533219\n",
      "TimeSinceStart : 462.15458822250366\n",
      "Training Loss : 0.11330550909042358\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 10.853580\n",
      "best mean reward 10.853580\n",
      "running time 493.669746\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 10.853580487896\n",
      "Train_BestReturn : 10.853580487896\n",
      "TimeSinceStart : 493.6697463989258\n",
      "Training Loss : 0.8794803023338318\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 13.416368\n",
      "best mean reward 13.416368\n",
      "running time 525.230718\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 13.416367898847534\n",
      "Train_BestReturn : 13.416367898847534\n",
      "TimeSinceStart : 525.2307176589966\n",
      "Training Loss : 1.171033263206482\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 21.899354\n",
      "best mean reward 21.899354\n",
      "running time 559.060843\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 21.899353659147682\n",
      "Train_BestReturn : 21.899353659147682\n",
      "TimeSinceStart : 559.0608427524567\n",
      "Training Loss : 0.09641505777835846\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 28.328342\n",
      "best mean reward 28.328342\n",
      "running time 590.335042\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 28.328341689313664\n",
      "Train_BestReturn : 28.328341689313664\n",
      "TimeSinceStart : 590.3350415229797\n",
      "Training Loss : 0.26748716831207275\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 35.887378\n",
      "best mean reward 35.887378\n",
      "running time 621.834005\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 35.887377563149926\n",
      "Train_BestReturn : 35.887377563149926\n",
      "TimeSinceStart : 621.834005355835\n",
      "Training Loss : 0.5835155248641968\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 55.916125\n",
      "best mean reward 55.916125\n",
      "running time 654.077686\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 55.916124588278414\n",
      "Train_BestReturn : 55.916124588278414\n",
      "TimeSinceStart : 654.0776858329773\n",
      "Training Loss : 0.07503440231084824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 70.717066\n",
      "best mean reward 70.717066\n",
      "running time 682.576424\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 70.71706584701398\n",
      "Train_BestReturn : 70.71706584701398\n",
      "TimeSinceStart : 682.5764243602753\n",
      "Training Loss : 0.14026430249214172\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 88.811989\n",
      "best mean reward 88.811989\n",
      "running time 712.216316\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 88.81198882627821\n",
      "Train_BestReturn : 88.81198882627821\n",
      "TimeSinceStart : 712.216315984726\n",
      "Training Loss : 0.08210251480340958\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 93.261374\n",
      "best mean reward 93.261374\n",
      "running time 740.658032\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 93.26137397981199\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 740.6580321788788\n",
      "Training Loss : 0.07484613358974457\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 82.936324\n",
      "best mean reward 93.261374\n",
      "running time 767.787389\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 82.93632398901428\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 767.7873888015747\n",
      "Training Loss : 0.8163758516311646\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 65.090471\n",
      "best mean reward 93.261374\n",
      "running time 795.308635\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 65.09047056741053\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 795.3086352348328\n",
      "Training Loss : 0.37622660398483276\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 70.901335\n",
      "best mean reward 93.261374\n",
      "running time 827.145881\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 70.90133522172\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 827.1458809375763\n",
      "Training Loss : 2.146144390106201\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 66.113039\n",
      "best mean reward 93.261374\n",
      "running time 854.068749\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 66.1130391923289\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 854.0687494277954\n",
      "Training Loss : 1.3436007499694824\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 34.009338\n",
      "best mean reward 93.261374\n",
      "running time 883.706588\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 34.009337620813604\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 883.7065875530243\n",
      "Training Loss : 0.10620377957820892\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 44.873760\n",
      "best mean reward 93.261374\n",
      "running time 911.477881\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 44.87375957530031\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 911.4778809547424\n",
      "Training Loss : 4.609378337860107\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 30.467173\n",
      "best mean reward 93.261374\n",
      "running time 939.477990\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 30.467172848694563\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 939.4779901504517\n",
      "Training Loss : 0.4950515925884247\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 22.619020\n",
      "best mean reward 93.261374\n",
      "running time 988.756312\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 22.619019634442385\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 988.7563123703003\n",
      "Training Loss : 2.6288552284240723\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 20.005253\n",
      "best mean reward 93.261374\n",
      "running time 1016.952899\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 20.005253069277067\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1016.9528994560242\n",
      "Training Loss : 0.10984078049659729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 42.549779\n",
      "best mean reward 93.261374\n",
      "running time 1045.692045\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 42.549779310817804\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1045.692045211792\n",
      "Training Loss : 0.4287061095237732\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 38.427230\n",
      "best mean reward 93.261374\n",
      "running time 1077.270886\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 38.42723030543903\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1077.2708864212036\n",
      "Training Loss : 0.2547489106655121\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 61.607969\n",
      "best mean reward 93.261374\n",
      "running time 1104.353496\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 61.607969306294905\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1104.3534960746765\n",
      "Training Loss : 0.827163815498352\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 72.021468\n",
      "best mean reward 93.261374\n",
      "running time 1130.813623\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 72.02146803937157\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1130.813622713089\n",
      "Training Loss : 0.4444549083709717\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 72.572827\n",
      "best mean reward 93.261374\n",
      "running time 1230.615329\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 72.57282700409044\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1230.6153287887573\n",
      "Training Loss : 0.5798593163490295\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 73.853600\n",
      "best mean reward 93.261374\n",
      "running time 1256.405571\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 73.8535995874437\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1256.4055714607239\n",
      "Training Loss : 0.4121331572532654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 88.333077\n",
      "best mean reward 93.261374\n",
      "running time 1281.593088\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 88.33307676533792\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1281.5930879116058\n",
      "Training Loss : 0.569091796875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 46.601607\n",
      "best mean reward 93.261374\n",
      "running time 1306.910720\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 46.601607064049425\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1306.9107203483582\n",
      "Training Loss : 0.7565723061561584\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 23.879193\n",
      "best mean reward 93.261374\n",
      "running time 1332.138136\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 23.879192926637963\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1332.1381359100342\n",
      "Training Loss : 0.24198631942272186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 5.132885\n",
      "best mean reward 93.261374\n",
      "running time 1357.391049\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 5.132885051777012\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1357.3910491466522\n",
      "Training Loss : 1.5997356176376343\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) -29.180161\n",
      "best mean reward 93.261374\n",
      "running time 1382.161061\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : -29.180161217043384\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1382.1610605716705\n",
      "Training Loss : 1.2732834815979004\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) -48.059145\n",
      "best mean reward 93.261374\n",
      "running time 1408.761806\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : -48.05914514500519\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1408.7618057727814\n",
      "Training Loss : 0.28429704904556274\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) -37.407190\n",
      "best mean reward 93.261374\n",
      "running time 1434.447243\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : -37.40718961840886\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1434.4472434520721\n",
      "Training Loss : 0.2298305481672287\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) -81.850754\n",
      "best mean reward 93.261374\n",
      "running time 1459.040978\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : -81.85075429902854\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1459.040978193283\n",
      "Training Loss : 0.2184857577085495\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) -54.193045\n",
      "best mean reward 93.261374\n",
      "running time 1483.747981\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : -54.19304536844542\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1483.7479810714722\n",
      "Training Loss : 2.3259105682373047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) -36.834200\n",
      "best mean reward 93.261374\n",
      "running time 1508.843403\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : -36.83420010795273\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1508.843403339386\n",
      "Training Loss : 7.51668119430542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) -63.130024\n",
      "best mean reward 93.261374\n",
      "running time 1534.195114\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : -63.1300239515093\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1534.1951141357422\n",
      "Training Loss : 10.201117515563965\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) -20.277384\n",
      "best mean reward 93.261374\n",
      "running time 1840.836961\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : -20.277384421731146\n",
      "Train_BestReturn : 93.26137397981199\n",
      "TimeSinceStart : 1840.8369612693787\n",
      "Training Loss : 0.4787808954715729\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q3_hparam1 --learning_starts 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q3_hparam2_LunarLander-v3_17-10-2020_19-08-51 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q3_hparam2_LunarLander-v3_17-10-2020_19-08-51\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.007056\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.007055997848510742\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -291.335859\n",
      "best mean reward -inf\n",
      "running time 615.641493\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -291.33585901782016\n",
      "TimeSinceStart : 615.6414933204651\n",
      "Training Loss : 0.20642799139022827\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -253.342545\n",
      "best mean reward -253.342545\n",
      "running time 641.849817\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -253.34254466724707\n",
      "Train_BestReturn : -253.34254466724707\n",
      "TimeSinceStart : 641.8498170375824\n",
      "Training Loss : 0.839765191078186\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -227.731441\n",
      "best mean reward -227.731441\n",
      "running time 870.278673\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -227.73144064812476\n",
      "Train_BestReturn : -227.73144064812476\n",
      "TimeSinceStart : 870.2786729335785\n",
      "Training Loss : 0.4295825958251953\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -201.630550\n",
      "best mean reward -201.630550\n",
      "running time 903.507611\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -201.63055028810544\n",
      "Train_BestReturn : -201.63055028810544\n",
      "TimeSinceStart : 903.5076110363007\n",
      "Training Loss : 0.3303343653678894\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -179.018599\n",
      "best mean reward -179.018599\n",
      "running time 938.251864\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -179.01859922403696\n",
      "Train_BestReturn : -179.01859922403696\n",
      "TimeSinceStart : 938.2518644332886\n",
      "Training Loss : 0.1690969467163086\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -162.358290\n",
      "best mean reward -162.358290\n",
      "running time 972.009212\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -162.35828998047987\n",
      "Train_BestReturn : -162.35828998047987\n",
      "TimeSinceStart : 972.0092122554779\n",
      "Training Loss : 1.5414540767669678\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -134.204333\n",
      "best mean reward -134.204333\n",
      "running time 1003.600133\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -134.20433302925315\n",
      "Train_BestReturn : -134.20433302925315\n",
      "TimeSinceStart : 1003.6001327037811\n",
      "Training Loss : 0.22809602320194244\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -99.945761\n",
      "best mean reward -99.945761\n",
      "running time 1032.601775\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -99.94576125923182\n",
      "Train_BestReturn : -99.94576125923182\n",
      "TimeSinceStart : 1032.601774930954\n",
      "Training Loss : 0.21472617983818054\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -78.614019\n",
      "best mean reward -78.614019\n",
      "running time 1879.613035\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -78.6140194251008\n",
      "Train_BestReturn : -78.6140194251008\n",
      "TimeSinceStart : 1879.6130349636078\n",
      "Training Loss : 0.3648897409439087\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -49.383341\n",
      "best mean reward -49.383341\n",
      "running time 2090.156808\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -49.38334144840073\n",
      "Train_BestReturn : -49.38334144840073\n",
      "TimeSinceStart : 2090.1568081378937\n",
      "Training Loss : 0.1697266548871994\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -5.723345\n",
      "best mean reward -5.723345\n",
      "running time 2118.941473\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -5.7233448408285446\n",
      "Train_BestReturn : -5.7233448408285446\n",
      "TimeSinceStart : 2118.941472530365\n",
      "Training Loss : 0.43718037009239197\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) 5.788546\n",
      "best mean reward 5.788546\n",
      "running time 2151.763919\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : 5.788546062297222\n",
      "Train_BestReturn : 5.788546062297222\n",
      "TimeSinceStart : 2151.7639191150665\n",
      "Training Loss : 0.39726102352142334\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) 25.953240\n",
      "best mean reward 25.953240\n",
      "running time 2184.080292\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : 25.95323958093803\n",
      "Train_BestReturn : 25.95323958093803\n",
      "TimeSinceStart : 2184.0802915096283\n",
      "Training Loss : 0.26449570059776306\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) 52.640511\n",
      "best mean reward 52.640511\n",
      "running time 2215.779652\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : 52.64051092872015\n",
      "Train_BestReturn : 52.64051092872015\n",
      "TimeSinceStart : 2215.779652118683\n",
      "Training Loss : 0.14969030022621155\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) 67.668161\n",
      "best mean reward 67.668161\n",
      "running time 2247.551663\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : 67.66816060321224\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2247.5516629219055\n",
      "Training Loss : 0.710405707359314\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 63.299914\n",
      "best mean reward 67.668161\n",
      "running time 2566.458544\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 63.299914068592805\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2566.4585440158844\n",
      "Training Loss : 0.11009570956230164\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 58.149064\n",
      "best mean reward 67.668161\n",
      "running time 2598.551076\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 58.14906443121443\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2598.551076412201\n",
      "Training Loss : 0.08975451439619064\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 55.949415\n",
      "best mean reward 67.668161\n",
      "running time 2633.353302\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 55.94941534937291\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2633.3533017635345\n",
      "Training Loss : 0.10707878321409225\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 55.122947\n",
      "best mean reward 67.668161\n",
      "running time 2664.408600\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 55.12294660168757\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2664.408600091934\n",
      "Training Loss : 0.1558760404586792\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 56.283786\n",
      "best mean reward 67.668161\n",
      "running time 2695.811457\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 56.283785995992105\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2695.8114569187164\n",
      "Training Loss : 0.1675320416688919\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 47.417880\n",
      "best mean reward 67.668161\n",
      "running time 2729.583570\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 47.41787984598866\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2729.5835695266724\n",
      "Training Loss : 0.3435714542865753\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 37.455712\n",
      "best mean reward 67.668161\n",
      "running time 2760.796963\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 37.4557122133667\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2760.796963453293\n",
      "Training Loss : 0.08184684813022614\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 52.595793\n",
      "best mean reward 67.668161\n",
      "running time 2792.569841\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 52.59579283628132\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2792.569840669632\n",
      "Training Loss : 0.18596571683883667\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 44.050341\n",
      "best mean reward 67.668161\n",
      "running time 2823.034734\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 44.05034144448181\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2823.0347335338593\n",
      "Training Loss : 1.1294649839401245\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 48.156175\n",
      "best mean reward 67.668161\n",
      "running time 2950.573410\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 48.156175101289726\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2950.5734102725983\n",
      "Training Loss : 0.05534863844513893\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 49.196354\n",
      "best mean reward 67.668161\n",
      "running time 2980.640083\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 49.1963538353399\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 2980.6400830745697\n",
      "Training Loss : 0.8172901272773743\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 33.785405\n",
      "best mean reward 67.668161\n",
      "running time 3010.110030\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 33.78540476699971\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 3010.1100301742554\n",
      "Training Loss : 0.8397071361541748\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 35.075441\n",
      "best mean reward 67.668161\n",
      "running time 3041.611752\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 35.07544130465785\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 3041.611752271652\n",
      "Training Loss : 0.8910499215126038\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 43.227885\n",
      "best mean reward 67.668161\n",
      "running time 3072.190604\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 43.227884675011076\n",
      "Train_BestReturn : 67.66816060321224\n",
      "TimeSinceStart : 3072.1906042099\n",
      "Training Loss : 0.6241416931152344\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 73.818359\n",
      "best mean reward 73.818359\n",
      "running time 3103.195990\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 73.81835928495407\n",
      "Train_BestReturn : 73.81835928495407\n",
      "TimeSinceStart : 3103.1959896087646\n",
      "Training Loss : 0.4620685577392578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 97.237315\n",
      "best mean reward 97.237315\n",
      "running time 3131.396153\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 97.23731481974\n",
      "Train_BestReturn : 97.23731481974\n",
      "TimeSinceStart : 3131.3961532115936\n",
      "Training Loss : 1.5863800048828125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 113.514912\n",
      "best mean reward 113.514912\n",
      "running time 3448.481619\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 113.51491165364065\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3448.481619119644\n",
      "Training Loss : 0.1878209114074707\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 100.604924\n",
      "best mean reward 113.514912\n",
      "running time 3477.372594\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 100.60492411603974\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3477.3725938796997\n",
      "Training Loss : 0.10570767521858215\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 97.616972\n",
      "best mean reward 113.514912\n",
      "running time 3507.523792\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 97.61697150548966\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3507.5237917900085\n",
      "Training Loss : 0.5498032569885254\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 93.651336\n",
      "best mean reward 113.514912\n",
      "running time 3547.908280\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 93.65133595797141\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3547.9082798957825\n",
      "Training Loss : 0.6842187643051147\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 89.018300\n",
      "best mean reward 113.514912\n",
      "running time 3589.919623\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 89.01829979559659\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3589.9196226596832\n",
      "Training Loss : 0.2253134846687317\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 88.449886\n",
      "best mean reward 113.514912\n",
      "running time 3631.336805\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 88.44988577594683\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3631.336805343628\n",
      "Training Loss : 0.30336904525756836\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 89.540720\n",
      "best mean reward 113.514912\n",
      "running time 3671.768077\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 89.54071980558179\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3671.768077135086\n",
      "Training Loss : 0.5675680041313171\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 95.427023\n",
      "best mean reward 113.514912\n",
      "running time 3711.442921\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 95.427023392722\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3711.4429206848145\n",
      "Training Loss : 0.2152339220046997\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 90.548317\n",
      "best mean reward 113.514912\n",
      "running time 3869.091029\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 90.54831708277645\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3869.0910291671753\n",
      "Training Loss : 0.16329362988471985\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 96.350349\n",
      "best mean reward 113.514912\n",
      "running time 3905.839264\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 96.35034888251401\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3905.839264154434\n",
      "Training Loss : 0.6426569223403931\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 94.438770\n",
      "best mean reward 113.514912\n",
      "running time 3942.530489\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 94.43876976257285\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3942.530488729477\n",
      "Training Loss : 2.9302423000335693\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 86.573052\n",
      "best mean reward 113.514912\n",
      "running time 3981.179805\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 86.57305228728909\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 3981.179804801941\n",
      "Training Loss : 0.36221781373023987\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 97.094276\n",
      "best mean reward 113.514912\n",
      "running time 4017.808897\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 97.09427578491918\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 4017.808896780014\n",
      "Training Loss : 2.9179012775421143\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 95.894894\n",
      "best mean reward 113.514912\n",
      "running time 4055.714007\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 95.89489407308363\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 4055.7140069007874\n",
      "Training Loss : 0.37470489740371704\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 106.154063\n",
      "best mean reward 113.514912\n",
      "running time 4094.675875\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 106.15406265556447\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 4094.67587518692\n",
      "Training Loss : 0.1564033031463623\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 92.392872\n",
      "best mean reward 113.514912\n",
      "running time 4130.639382\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 92.39287233135127\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 4130.639381885529\n",
      "Training Loss : 0.29259276390075684\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 85.448897\n",
      "best mean reward 113.514912\n",
      "running time 4164.523654\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 85.44889662442833\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 4164.523653507233\n",
      "Training Loss : 0.09247292578220367\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 90.783699\n",
      "best mean reward 113.514912\n",
      "running time 4199.293497\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 90.7836989815185\n",
      "Train_BestReturn : 113.51491165364065\n",
      "TimeSinceStart : 4199.293496847153\n",
      "Training Loss : 1.2002804279327393\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q3_hparam2 --learning_starts 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q3_hparam3_LunarLander-v3_17-10-2020_20-19-27 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_q3_hparam3_LunarLander-v3_17-10-2020_20-19-27\n",
      "########################\n",
      "Using GPU id 0\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "/home/tomas/.local/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Environment '<class 'cs285.envs.box2d.lunar_lander.LunarLander'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 1\n",
      "mean reward (100 episodes) nan\n",
      "best mean reward -inf\n",
      "running time 0.008468\n",
      "Train_EnvstepsSoFar : 1\n",
      "TimeSinceStart : 0.008467912673950195\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 10001\n",
      "mean reward (100 episodes) -267.172243\n",
      "best mean reward -inf\n",
      "running time 569.170250\n",
      "Train_EnvstepsSoFar : 10001\n",
      "Train_AverageReturn : -267.1722431784343\n",
      "TimeSinceStart : 569.1702497005463\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 20001\n",
      "mean reward (100 episodes) -239.524444\n",
      "best mean reward -239.524444\n",
      "running time 744.623570\n",
      "Train_EnvstepsSoFar : 20001\n",
      "Train_AverageReturn : -239.52444391976024\n",
      "Train_BestReturn : -239.52444391976024\n",
      "TimeSinceStart : 744.6235702037811\n",
      "Training Loss : 0.32501718401908875\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 30001\n",
      "mean reward (100 episodes) -211.008668\n",
      "best mean reward -211.008668\n",
      "running time 776.878036\n",
      "Train_EnvstepsSoFar : 30001\n",
      "Train_AverageReturn : -211.0086680181079\n",
      "Train_BestReturn : -211.0086680181079\n",
      "TimeSinceStart : 776.8780362606049\n",
      "Training Loss : 0.29661935567855835\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 40001\n",
      "mean reward (100 episodes) -190.030315\n",
      "best mean reward -190.030315\n",
      "running time 813.202990\n",
      "Train_EnvstepsSoFar : 40001\n",
      "Train_AverageReturn : -190.0303147068918\n",
      "Train_BestReturn : -190.0303147068918\n",
      "TimeSinceStart : 813.2029895782471\n",
      "Training Loss : 1.620305061340332\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 50001\n",
      "mean reward (100 episodes) -169.048089\n",
      "best mean reward -169.048089\n",
      "running time 846.551991\n",
      "Train_EnvstepsSoFar : 50001\n",
      "Train_AverageReturn : -169.0480888430613\n",
      "Train_BestReturn : -169.0480888430613\n",
      "TimeSinceStart : 846.5519909858704\n",
      "Training Loss : 0.22354155778884888\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 60001\n",
      "mean reward (100 episodes) -143.645018\n",
      "best mean reward -143.645018\n",
      "running time 879.055427\n",
      "Train_EnvstepsSoFar : 60001\n",
      "Train_AverageReturn : -143.64501839290762\n",
      "Train_BestReturn : -143.64501839290762\n",
      "TimeSinceStart : 879.0554270744324\n",
      "Training Loss : 1.1293179988861084\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 70001\n",
      "mean reward (100 episodes) -123.865771\n",
      "best mean reward -123.865771\n",
      "running time 912.689770\n",
      "Train_EnvstepsSoFar : 70001\n",
      "Train_AverageReturn : -123.86577123268917\n",
      "Train_BestReturn : -123.86577123268917\n",
      "TimeSinceStart : 912.6897702217102\n",
      "Training Loss : 0.4852082133293152\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 80001\n",
      "mean reward (100 episodes) -95.124083\n",
      "best mean reward -95.124083\n",
      "running time 948.410697\n",
      "Train_EnvstepsSoFar : 80001\n",
      "Train_AverageReturn : -95.1240833691022\n",
      "Train_BestReturn : -95.1240833691022\n",
      "TimeSinceStart : 948.4106969833374\n",
      "Training Loss : 0.24934878945350647\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 90001\n",
      "mean reward (100 episodes) -90.588684\n",
      "best mean reward -90.588684\n",
      "running time 1353.085923\n",
      "Train_EnvstepsSoFar : 90001\n",
      "Train_AverageReturn : -90.58868441355794\n",
      "Train_BestReturn : -90.58868441355794\n",
      "TimeSinceStart : 1353.0859229564667\n",
      "Training Loss : 0.17113548517227173\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 100000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 100001\n",
      "mean reward (100 episodes) -80.896091\n",
      "best mean reward -80.896091\n",
      "running time 1387.611591\n",
      "Train_EnvstepsSoFar : 100001\n",
      "Train_AverageReturn : -80.89609110499698\n",
      "Train_BestReturn : -80.89609110499698\n",
      "TimeSinceStart : 1387.6115906238556\n",
      "Training Loss : 0.18335194885730743\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 102000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 103000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 104000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 105000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 106000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 107000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 108000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 109000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 110000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 110001\n",
      "mean reward (100 episodes) -68.080640\n",
      "best mean reward -68.080640\n",
      "running time 1422.298567\n",
      "Train_EnvstepsSoFar : 110001\n",
      "Train_AverageReturn : -68.08064041256124\n",
      "Train_BestReturn : -68.08064041256124\n",
      "TimeSinceStart : 1422.2985668182373\n",
      "Training Loss : 0.13348962366580963\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 112000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 113000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 114000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 115000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 116000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 117000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 118000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 119000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 120000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 120001\n",
      "mean reward (100 episodes) -53.483540\n",
      "best mean reward -53.483540\n",
      "running time 1455.278786\n",
      "Train_EnvstepsSoFar : 120001\n",
      "Train_AverageReturn : -53.483539810850196\n",
      "Train_BestReturn : -53.483539810850196\n",
      "TimeSinceStart : 1455.2787864208221\n",
      "Training Loss : 0.17925292253494263\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 122000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 123000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 124000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 125000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 126000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 127000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 128000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 129000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 130000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 130001\n",
      "mean reward (100 episodes) -40.821523\n",
      "best mean reward -40.821523\n",
      "running time 1487.440010\n",
      "Train_EnvstepsSoFar : 130001\n",
      "Train_AverageReturn : -40.82152250352418\n",
      "Train_BestReturn : -40.82152250352418\n",
      "TimeSinceStart : 1487.4400098323822\n",
      "Training Loss : 0.5143734812736511\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 132000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 133000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 134000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 135000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 136000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 137000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 138000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 139000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 140000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 140001\n",
      "mean reward (100 episodes) -22.348192\n",
      "best mean reward -22.348192\n",
      "running time 1519.458713\n",
      "Train_EnvstepsSoFar : 140001\n",
      "Train_AverageReturn : -22.34819185608384\n",
      "Train_BestReturn : -22.34819185608384\n",
      "TimeSinceStart : 1519.458713054657\n",
      "Training Loss : 0.08021572977304459\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 142000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 143000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 144000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 145000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 146000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 147000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 148000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 149000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 150000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 150001\n",
      "mean reward (100 episodes) -5.950641\n",
      "best mean reward -5.950641\n",
      "running time 1551.092839\n",
      "Train_EnvstepsSoFar : 150001\n",
      "Train_AverageReturn : -5.950640977140831\n",
      "Train_BestReturn : -5.950640977140831\n",
      "TimeSinceStart : 1551.0928390026093\n",
      "Training Loss : 0.22199535369873047\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 151000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 152000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 153000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 154000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 155000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 156000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 157000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 158000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 159000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 160000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 160001\n",
      "mean reward (100 episodes) 21.702209\n",
      "best mean reward 21.702209\n",
      "running time 1582.591891\n",
      "Train_EnvstepsSoFar : 160001\n",
      "Train_AverageReturn : 21.702209021734927\n",
      "Train_BestReturn : 21.702209021734927\n",
      "TimeSinceStart : 1582.5918910503387\n",
      "Training Loss : 0.13353952765464783\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 161000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 162000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 163000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 164000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 165000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 166000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 167000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 168000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 169000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 170000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 170001\n",
      "mean reward (100 episodes) 53.014364\n",
      "best mean reward 53.014364\n",
      "running time 1693.450588\n",
      "Train_EnvstepsSoFar : 170001\n",
      "Train_AverageReturn : 53.01436433314056\n",
      "Train_BestReturn : 53.01436433314056\n",
      "TimeSinceStart : 1693.4505882263184\n",
      "Training Loss : 0.10042433440685272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 171000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 172000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 173000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 174000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 175000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 176000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 177000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 178000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 179000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 180000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 180001\n",
      "mean reward (100 episodes) 74.200649\n",
      "best mean reward 74.200649\n",
      "running time 1725.365014\n",
      "Train_EnvstepsSoFar : 180001\n",
      "Train_AverageReturn : 74.20064929924553\n",
      "Train_BestReturn : 74.20064929924553\n",
      "TimeSinceStart : 1725.365014076233\n",
      "Training Loss : 2.0442776679992676\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 181000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 182000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 183000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 184000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 185000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 186000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 187000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 188000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 189000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 190000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 190001\n",
      "mean reward (100 episodes) 83.951037\n",
      "best mean reward 83.951037\n",
      "running time 1755.903563\n",
      "Train_EnvstepsSoFar : 190001\n",
      "Train_AverageReturn : 83.95103728347274\n",
      "Train_BestReturn : 83.95103728347274\n",
      "TimeSinceStart : 1755.9035634994507\n",
      "Training Loss : 0.32213422656059265\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 191000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 192000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 193000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 194000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 195000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 196000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 197000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 198000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 199000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 200000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 200001\n",
      "mean reward (100 episodes) 85.897217\n",
      "best mean reward 85.897217\n",
      "running time 1785.758371\n",
      "Train_EnvstepsSoFar : 200001\n",
      "Train_AverageReturn : 85.89721725785796\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 1785.7583706378937\n",
      "Training Loss : 0.1845257431268692\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 201000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 202000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 203000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 204000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 205000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 206000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 207000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 208000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 209000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 210000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 210001\n",
      "mean reward (100 episodes) 75.905312\n",
      "best mean reward 85.897217\n",
      "running time 1815.538539\n",
      "Train_EnvstepsSoFar : 210001\n",
      "Train_AverageReturn : 75.90531209341215\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 1815.5385386943817\n",
      "Training Loss : 0.18882742524147034\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 211000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 212000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 213000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 214000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 215000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 216000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 217000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 218000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 219000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 220000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 220001\n",
      "mean reward (100 episodes) 62.611030\n",
      "best mean reward 85.897217\n",
      "running time 1846.825542\n",
      "Train_EnvstepsSoFar : 220001\n",
      "Train_AverageReturn : 62.611029692822186\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 1846.8255417346954\n",
      "Training Loss : 0.3011416792869568\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 221000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 222000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 223000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 224000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 225000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 226000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 227000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 228000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 229000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 230000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 230001\n",
      "mean reward (100 episodes) 63.246308\n",
      "best mean reward 85.897217\n",
      "running time 1877.799965\n",
      "Train_EnvstepsSoFar : 230001\n",
      "Train_AverageReturn : 63.246308023573214\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 1877.7999649047852\n",
      "Training Loss : 0.9217205047607422\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 231000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 232000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 233000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 234000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 235000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 236000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 237000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 238000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 239000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 240000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 240001\n",
      "mean reward (100 episodes) 58.868704\n",
      "best mean reward 85.897217\n",
      "running time 2032.796405\n",
      "Train_EnvstepsSoFar : 240001\n",
      "Train_AverageReturn : 58.8687043762598\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 2032.7964053153992\n",
      "Training Loss : 2.053210973739624\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 241000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 242000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 243000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 244000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 245000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 246000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 247000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 248000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 249000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 250000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 250001\n",
      "mean reward (100 episodes) 64.160238\n",
      "best mean reward 85.897217\n",
      "running time 2068.485838\n",
      "Train_EnvstepsSoFar : 250001\n",
      "Train_AverageReturn : 64.16023759632871\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 2068.48583817482\n",
      "Training Loss : 0.22656035423278809\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 251000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 252000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 253000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 254000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 255000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 256000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 257000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 258000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 259000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 260000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 260001\n",
      "mean reward (100 episodes) 65.042198\n",
      "best mean reward 85.897217\n",
      "running time 2108.616394\n",
      "Train_EnvstepsSoFar : 260001\n",
      "Train_AverageReturn : 65.04219781201168\n",
      "Train_BestReturn : 85.89721725785796\n",
      "TimeSinceStart : 2108.6163935661316\n",
      "Training Loss : 0.6835476756095886\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 261000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 262000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 263000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 264000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 265000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 266000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 267000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 268000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 269000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 270000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 270001\n",
      "mean reward (100 episodes) 94.731275\n",
      "best mean reward 94.731275\n",
      "running time 2144.452774\n",
      "Train_EnvstepsSoFar : 270001\n",
      "Train_AverageReturn : 94.73127481226376\n",
      "Train_BestReturn : 94.73127481226376\n",
      "TimeSinceStart : 2144.4527740478516\n",
      "Training Loss : 0.29293978214263916\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 271000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 272000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 273000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 274000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 275000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 276000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 277000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 278000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 279000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 280000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 280001\n",
      "mean reward (100 episodes) 94.493017\n",
      "best mean reward 94.731275\n",
      "running time 2181.409757\n",
      "Train_EnvstepsSoFar : 280001\n",
      "Train_AverageReturn : 94.4930172577312\n",
      "Train_BestReturn : 94.73127481226376\n",
      "TimeSinceStart : 2181.4097571372986\n",
      "Training Loss : 0.26970863342285156\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 281000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 282000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 283000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 284000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 285000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 286000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 287000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 288000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 289000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 290000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 290001\n",
      "mean reward (100 episodes) 106.432557\n",
      "best mean reward 106.432557\n",
      "running time 2488.005547\n",
      "Train_EnvstepsSoFar : 290001\n",
      "Train_AverageReturn : 106.432557222053\n",
      "Train_BestReturn : 106.432557222053\n",
      "TimeSinceStart : 2488.005546808243\n",
      "Training Loss : 1.0263822078704834\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 291000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 292000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 293000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 294000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 295000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 296000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 297000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 298000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 299000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 300000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 300001\n",
      "mean reward (100 episodes) 119.759801\n",
      "best mean reward 119.759801\n",
      "running time 2515.643373\n",
      "Train_EnvstepsSoFar : 300001\n",
      "Train_AverageReturn : 119.75980090744328\n",
      "Train_BestReturn : 119.75980090744328\n",
      "TimeSinceStart : 2515.643372774124\n",
      "Training Loss : 0.18642675876617432\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 301000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 302000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 303000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 304000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 305000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 306000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 307000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 308000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 309000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 310000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 310001\n",
      "mean reward (100 episodes) 127.402083\n",
      "best mean reward 127.402083\n",
      "running time 2543.492694\n",
      "Train_EnvstepsSoFar : 310001\n",
      "Train_AverageReturn : 127.402082908237\n",
      "Train_BestReturn : 127.402082908237\n",
      "TimeSinceStart : 2543.4926941394806\n",
      "Training Loss : 0.9001518487930298\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 311000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 312000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 313000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 314000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 315000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 316000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 317000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 318000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 319000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 320000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 320001\n",
      "mean reward (100 episodes) 134.377130\n",
      "best mean reward 134.377130\n",
      "running time 2569.979649\n",
      "Train_EnvstepsSoFar : 320001\n",
      "Train_AverageReturn : 134.37712989633667\n",
      "Train_BestReturn : 134.37712989633667\n",
      "TimeSinceStart : 2569.9796488285065\n",
      "Training Loss : 0.6673612594604492\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 321000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 322000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 323000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 324000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 325000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 326000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 327000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 328000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 329000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 330000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 330001\n",
      "mean reward (100 episodes) 141.717479\n",
      "best mean reward 141.717479\n",
      "running time 2596.351704\n",
      "Train_EnvstepsSoFar : 330001\n",
      "Train_AverageReturn : 141.71747879298576\n",
      "Train_BestReturn : 141.71747879298576\n",
      "TimeSinceStart : 2596.351704120636\n",
      "Training Loss : 0.4158356785774231\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 331000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 332000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 333000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 334000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 335000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 336000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 337000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 338000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 339000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 340000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 340001\n",
      "mean reward (100 episodes) 124.177921\n",
      "best mean reward 141.717479\n",
      "running time 2727.570524\n",
      "Train_EnvstepsSoFar : 340001\n",
      "Train_AverageReturn : 124.17792107760708\n",
      "Train_BestReturn : 141.71747879298576\n",
      "TimeSinceStart : 2727.570523738861\n",
      "Training Loss : 2.0468175411224365\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 341000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 342000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 343000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 344000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 345000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 346000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 347000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 348000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 349000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 350000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 350001\n",
      "mean reward (100 episodes) 137.495259\n",
      "best mean reward 141.717479\n",
      "running time 2754.566444\n",
      "Train_EnvstepsSoFar : 350001\n",
      "Train_AverageReturn : 137.49525912176395\n",
      "Train_BestReturn : 141.71747879298576\n",
      "TimeSinceStart : 2754.566444158554\n",
      "Training Loss : 2.5057387351989746\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 351000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 352000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 353000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 354000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 355000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 356000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 357000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 358000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 359000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 360000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 360001\n",
      "mean reward (100 episodes) 157.355023\n",
      "best mean reward 157.355023\n",
      "running time 2781.301607\n",
      "Train_EnvstepsSoFar : 360001\n",
      "Train_AverageReturn : 157.3550226863579\n",
      "Train_BestReturn : 157.3550226863579\n",
      "TimeSinceStart : 2781.3016073703766\n",
      "Training Loss : 2.807241916656494\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 361000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 362000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 363000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 364000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 365000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 366000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 367000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 368000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 369000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 370000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 370001\n",
      "mean reward (100 episodes) 167.092453\n",
      "best mean reward 167.092453\n",
      "running time 2807.695738\n",
      "Train_EnvstepsSoFar : 370001\n",
      "Train_AverageReturn : 167.09245346360353\n",
      "Train_BestReturn : 167.09245346360353\n",
      "TimeSinceStart : 2807.6957383155823\n",
      "Training Loss : 0.2920997440814972\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 371000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 372000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 373000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 374000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 375000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 376000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 377000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 378000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 379000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 380000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 380001\n",
      "mean reward (100 episodes) 186.842606\n",
      "best mean reward 186.842606\n",
      "running time 2834.301226\n",
      "Train_EnvstepsSoFar : 380001\n",
      "Train_AverageReturn : 186.84260585394892\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 2834.301226377487\n",
      "Training Loss : 0.5793144106864929\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 381000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 382000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 383000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 384000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 385000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 386000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 387000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 388000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 389000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 390000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 390001\n",
      "mean reward (100 episodes) 171.264081\n",
      "best mean reward 186.842606\n",
      "running time 2861.437357\n",
      "Train_EnvstepsSoFar : 390001\n",
      "Train_AverageReturn : 171.26408119453015\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 2861.4373569488525\n",
      "Training Loss : 0.1929548978805542\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 391000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 392000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 393000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 394000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 395000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 396000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 397000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 398000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 399000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 400000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 400001\n",
      "mean reward (100 episodes) 158.918942\n",
      "best mean reward 186.842606\n",
      "running time 2889.517894\n",
      "Train_EnvstepsSoFar : 400001\n",
      "Train_AverageReturn : 158.9189419170717\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 2889.5178940296173\n",
      "Training Loss : 0.1900177299976349\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 401000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 402000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 403000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 404000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 405000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 406000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 407000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 408000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 409000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 410000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 410001\n",
      "mean reward (100 episodes) 127.000959\n",
      "best mean reward 186.842606\n",
      "running time 2918.627990\n",
      "Train_EnvstepsSoFar : 410001\n",
      "Train_AverageReturn : 127.0009585390364\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 2918.6279895305634\n",
      "Training Loss : 0.30675238370895386\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 411000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 412000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 413000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 414000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 415000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 416000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 417000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 418000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 419000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 420000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 420001\n",
      "mean reward (100 episodes) 97.780286\n",
      "best mean reward 186.842606\n",
      "running time 2951.544966\n",
      "Train_EnvstepsSoFar : 420001\n",
      "Train_AverageReturn : 97.78028612136127\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 2951.5449657440186\n",
      "Training Loss : 1.715850591659546\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 421000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 422000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 423000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 424000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 425000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 426000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 427000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 428000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 429000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 430000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 430001\n",
      "mean reward (100 episodes) 103.780310\n",
      "best mean reward 186.842606\n",
      "running time 2976.813174\n",
      "Train_EnvstepsSoFar : 430001\n",
      "Train_AverageReturn : 103.78031042978203\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 2976.8131737709045\n",
      "Training Loss : 0.16802701354026794\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 431000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 432000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 433000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 434000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 435000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 436000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 437000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 438000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 439000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 440000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 440001\n",
      "mean reward (100 episodes) 108.983636\n",
      "best mean reward 186.842606\n",
      "running time 3002.212906\n",
      "Train_EnvstepsSoFar : 440001\n",
      "Train_AverageReturn : 108.98363639029485\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 3002.2129061222076\n",
      "Training Loss : 0.7170608043670654\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 441000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 442000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 443000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 444000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 445000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 446000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 447000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 448000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 449000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 450000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 450001\n",
      "mean reward (100 episodes) 82.267702\n",
      "best mean reward 186.842606\n",
      "running time 3027.440703\n",
      "Train_EnvstepsSoFar : 450001\n",
      "Train_AverageReturn : 82.26770242623589\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 3027.4407029151917\n",
      "Training Loss : 0.37352409958839417\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 451000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 452000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 453000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 454000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 455000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 456000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 457000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 458000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 459000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 460000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 460001\n",
      "mean reward (100 episodes) 107.996512\n",
      "best mean reward 186.842606\n",
      "running time 3052.670655\n",
      "Train_EnvstepsSoFar : 460001\n",
      "Train_AverageReturn : 107.99651239549016\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 3052.6706550121307\n",
      "Training Loss : 0.275119423866272\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 461000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 462000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 463000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 464000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 465000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 466000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 467000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 468000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 469000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 470000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 470001\n",
      "mean reward (100 episodes) 112.539511\n",
      "best mean reward 186.842606\n",
      "running time 3077.775206\n",
      "Train_EnvstepsSoFar : 470001\n",
      "Train_AverageReturn : 112.5395111487559\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 3077.7752063274384\n",
      "Training Loss : 0.11840971559286118\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 471000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 472000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 473000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 474000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 475000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 476000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 477000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 478000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 479000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 480000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 480001\n",
      "mean reward (100 episodes) 121.267954\n",
      "best mean reward 186.842606\n",
      "running time 3102.794077\n",
      "Train_EnvstepsSoFar : 480001\n",
      "Train_AverageReturn : 121.26795447502569\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 3102.794077396393\n",
      "Training Loss : 0.572269856929779\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 481000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 482000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 483000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 484000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 485000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 486000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 487000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 488000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 489000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 490000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "Timestep 490001\n",
      "mean reward (100 episodes) 91.577032\n",
      "best mean reward 186.842606\n",
      "running time 3209.925024\n",
      "Train_EnvstepsSoFar : 490001\n",
      "Train_AverageReturn : 91.57703176423443\n",
      "Train_BestReturn : 186.84260585394892\n",
      "TimeSinceStart : 3209.925023794174\n",
      "Training Loss : 0.16530223190784454\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 491000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 492000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 493000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 494000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 495000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 496000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 497000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 498000 ************\n",
      "\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 499000 ************\n",
      "\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_dqn.py --env_name LunarLander-v3 --exp_name q3_hparam3 --learning_starts 10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_ac_1_1_CartPole-v0_16-10-2020_21-28-18 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_ac_1_1_CartPole-v0_16-10-2020_21-28-18\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 25.235294342041016\n",
      "Eval_StdReturn : 11.694792747497559\n",
      "Eval_MaxReturn : 54.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 25.235294117647058\n",
      "Train_AverageReturn : 27.648649215698242\n",
      "Train_StdReturn : 12.643160820007324\n",
      "Train_MaxReturn : 65.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 27.64864864864865\n",
      "Train_EnvstepsSoFar : 1023\n",
      "TimeSinceStart : 0.7134275436401367\n",
      "Critic_Loss : 1.0069358348846436\n",
      "Actor_Loss : -6.161796569824219\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1018 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1014 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1016 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1013 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1012 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 10.149999618530273\n",
      "Eval_StdReturn : 1.314344048500061\n",
      "Eval_MaxReturn : 13.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 10.15\n",
      "Train_AverageReturn : 10.860215187072754\n",
      "Train_StdReturn : 1.4853506088256836\n",
      "Train_MaxReturn : 15.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 10.86021505376344\n",
      "Train_EnvstepsSoFar : 11135\n",
      "TimeSinceStart : 6.286706447601318\n",
      "Critic_Loss : 2.134030342102051\n",
      "Actor_Loss : -91.19773864746094\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 12.176470756530762\n",
      "Eval_StdReturn : 1.5429267883300781\n",
      "Eval_MaxReturn : 15.0\n",
      "Eval_MinReturn : 9.0\n",
      "Eval_AverageEpLen : 12.176470588235293\n",
      "Train_AverageReturn : 11.505746841430664\n",
      "Train_StdReturn : 1.5303335189819336\n",
      "Train_MaxReturn : 15.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 11.505747126436782\n",
      "Train_EnvstepsSoFar : 21165\n",
      "TimeSinceStart : 12.026768445968628\n",
      "Critic_Loss : 6.353356838226318\n",
      "Actor_Loss : -53.58641052246094\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 11.882352828979492\n",
      "Eval_StdReturn : 1.6407442092895508\n",
      "Eval_MaxReturn : 15.0\n",
      "Eval_MinReturn : 9.0\n",
      "Eval_AverageEpLen : 11.882352941176471\n",
      "Train_AverageReturn : 12.0119047164917\n",
      "Train_StdReturn : 1.6293004751205444\n",
      "Train_MaxReturn : 16.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 12.011904761904763\n",
      "Train_EnvstepsSoFar : 31214\n",
      "TimeSinceStart : 17.587049961090088\n",
      "Critic_Loss : 10.683615684509277\n",
      "Actor_Loss : -38.78687286376953\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 11.428571701049805\n",
      "Eval_StdReturn : 1.3582701683044434\n",
      "Eval_MaxReturn : 15.0\n",
      "Eval_MinReturn : 9.0\n",
      "Eval_AverageEpLen : 11.428571428571429\n",
      "Train_AverageReturn : 11.420454978942871\n",
      "Train_StdReturn : 1.2944074869155884\n",
      "Train_MaxReturn : 14.0\n",
      "Train_MinReturn : 9.0\n",
      "Train_AverageEpLen : 11.420454545454545\n",
      "Train_EnvstepsSoFar : 41278\n",
      "TimeSinceStart : 23.42927312850952\n",
      "Critic_Loss : 13.45235824584961\n",
      "Actor_Loss : -62.262237548828125\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 9.780488014221191\n",
      "Eval_StdReturn : 0.9242583513259888\n",
      "Eval_MaxReturn : 12.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 9.78048780487805\n",
      "Train_AverageReturn : 10.416666984558105\n",
      "Train_StdReturn : 1.3202482461929321\n",
      "Train_MaxReturn : 13.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 10.416666666666666\n",
      "Train_EnvstepsSoFar : 51339\n",
      "TimeSinceStart : 29.096556425094604\n",
      "Critic_Loss : 12.971205711364746\n",
      "Actor_Loss : -76.58837127685547\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 9.250\n",
      "Eval_StdReturn : 0.7111130952835083\n",
      "Eval_MaxReturn : 10.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 9.25\n",
      "Train_AverageReturn : 9.324073791503906\n",
      "Train_StdReturn : 0.7048601508140564\n",
      "Train_MaxReturn : 11.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 9.324074074074074\n",
      "Train_EnvstepsSoFar : 61389\n",
      "TimeSinceStart : 34.7247633934021\n",
      "Critic_Loss : 10.116832733154297\n",
      "Actor_Loss : -15.04106330871582\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 9.418604850769043\n",
      "Eval_StdReturn : 0.6898789405822754\n",
      "Eval_MaxReturn : 11.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 9.418604651162791\n",
      "Train_AverageReturn : 9.44339656829834\n",
      "Train_StdReturn : 0.7407312393188477\n",
      "Train_MaxReturn : 11.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 9.443396226415095\n",
      "Train_EnvstepsSoFar : 71426\n",
      "TimeSinceStart : 40.28857731819153\n",
      "Critic_Loss : 5.112473011016846\n",
      "Actor_Loss : -0.15873289108276367\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 9.511628150939941\n",
      "Eval_StdReturn : 0.7886430025100708\n",
      "Eval_MaxReturn : 11.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 9.511627906976743\n",
      "Train_AverageReturn : 9.44339656829834\n",
      "Train_StdReturn : 0.6740497946739197\n",
      "Train_MaxReturn : 11.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 9.443396226415095\n",
      "Train_EnvstepsSoFar : 81479\n",
      "TimeSinceStart : 45.94551610946655\n",
      "Critic_Loss : 0.766416072845459\n",
      "Actor_Loss : 0.08089570701122284\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1007 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1002 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 9.595237731933594\n",
      "Eval_StdReturn : 0.5797997713088989\n",
      "Eval_MaxReturn : 11.0\n",
      "Eval_MinReturn : 8.0\n",
      "Eval_AverageEpLen : 9.595238095238095\n",
      "Train_AverageReturn : 9.373831748962402\n",
      "Train_StdReturn : 0.7796880006790161\n",
      "Train_MaxReturn : 11.0\n",
      "Train_MinReturn : 8.0\n",
      "Train_AverageEpLen : 9.373831775700934\n",
      "Train_EnvstepsSoFar : 91509\n",
      "TimeSinceStart : 51.572110176086426\n",
      "Critic_Loss : 1.069104790687561\n",
      "Actor_Loss : 4.355146408081055\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_ac_1_1 -ntu 1 -ngsptu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_100_1_CartPole-v0_16-10-2020_21-29-17 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_100_1_CartPole-v0_16-10-2020_21-29-17\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 29.14285659790039\n",
      "Eval_StdReturn : 21.346988677978516\n",
      "Eval_MaxReturn : 103.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 29.142857142857142\n",
      "Train_AverageReturn : 27.648649215698242\n",
      "Train_StdReturn : 12.643160820007324\n",
      "Train_MaxReturn : 65.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 27.64864864864865\n",
      "Train_EnvstepsSoFar : 1023\n",
      "TimeSinceStart : 0.795743465423584\n",
      "Critic_Loss : 10.573396682739258\n",
      "Actor_Loss : -23.988265991210938\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1050 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1020 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1021 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1122 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1032 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1018 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1038 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 97.4000015258789\n",
      "Eval_StdReturn : 31.27043342590332\n",
      "Eval_MaxReturn : 154.0\n",
      "Eval_MinReturn : 59.0\n",
      "Eval_AverageEpLen : 97.4\n",
      "Train_AverageReturn : 64.875\n",
      "Train_StdReturn : 30.63673210144043\n",
      "Train_MaxReturn : 154.0\n",
      "Train_MinReturn : 27.0\n",
      "Train_AverageEpLen : 64.875\n",
      "Train_EnvstepsSoFar : 11337\n",
      "TimeSinceStart : 7.2448554039001465\n",
      "Critic_Loss : 7.733628749847412\n",
      "Actor_Loss : -15.398021697998047\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1041 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1124 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1069 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1131 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1039 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1021 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 107.0\n",
      "Eval_StdReturn : 38.96152114868164\n",
      "Eval_MaxReturn : 147.0\n",
      "Eval_MinReturn : 57.0\n",
      "Eval_AverageEpLen : 107.0\n",
      "Train_AverageReturn : 112.0\n",
      "Train_StdReturn : 33.33333206176758\n",
      "Train_MaxReturn : 158.0\n",
      "Train_MinReturn : 57.0\n",
      "Train_AverageEpLen : 112.0\n",
      "Train_EnvstepsSoFar : 21800\n",
      "TimeSinceStart : 14.92664361000061\n",
      "Critic_Loss : 11.241166114807129\n",
      "Actor_Loss : 34.25299072265625\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1020 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1145 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1050 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1038 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1036 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1024 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1057 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1040 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1054 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 49.77777862548828\n",
      "Eval_StdReturn : 5.072316646575928\n",
      "Eval_MaxReturn : 59.0\n",
      "Eval_MinReturn : 44.0\n",
      "Eval_AverageEpLen : 49.77777777777778\n",
      "Train_AverageReturn : 55.47368240356445\n",
      "Train_StdReturn : 11.856014251708984\n",
      "Train_MaxReturn : 80.0\n",
      "Train_MinReturn : 36.0\n",
      "Train_AverageEpLen : 55.473684210526315\n",
      "Train_EnvstepsSoFar : 32270\n",
      "TimeSinceStart : 21.437668323516846\n",
      "Critic_Loss : 42.93931579589844\n",
      "Actor_Loss : 20.930713653564453\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1021 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1028 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1018 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1041 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1040 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1017 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1033 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 61.71428680419922\n",
      "Eval_StdReturn : 14.635713577270508\n",
      "Eval_MaxReturn : 78.0\n",
      "Eval_MinReturn : 35.0\n",
      "Eval_AverageEpLen : 61.714285714285715\n",
      "Train_AverageReturn : 51.650001525878906\n",
      "Train_StdReturn : 13.730531692504883\n",
      "Train_MaxReturn : 72.0\n",
      "Train_MinReturn : 29.0\n",
      "Train_AverageEpLen : 51.65\n",
      "Train_EnvstepsSoFar : 42504\n",
      "TimeSinceStart : 27.84802794456482\n",
      "Critic_Loss : 38.99164962768555\n",
      "Actor_Loss : 11.424552917480469\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1017 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1045 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1045 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1032 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1026 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1020 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1060 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1028 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 64.28571319580078\n",
      "Eval_StdReturn : 15.763559341430664\n",
      "Eval_MaxReturn : 90.0\n",
      "Eval_MinReturn : 43.0\n",
      "Eval_AverageEpLen : 64.28571428571429\n",
      "Train_AverageReturn : 57.11111068725586\n",
      "Train_StdReturn : 10.826351165771484\n",
      "Train_MaxReturn : 77.0\n",
      "Train_MinReturn : 43.0\n",
      "Train_AverageEpLen : 57.111111111111114\n",
      "Train_EnvstepsSoFar : 52803\n",
      "TimeSinceStart : 34.29528832435608\n",
      "Critic_Loss : 53.851966857910156\n",
      "Actor_Loss : 24.92835235595703\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1053 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1038 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1049 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1050 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1030 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1033 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1055 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 74.83333587646484\n",
      "Eval_StdReturn : 25.43892478942871\n",
      "Eval_MaxReturn : 115.0\n",
      "Eval_MinReturn : 53.0\n",
      "Eval_AverageEpLen : 74.83333333333333\n",
      "Train_AverageReturn : 70.33333587646484\n",
      "Train_StdReturn : 13.479202270507812\n",
      "Train_MaxReturn : 99.0\n",
      "Train_MinReturn : 45.0\n",
      "Train_AverageEpLen : 70.33333333333333\n",
      "Train_EnvstepsSoFar : 63131\n",
      "TimeSinceStart : 40.770835161209106\n",
      "Critic_Loss : 36.30858612060547\n",
      "Actor_Loss : 32.22599792480469\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1061 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1021 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1060 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1101 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1058 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1083 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1052 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1095 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1073 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1038 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 81.19999694824219\n",
      "Eval_StdReturn : 19.322526931762695\n",
      "Eval_MaxReturn : 108.0\n",
      "Eval_MinReturn : 62.0\n",
      "Eval_AverageEpLen : 81.2\n",
      "Train_AverageReturn : 115.33333587646484\n",
      "Train_StdReturn : 57.34108352661133\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 53.0\n",
      "Train_AverageEpLen : 115.33333333333333\n",
      "Train_EnvstepsSoFar : 73773\n",
      "TimeSinceStart : 47.31920576095581\n",
      "Critic_Loss : 23.909427642822266\n",
      "Actor_Loss : 89.77190399169922\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1056 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1088 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1078 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1074 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1029 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1120 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1041 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 156.3333282470703\n",
      "Eval_StdReturn : 32.427696228027344\n",
      "Eval_MaxReturn : 185.0\n",
      "Eval_MinReturn : 111.0\n",
      "Eval_AverageEpLen : 156.33333333333334\n",
      "Train_AverageReturn : 148.7142791748047\n",
      "Train_StdReturn : 34.27916717529297\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 92.0\n",
      "Train_AverageEpLen : 148.71428571428572\n",
      "Train_EnvstepsSoFar : 84274\n",
      "TimeSinceStart : 53.85424590110779\n",
      "Critic_Loss : 18.84291648864746\n",
      "Actor_Loss : -14.148578643798828\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1094 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1120 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1094 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1066 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1004 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1052 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1012 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1053 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1172 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1056 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 122.25\n",
      "Eval_StdReturn : 22.77471160888672\n",
      "Eval_MaxReturn : 158.0\n",
      "Eval_MinReturn : 95.0\n",
      "Eval_AverageEpLen : 122.25\n",
      "Train_AverageReturn : 176.0\n",
      "Train_StdReturn : 27.09858512878418\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 137.0\n",
      "Train_AverageEpLen : 176.0\n",
      "Train_EnvstepsSoFar : 94997\n",
      "TimeSinceStart : 60.504230260849\n",
      "Critic_Loss : 42.77556610107422\n",
      "Actor_Loss : 1.7343883514404297\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1110 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1071 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1045 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1197 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1195 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1055 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1129 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1099 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1024 / 1000\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_100_1 -ntu 100 -ngsptu 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_1_100_CartPole-v0_16-10-2020_21-30-26 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_1_100_CartPole-v0_16-10-2020_21-30-26\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 22.63157844543457\n",
      "Eval_StdReturn : 13.955608367919922\n",
      "Eval_MaxReturn : 67.0\n",
      "Eval_MinReturn : 9.0\n",
      "Eval_AverageEpLen : 22.63157894736842\n",
      "Train_AverageReturn : 27.648649215698242\n",
      "Train_StdReturn : 12.643160820007324\n",
      "Train_MaxReturn : 65.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 27.64864864864865\n",
      "Train_EnvstepsSoFar : 1023\n",
      "TimeSinceStart : 0.7826838493347168\n",
      "Critic_Loss : 0.0019927702378481627\n",
      "Actor_Loss : -1.41624116897583\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1008 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1001 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1017 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1062 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1045 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1036 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 50.55555725097656\n",
      "Eval_StdReturn : 15.391997337341309\n",
      "Eval_MaxReturn : 72.0\n",
      "Eval_MinReturn : 25.0\n",
      "Eval_AverageEpLen : 50.55555555555556\n",
      "Train_AverageReturn : 60.17647171020508\n",
      "Train_StdReturn : 21.530376434326172\n",
      "Train_MaxReturn : 124.0\n",
      "Train_MinReturn : 26.0\n",
      "Train_AverageEpLen : 60.1764705882353\n",
      "Train_EnvstepsSoFar : 11272\n",
      "TimeSinceStart : 6.884557008743286\n",
      "Critic_Loss : 0.42361363768577576\n",
      "Actor_Loss : -33.42341232299805\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1031 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1005 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1027 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1046 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1024 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1155 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1009 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1091 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1016 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1093 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 156.14285278320312\n",
      "Train_StdReturn : 9.891244888305664\n",
      "Train_MaxReturn : 170.0\n",
      "Train_MinReturn : 143.0\n",
      "Train_AverageEpLen : 156.14285714285714\n",
      "Train_EnvstepsSoFar : 21769\n",
      "TimeSinceStart : 13.15064787864685\n",
      "Critic_Loss : 0.08910669386386871\n",
      "Actor_Loss : -88.70091247558594\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1093 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1104 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1106 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1010 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1130 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1025 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1103 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 140.3333282470703\n",
      "Eval_StdReturn : 5.734883785247803\n",
      "Eval_MaxReturn : 147.0\n",
      "Eval_MinReturn : 133.0\n",
      "Eval_AverageEpLen : 140.33333333333334\n",
      "Train_AverageReturn : 137.875\n",
      "Train_StdReturn : 7.389815807342529\n",
      "Train_MaxReturn : 152.0\n",
      "Train_MinReturn : 128.0\n",
      "Train_AverageEpLen : 137.875\n",
      "Train_EnvstepsSoFar : 32340\n",
      "TimeSinceStart : 19.406886100769043\n",
      "Critic_Loss : 0.03760455548763275\n",
      "Actor_Loss : -64.49830627441406\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1006 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1120 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1104 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1047 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1148 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1045 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1124 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1118 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1163 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1119 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 186.5\n",
      "Train_StdReturn : 13.997023582458496\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 160.0\n",
      "Train_AverageEpLen : 186.5\n",
      "Train_EnvstepsSoFar : 43334\n",
      "TimeSinceStart : 25.93847107887268\n",
      "Critic_Loss : 0.14228364825248718\n",
      "Actor_Loss : -7.705326080322266\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1155 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1157 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 53646\n",
      "TimeSinceStart : 32.05450391769409\n",
      "Critic_Loss : 2.4303269386291504\n",
      "Actor_Loss : -7.782432556152344\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1188 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1191 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1143 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1108 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1111 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1153 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1153 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1185 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1187 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 65065\n",
      "TimeSinceStart : 38.738412857055664\n",
      "Critic_Loss : 2.8018031120300293\n",
      "Actor_Loss : -4.059541702270508\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 75065\n",
      "TimeSinceStart : 44.694212675094604\n",
      "Critic_Loss : 16.617115020751953\n",
      "Actor_Loss : -27.117107391357422\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 85065\n",
      "TimeSinceStart : 50.72468066215515\n",
      "Critic_Loss : 17.8466796875\n",
      "Actor_Loss : -15.395957946777344\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 95065\n",
      "TimeSinceStart : 56.76518511772156\n",
      "Critic_Loss : 27.623231887817383\n",
      "Actor_Loss : 0.7755880355834961\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_1_100 -ntu 1 -ngsptu 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_10_10_CartPole-v0_16-10-2020_21-31-30 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q4_10_10_CartPole-v0_16-10-2020_21-31-30\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1023 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 40.50\n",
      "Eval_StdReturn : 44.75768280029297\n",
      "Eval_MaxReturn : 160.0\n",
      "Eval_MinReturn : 12.0\n",
      "Eval_AverageEpLen : 40.5\n",
      "Train_AverageReturn : 27.648649215698242\n",
      "Train_StdReturn : 12.643160820007324\n",
      "Train_MaxReturn : 65.0\n",
      "Train_MinReturn : 11.0\n",
      "Train_AverageEpLen : 27.64864864864865\n",
      "Train_EnvstepsSoFar : 1023\n",
      "TimeSinceStart : 0.7830064296722412\n",
      "Critic_Loss : 2.1959376335144043\n",
      "Actor_Loss : -29.275222778320312\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1027 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1031 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1015 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1003 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1021 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1099 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1011 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1097 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1042 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1038 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 158.0\n",
      "Eval_StdReturn : 22.553640365600586\n",
      "Eval_MaxReturn : 180.0\n",
      "Eval_MinReturn : 127.0\n",
      "Eval_AverageEpLen : 158.0\n",
      "Train_AverageReturn : 103.80000305175781\n",
      "Train_StdReturn : 47.830535888671875\n",
      "Train_MaxReturn : 191.0\n",
      "Train_MinReturn : 37.0\n",
      "Train_AverageEpLen : 103.8\n",
      "Train_EnvstepsSoFar : 11407\n",
      "TimeSinceStart : 7.053950548171997\n",
      "Critic_Loss : 3.9593350887298584\n",
      "Actor_Loss : -40.51837158203125\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1087 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1028 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1030 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1093 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1058 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1084 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1049 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1021 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1074 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 131.25\n",
      "Eval_StdReturn : 6.219927787780762\n",
      "Eval_MaxReturn : 140.0\n",
      "Eval_MinReturn : 124.0\n",
      "Eval_AverageEpLen : 131.25\n",
      "Train_AverageReturn : 107.4000015258789\n",
      "Train_StdReturn : 29.017925262451172\n",
      "Train_MaxReturn : 165.0\n",
      "Train_MinReturn : 47.0\n",
      "Train_AverageEpLen : 107.4\n",
      "Train_EnvstepsSoFar : 21931\n",
      "TimeSinceStart : 13.465354681015015\n",
      "Critic_Loss : 3.7847185134887695\n",
      "Actor_Loss : -59.97798156738281\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1042 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1111 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1183 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1162 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1163 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1100 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1075 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1089 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1054 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1059 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 97.80000305175781\n",
      "Eval_StdReturn : 8.231645584106445\n",
      "Eval_MaxReturn : 109.0\n",
      "Eval_MinReturn : 89.0\n",
      "Eval_AverageEpLen : 97.8\n",
      "Train_AverageReturn : 88.25\n",
      "Train_StdReturn : 26.839414596557617\n",
      "Train_MaxReturn : 116.0\n",
      "Train_MinReturn : 30.0\n",
      "Train_AverageEpLen : 88.25\n",
      "Train_EnvstepsSoFar : 32969\n",
      "TimeSinceStart : 20.00579023361206\n",
      "Critic_Loss : 6.224092483520508\n",
      "Actor_Loss : -24.613800048828125\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1030 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1030 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1012 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1030 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1072 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1019 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1136 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1091 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1033 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1143 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 190.5\n",
      "Train_StdReturn : 5.649483680725098\n",
      "Train_MaxReturn : 197.0\n",
      "Train_MinReturn : 183.0\n",
      "Train_AverageEpLen : 190.5\n",
      "Train_EnvstepsSoFar : 43565\n",
      "TimeSinceStart : 26.341508626937866\n",
      "Critic_Loss : 0.3469945788383484\n",
      "Actor_Loss : -19.166118621826172\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1198 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 53763\n",
      "TimeSinceStart : 32.47583484649658\n",
      "Critic_Loss : 126.85792541503906\n",
      "Actor_Loss : 6.916964054107666\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 63763\n",
      "TimeSinceStart : 38.5458197593689\n",
      "Critic_Loss : 172.69065856933594\n",
      "Actor_Loss : -1.2216339111328125\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 73763\n",
      "TimeSinceStart : 44.598647356033325\n",
      "Critic_Loss : 15.421321868896484\n",
      "Actor_Loss : 14.456255912780762\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 200.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 200.0\n",
      "Train_AverageEpLen : 200.0\n",
      "Train_EnvstepsSoFar : 83763\n",
      "TimeSinceStart : 50.63336396217346\n",
      "Critic_Loss : 35.672855377197266\n",
      "Actor_Loss : 5.1302714347839355\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1194 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1181 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1133 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1185 / 1000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 200.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 200.0\n",
      "Eval_MinReturn : 200.0\n",
      "Eval_AverageEpLen : 200.0\n",
      "Train_AverageReturn : 197.5\n",
      "Train_StdReturn : 5.155902862548828\n",
      "Train_MaxReturn : 200.0\n",
      "Train_MinReturn : 186.0\n",
      "Train_AverageEpLen : 197.5\n",
      "Train_EnvstepsSoFar : 94456\n",
      "TimeSinceStart : 57.07298731803894\n",
      "Critic_Loss : 115.24964904785156\n",
      "Actor_Loss : 7.454844951629639\n",
      "Initial_DataCollection_AverageReturn : 27.648649215698242\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1149 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1146 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1000 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1189 / 1000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     1148 / 1000\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_actor_critic.py --env_name CartPole-v0 -n 100 -b 1000 --exp_name q4_10_10 -ntu 10 -ngsptu 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q5_10_10_InvertedPendulum-v2_17-10-2020_13-12-38 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q5_10_10_InvertedPendulum-v2_17-10-2020_13-12-38\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5004 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 15.65384578704834\n",
      "Eval_StdReturn : 12.763958930969238\n",
      "Eval_MaxReturn : 55.0\n",
      "Eval_MinReturn : 6.0\n",
      "Eval_AverageEpLen : 15.653846153846153\n",
      "Train_AverageReturn : 8.510204315185547\n",
      "Train_StdReturn : 5.211339473724365\n",
      "Train_MaxReturn : 40.0\n",
      "Train_MinReturn : 3.0\n",
      "Train_AverageEpLen : 8.510204081632653\n",
      "Train_EnvstepsSoFar : 5004\n",
      "TimeSinceStart : 3.1884918212890625\n",
      "Critic_Loss : 1.0860847234725952\n",
      "Actor_Loss : -726.0419921875\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5030 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5023 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5005 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5031 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5013 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5024 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5025 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5029 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5066 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5108 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 112.75\n",
      "Eval_StdReturn : 37.09026336669922\n",
      "Eval_MaxReturn : 168.0\n",
      "Eval_MinReturn : 72.0\n",
      "Eval_AverageEpLen : 112.75\n",
      "Train_AverageReturn : 94.59259033203125\n",
      "Train_StdReturn : 45.79160690307617\n",
      "Train_MaxReturn : 206.0\n",
      "Train_MinReturn : 21.0\n",
      "Train_AverageEpLen : 94.5925925925926\n",
      "Train_EnvstepsSoFar : 55358\n",
      "TimeSinceStart : 34.16246795654297\n",
      "Critic_Loss : 0.7947757840156555\n",
      "Actor_Loss : -567.6353759765625\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5008 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5106 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5040 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5048 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5065 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5023 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5100 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5041 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5284 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5291 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 199.0\n",
      "Eval_StdReturn : 20.461345672607422\n",
      "Eval_MaxReturn : 223.0\n",
      "Eval_MinReturn : 173.0\n",
      "Eval_AverageEpLen : 199.0\n",
      "Train_AverageReturn : 220.4583282470703\n",
      "Train_StdReturn : 97.05238342285156\n",
      "Train_MaxReturn : 376.0\n",
      "Train_MinReturn : 53.0\n",
      "Train_AverageEpLen : 220.45833333333334\n",
      "Train_EnvstepsSoFar : 106364\n",
      "TimeSinceStart : 66.39308166503906\n",
      "Critic_Loss : 0.33917343616485596\n",
      "Actor_Loss : -695.2572021484375\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5224 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5007 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5150 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5016 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5290 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5825 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5653 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5572 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 159101\n",
      "TimeSinceStart : 95.44132232666016\n",
      "Critic_Loss : 0.34111058712005615\n",
      "Actor_Loss : -7.030601501464844\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 209101\n",
      "TimeSinceStart : 123.07045269012451\n",
      "Critic_Loss : 0.34855854511260986\n",
      "Actor_Loss : 61.66675567626953\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5879 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5058 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5088 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 260126\n",
      "TimeSinceStart : 151.25287318229675\n",
      "Critic_Loss : 0.3469879925251007\n",
      "Actor_Loss : 61.814693450927734\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5009 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5047 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5017 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 203.8000030517578\n",
      "Eval_StdReturn : 398.10321044921875\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 2.0\n",
      "Eval_AverageEpLen : 203.8\n",
      "Train_AverageReturn : 627.125\n",
      "Train_StdReturn : 481.3801574707031\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 4.0\n",
      "Train_AverageEpLen : 627.125\n",
      "Train_EnvstepsSoFar : 310199\n",
      "TimeSinceStart : 178.7179205417633\n",
      "Critic_Loss : 0.40724727511405945\n",
      "Actor_Loss : -19.539566040039062\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5032 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5012 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5107 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5584 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5144 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5186 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5522 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5179 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5133 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 270.15789794921875\n",
      "Train_StdReturn : 226.27027893066406\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 43.0\n",
      "Train_AverageEpLen : 270.1578947368421\n",
      "Train_EnvstepsSoFar : 362098\n",
      "TimeSinceStart : 206.72155833244324\n",
      "Critic_Loss : 0.6568212509155273\n",
      "Actor_Loss : -445.37158203125\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5724 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5099 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5188 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 428.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 428.0\n",
      "Eval_MinReturn : 428.0\n",
      "Eval_AverageEpLen : 428.0\n",
      "Train_AverageReturn : 864.6666870117188\n",
      "Train_StdReturn : 302.6145324707031\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 188.0\n",
      "Train_AverageEpLen : 864.6666666666666\n",
      "Train_EnvstepsSoFar : 413109\n",
      "TimeSinceStart : 234.1413917541504\n",
      "Critic_Loss : 0.37696003913879395\n",
      "Actor_Loss : -133.88787841796875\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5235 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1000.0\n",
      "Eval_StdReturn : 0.0\n",
      "Eval_MaxReturn : 1000.0\n",
      "Eval_MinReturn : 1000.0\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1000.0\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 1000.0\n",
      "Train_MinReturn : 1000.0\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 463344\n",
      "TimeSinceStart : 261.37667655944824\n",
      "Critic_Loss : 0.3473062515258789\n",
      "Actor_Loss : -32.132781982421875\n",
      "Initial_DataCollection_AverageReturn : 8.510204315185547\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     5000 / 5000\n",
      "Training agent...\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_actor_critic.py --env_name InvertedPendulum-v2 --ep_len 1000 --discount 0.95 \\\n",
    "-n 100 -l 2 -s 64 -b 5000 -lr 0.01 --exp_name q5_10_10 -ntu 10 -ngsptu 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "LOGGING TO:  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q5_10_10_HalfCheetah-v2_17-10-2020_13-17-27 \n",
      "\n",
      "\n",
      "\n",
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw3/cs285/scripts/../data/hw3_ q5_10_10_HalfCheetah-v2_17-10-2020_13-17-27\n",
      "########################\n",
      "Using GPU id 0\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -85.6175765991211\n",
      "Eval_StdReturn : 28.744768142700195\n",
      "Eval_MaxReturn : -48.76361846923828\n",
      "Eval_MinReturn : -143.397216796875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -89.08087921142578\n",
      "Train_StdReturn : 38.87763595581055\n",
      "Train_MaxReturn : -1.1699542999267578\n",
      "Train_MinReturn : -194.1424560546875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 30049\n",
      "TimeSinceStart : 18.14404058456421\n",
      "Critic_Loss : 1.0509347915649414\n",
      "Actor_Loss : -15285.0107421875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -58.799278259277344\n",
      "Eval_StdReturn : 29.95056915283203\n",
      "Eval_MaxReturn : -8.570535659790039\n",
      "Eval_MinReturn : -115.3890380859375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -78.5384292602539\n",
      "Train_StdReturn : 33.48765563964844\n",
      "Train_MaxReturn : 40.44258117675781\n",
      "Train_MinReturn : -165.39492797851562\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 60098\n",
      "TimeSinceStart : 36.81688594818115\n",
      "Critic_Loss : 1.3035959005355835\n",
      "Actor_Loss : -15229.1650390625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -58.17827224731445\n",
      "Eval_StdReturn : 22.0148868560791\n",
      "Eval_MaxReturn : -12.94913387298584\n",
      "Eval_MinReturn : -84.27778625488281\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -72.3239517211914\n",
      "Train_StdReturn : 30.19342803955078\n",
      "Train_MaxReturn : -1.2996578216552734\n",
      "Train_MinReturn : -175.9370574951172\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 90147\n",
      "TimeSinceStart : 55.50096106529236\n",
      "Critic_Loss : 1.413073182106018\n",
      "Actor_Loss : -14495.490234375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -71.17927551269531\n",
      "Eval_StdReturn : 36.21220397949219\n",
      "Eval_MaxReturn : -40.79042434692383\n",
      "Eval_MinReturn : -169.4820098876953\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -68.14401245117188\n",
      "Train_StdReturn : 29.18878746032715\n",
      "Train_MaxReturn : 10.596769332885742\n",
      "Train_MinReturn : -152.50128173828125\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 120196\n",
      "TimeSinceStart : 74.79214382171631\n",
      "Critic_Loss : 1.2313859462738037\n",
      "Actor_Loss : -14328.9296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -61.35023880004883\n",
      "Eval_StdReturn : 28.718358993530273\n",
      "Eval_MaxReturn : 5.578876972198486\n",
      "Eval_MinReturn : -110.60623931884766\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -62.812774658203125\n",
      "Train_StdReturn : 30.622268676757812\n",
      "Train_MaxReturn : 15.075092315673828\n",
      "Train_MinReturn : -161.22372436523438\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 150245\n",
      "TimeSinceStart : 92.72080135345459\n",
      "Critic_Loss : 1.144758701324463\n",
      "Actor_Loss : -14327.259765625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -56.761314392089844\n",
      "Eval_StdReturn : 20.644329071044922\n",
      "Eval_MaxReturn : -22.591064453125\n",
      "Eval_MinReturn : -89.9713134765625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -55.065975189208984\n",
      "Train_StdReturn : 28.330005645751953\n",
      "Train_MaxReturn : 7.144587993621826\n",
      "Train_MinReturn : -161.13282775878906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 180294\n",
      "TimeSinceStart : 111.22103071212769\n",
      "Critic_Loss : 1.167211890220642\n",
      "Actor_Loss : -13706.9921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -59.67958450317383\n",
      "Eval_StdReturn : 24.866851806640625\n",
      "Eval_MaxReturn : -30.903690338134766\n",
      "Eval_MinReturn : -104.43931579589844\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -57.79290008544922\n",
      "Train_StdReturn : 28.641027450561523\n",
      "Train_MaxReturn : 40.72222900390625\n",
      "Train_MinReturn : -161.6580810546875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 210343\n",
      "TimeSinceStart : 129.49368405342102\n",
      "Critic_Loss : 1.0364775657653809\n",
      "Actor_Loss : -13276.0810546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -35.603515625\n",
      "Eval_StdReturn : 22.520004272460938\n",
      "Eval_MaxReturn : 7.92414665222168\n",
      "Eval_MinReturn : -74.67623901367188\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -49.81177520751953\n",
      "Train_StdReturn : 24.915306091308594\n",
      "Train_MaxReturn : 12.56339168548584\n",
      "Train_MinReturn : -121.62811279296875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 240392\n",
      "TimeSinceStart : 147.7564935684204\n",
      "Critic_Loss : 0.9370710253715515\n",
      "Actor_Loss : -14298.7060546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -64.69342803955078\n",
      "Eval_StdReturn : 26.44278335571289\n",
      "Eval_MaxReturn : -16.123241424560547\n",
      "Eval_MinReturn : -110.21343994140625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -50.474388122558594\n",
      "Train_StdReturn : 26.98683738708496\n",
      "Train_MaxReturn : 25.8505859375\n",
      "Train_MinReturn : -140.6209716796875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 270441\n",
      "TimeSinceStart : 165.5845422744751\n",
      "Critic_Loss : 0.9117076992988586\n",
      "Actor_Loss : -13442.220703125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -47.6899299621582\n",
      "Eval_StdReturn : 12.634485244750977\n",
      "Eval_MaxReturn : -32.22032928466797\n",
      "Eval_MinReturn : -72.89215087890625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -42.04939651489258\n",
      "Train_StdReturn : 24.131553649902344\n",
      "Train_MaxReturn : 29.8792781829834\n",
      "Train_MinReturn : -142.54559326171875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 300490\n",
      "TimeSinceStart : 182.86424779891968\n",
      "Critic_Loss : 0.8798630833625793\n",
      "Actor_Loss : -14204.07421875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 10 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -51.016510009765625\n",
      "Eval_StdReturn : 15.774559020996094\n",
      "Eval_MaxReturn : -24.268198013305664\n",
      "Eval_MinReturn : -83.70121765136719\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -45.71683120727539\n",
      "Train_StdReturn : 23.1425838470459\n",
      "Train_MaxReturn : 14.024045944213867\n",
      "Train_MinReturn : -127.10399627685547\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 330539\n",
      "TimeSinceStart : 200.37318754196167\n",
      "Critic_Loss : 0.851627767086029\n",
      "Actor_Loss : -13226.009765625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 11 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -46.20756912231445\n",
      "Eval_StdReturn : 30.828947067260742\n",
      "Eval_MaxReturn : -7.686058044433594\n",
      "Eval_MinReturn : -125.23880767822266\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -43.56398391723633\n",
      "Train_StdReturn : 24.179645538330078\n",
      "Train_MaxReturn : 18.898483276367188\n",
      "Train_MinReturn : -129.3731231689453\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 360588\n",
      "TimeSinceStart : 217.97668957710266\n",
      "Critic_Loss : 0.8718010783195496\n",
      "Actor_Loss : -13091.919921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 12 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -32.802555084228516\n",
      "Eval_StdReturn : 21.946521759033203\n",
      "Eval_MaxReturn : -6.628256797790527\n",
      "Eval_MinReturn : -82.70947265625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -40.268653869628906\n",
      "Train_StdReturn : 22.18984031677246\n",
      "Train_MaxReturn : 31.33395767211914\n",
      "Train_MinReturn : -111.02662658691406\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 390637\n",
      "TimeSinceStart : 235.63438153266907\n",
      "Critic_Loss : 0.8428329229354858\n",
      "Actor_Loss : -13137.2802734375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 13 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -35.0695915222168\n",
      "Eval_StdReturn : 16.232202529907227\n",
      "Eval_MaxReturn : -6.020549774169922\n",
      "Eval_MinReturn : -55.4185676574707\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -36.85275650024414\n",
      "Train_StdReturn : 21.140954971313477\n",
      "Train_MaxReturn : 55.30115509033203\n",
      "Train_MinReturn : -109.31217193603516\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 420686\n",
      "TimeSinceStart : 253.01605367660522\n",
      "Critic_Loss : 0.8021552562713623\n",
      "Actor_Loss : -12602.1689453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 14 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -27.10480308532715\n",
      "Eval_StdReturn : 12.407051086425781\n",
      "Eval_MaxReturn : -9.28138542175293\n",
      "Eval_MinReturn : -43.31926727294922\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -34.440181732177734\n",
      "Train_StdReturn : 18.545583724975586\n",
      "Train_MaxReturn : 17.64385223388672\n",
      "Train_MinReturn : -87.26081085205078\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 450735\n",
      "TimeSinceStart : 270.5606734752655\n",
      "Critic_Loss : 0.7071506381034851\n",
      "Actor_Loss : -13767.8984375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 15 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -32.46923065185547\n",
      "Eval_StdReturn : 10.675131797790527\n",
      "Eval_MaxReturn : -18.412080764770508\n",
      "Eval_MinReturn : -53.393611907958984\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -32.3265266418457\n",
      "Train_StdReturn : 15.998472213745117\n",
      "Train_MaxReturn : 11.999658584594727\n",
      "Train_MinReturn : -97.52767944335938\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 480784\n",
      "TimeSinceStart : 287.9769780635834\n",
      "Critic_Loss : 0.5881991982460022\n",
      "Actor_Loss : -13996.580078125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 16 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -25.030582427978516\n",
      "Eval_StdReturn : 14.510791778564453\n",
      "Eval_MaxReturn : 10.586405754089355\n",
      "Eval_MinReturn : -45.253150939941406\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -30.15402603149414\n",
      "Train_StdReturn : 16.176048278808594\n",
      "Train_MaxReturn : 16.03609848022461\n",
      "Train_MinReturn : -99.28111267089844\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 510833\n",
      "TimeSinceStart : 305.38668036460876\n",
      "Critic_Loss : 0.6174759268760681\n",
      "Actor_Loss : -13805.3935546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 17 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -29.849544525146484\n",
      "Eval_StdReturn : 14.045513153076172\n",
      "Eval_MaxReturn : -15.227462768554688\n",
      "Eval_MinReturn : -62.008934020996094\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -23.86411476135254\n",
      "Train_StdReturn : 15.631379127502441\n",
      "Train_MaxReturn : 24.70967674255371\n",
      "Train_MinReturn : -72.76483154296875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 540882\n",
      "TimeSinceStart : 323.5928387641907\n",
      "Critic_Loss : 0.5205485820770264\n",
      "Actor_Loss : -13572.517578125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 18 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -17.870332717895508\n",
      "Eval_StdReturn : 17.66154670715332\n",
      "Eval_MaxReturn : 0.5862827301025391\n",
      "Eval_MinReturn : -58.98118591308594\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -20.597896575927734\n",
      "Train_StdReturn : 15.601126670837402\n",
      "Train_MaxReturn : 27.163389205932617\n",
      "Train_MinReturn : -90.51739501953125\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 570931\n",
      "TimeSinceStart : 340.9068777561188\n",
      "Critic_Loss : 0.49494263529777527\n",
      "Actor_Loss : -14527.3369140625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 19 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -15.046133041381836\n",
      "Eval_StdReturn : 10.00911808013916\n",
      "Eval_MaxReturn : 4.878812789916992\n",
      "Eval_MinReturn : -31.655792236328125\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -18.630107879638672\n",
      "Train_StdReturn : 15.615534782409668\n",
      "Train_MaxReturn : 25.334089279174805\n",
      "Train_MinReturn : -69.48117065429688\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 600980\n",
      "TimeSinceStart : 358.42942237854004\n",
      "Critic_Loss : 0.4924429655075073\n",
      "Actor_Loss : -13527.55078125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 20 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -7.767059326171875\n",
      "Eval_StdReturn : 9.484723091125488\n",
      "Eval_MaxReturn : 5.523726463317871\n",
      "Eval_MinReturn : -26.438968658447266\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -15.17431354522705\n",
      "Train_StdReturn : 14.58225154876709\n",
      "Train_MaxReturn : 22.72943115234375\n",
      "Train_MinReturn : -63.48004913330078\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 631029\n",
      "TimeSinceStart : 376.13511538505554\n",
      "Critic_Loss : 0.4634144604206085\n",
      "Actor_Loss : -13507.720703125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 21 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -26.14046287536621\n",
      "Eval_StdReturn : 15.04521369934082\n",
      "Eval_MaxReturn : -1.242645263671875\n",
      "Eval_MinReturn : -50.571510314941406\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -8.078399658203125\n",
      "Train_StdReturn : 12.420292854309082\n",
      "Train_MaxReturn : 25.74179458618164\n",
      "Train_MinReturn : -37.04892349243164\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 661078\n",
      "TimeSinceStart : 393.7154612541199\n",
      "Critic_Loss : 0.44322553277015686\n",
      "Actor_Loss : -13929.4921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 22 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : -4.085093021392822\n",
      "Eval_StdReturn : 18.73056983947754\n",
      "Eval_MaxReturn : 28.749553680419922\n",
      "Eval_MinReturn : -46.32471466064453\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : -5.0413432121276855\n",
      "Train_StdReturn : 16.362451553344727\n",
      "Train_MaxReturn : 31.45547866821289\n",
      "Train_MinReturn : -52.270851135253906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 691127\n",
      "TimeSinceStart : 411.3211467266083\n",
      "Critic_Loss : 0.482900470495224\n",
      "Actor_Loss : -13134.1337890625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 23 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 17.188396453857422\n",
      "Eval_StdReturn : 11.15262508392334\n",
      "Eval_MaxReturn : 37.954288482666016\n",
      "Eval_MinReturn : 4.400546073913574\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 0.11905325204133987\n",
      "Train_StdReturn : 15.173652648925781\n",
      "Train_MaxReturn : 39.106746673583984\n",
      "Train_MinReturn : -48.78905487060547\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 721176\n",
      "TimeSinceStart : 428.93042254447937\n",
      "Critic_Loss : 0.47297218441963196\n",
      "Actor_Loss : -13400.154296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 24 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 6.247337818145752\n",
      "Eval_StdReturn : 13.249981880187988\n",
      "Eval_MaxReturn : 30.52979278564453\n",
      "Eval_MinReturn : -14.987659454345703\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 5.900332927703857\n",
      "Train_StdReturn : 16.814208984375\n",
      "Train_MaxReturn : 44.186832427978516\n",
      "Train_MinReturn : -50.234764099121094\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 751225\n",
      "TimeSinceStart : 446.379102230072\n",
      "Critic_Loss : 0.5578311085700989\n",
      "Actor_Loss : -12607.6083984375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 25 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 15.085054397583008\n",
      "Eval_StdReturn : 17.556142807006836\n",
      "Eval_MaxReturn : 44.07524108886719\n",
      "Eval_MinReturn : -18.52591323852539\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 8.161784172058105\n",
      "Train_StdReturn : 16.684276580810547\n",
      "Train_MaxReturn : 42.57889175415039\n",
      "Train_MinReturn : -81.10063934326172\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 781274\n",
      "TimeSinceStart : 464.42086458206177\n",
      "Critic_Loss : 0.4188298285007477\n",
      "Actor_Loss : -13714.4033203125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 26 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 15.017122268676758\n",
      "Eval_StdReturn : 27.450592041015625\n",
      "Eval_MaxReturn : 33.71185302734375\n",
      "Eval_MinReturn : -63.881439208984375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 12.892471313476562\n",
      "Train_StdReturn : 18.36819076538086\n",
      "Train_MaxReturn : 51.170406341552734\n",
      "Train_MinReturn : -45.147727966308594\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 811323\n",
      "TimeSinceStart : 481.873172044754\n",
      "Critic_Loss : 0.4696372449398041\n",
      "Actor_Loss : -12830.66796875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 27 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 25.904958724975586\n",
      "Eval_StdReturn : 24.77543830871582\n",
      "Eval_MaxReturn : 60.751426696777344\n",
      "Eval_MinReturn : -26.99628448486328\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 17.25534439086914\n",
      "Train_StdReturn : 23.30549430847168\n",
      "Train_MaxReturn : 58.644615173339844\n",
      "Train_MinReturn : -68.44544982910156\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 841372\n",
      "TimeSinceStart : 499.7045991420746\n",
      "Critic_Loss : 0.5064511299133301\n",
      "Actor_Loss : -12235.998046875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 28 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 23.12274169921875\n",
      "Eval_StdReturn : 20.44327163696289\n",
      "Eval_MaxReturn : 54.613243103027344\n",
      "Eval_MinReturn : -14.735237121582031\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 18.7315731048584\n",
      "Train_StdReturn : 25.40676498413086\n",
      "Train_MaxReturn : 65.99876403808594\n",
      "Train_MinReturn : -64.65234375\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 871421\n",
      "TimeSinceStart : 517.7295393943787\n",
      "Critic_Loss : 0.5835741758346558\n",
      "Actor_Loss : -11612.96484375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 29 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 20.293970108032227\n",
      "Eval_StdReturn : 19.77559471130371\n",
      "Eval_MaxReturn : 45.63812255859375\n",
      "Eval_MinReturn : -26.646739959716797\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 23.7331485748291\n",
      "Train_StdReturn : 25.150390625\n",
      "Train_MaxReturn : 73.32581329345703\n",
      "Train_MinReturn : -69.38343811035156\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 901470\n",
      "TimeSinceStart : 536.5473895072937\n",
      "Critic_Loss : 0.5542230606079102\n",
      "Actor_Loss : -12242.646484375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 30 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 29.792804718017578\n",
      "Eval_StdReturn : 21.910276412963867\n",
      "Eval_MaxReturn : 63.74456024169922\n",
      "Eval_MinReturn : 2.2677173614501953\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 28.417163848876953\n",
      "Train_StdReturn : 19.799463272094727\n",
      "Train_MaxReturn : 67.20086669921875\n",
      "Train_MinReturn : -44.20655822753906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 931519\n",
      "TimeSinceStart : 554.1314945220947\n",
      "Critic_Loss : 0.5247548818588257\n",
      "Actor_Loss : -11468.1142578125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 31 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 41.79288864135742\n",
      "Eval_StdReturn : 22.30011558532715\n",
      "Eval_MaxReturn : 69.55730438232422\n",
      "Eval_MinReturn : -10.49582576751709\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 33.49351119995117\n",
      "Train_StdReturn : 19.761642456054688\n",
      "Train_MaxReturn : 79.27618408203125\n",
      "Train_MinReturn : -27.93592071533203\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 961568\n",
      "TimeSinceStart : 571.8940086364746\n",
      "Critic_Loss : 0.5464624166488647\n",
      "Actor_Loss : -10775.2890625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 32 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 44.620330810546875\n",
      "Eval_StdReturn : 28.5466365814209\n",
      "Eval_MaxReturn : 75.98255920410156\n",
      "Eval_MinReturn : -25.26404571533203\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 37.42537307739258\n",
      "Train_StdReturn : 20.804777145385742\n",
      "Train_MaxReturn : 81.01500701904297\n",
      "Train_MinReturn : -26.339092254638672\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 991617\n",
      "TimeSinceStart : 589.1445181369781\n",
      "Critic_Loss : 0.5570928454399109\n",
      "Actor_Loss : -11181.251953125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 33 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 58.605628967285156\n",
      "Eval_StdReturn : 18.948984146118164\n",
      "Eval_MaxReturn : 98.86648559570312\n",
      "Eval_MinReturn : 32.7630729675293\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 44.176334381103516\n",
      "Train_StdReturn : 19.641475677490234\n",
      "Train_MaxReturn : 95.87748718261719\n",
      "Train_MinReturn : -39.713497161865234\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1021666\n",
      "TimeSinceStart : 606.252846956253\n",
      "Critic_Loss : 0.5449522733688354\n",
      "Actor_Loss : -10993.6181640625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 34 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 47.58295822143555\n",
      "Eval_StdReturn : 22.310514450073242\n",
      "Eval_MaxReturn : 83.463623046875\n",
      "Eval_MinReturn : 6.287494659423828\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 48.348758697509766\n",
      "Train_StdReturn : 23.106502532958984\n",
      "Train_MaxReturn : 95.10969543457031\n",
      "Train_MinReturn : -36.726402282714844\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1051715\n",
      "TimeSinceStart : 623.5924954414368\n",
      "Critic_Loss : 0.5507863759994507\n",
      "Actor_Loss : -10709.37109375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 35 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 53.94707107543945\n",
      "Eval_StdReturn : 13.15809440612793\n",
      "Eval_MaxReturn : 68.9083251953125\n",
      "Eval_MinReturn : 19.586868286132812\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 51.395503997802734\n",
      "Train_StdReturn : 19.17697525024414\n",
      "Train_MaxReturn : 90.27545166015625\n",
      "Train_MinReturn : -29.768714904785156\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1081764\n",
      "TimeSinceStart : 640.6789190769196\n",
      "Critic_Loss : 0.5903677344322205\n",
      "Actor_Loss : -9843.755859375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 36 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 59.63970947265625\n",
      "Eval_StdReturn : 16.69888687133789\n",
      "Eval_MaxReturn : 81.78065490722656\n",
      "Eval_MinReturn : 26.63535499572754\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 52.64213180541992\n",
      "Train_StdReturn : 20.493574142456055\n",
      "Train_MaxReturn : 94.68147277832031\n",
      "Train_MinReturn : -25.05010223388672\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1111813\n",
      "TimeSinceStart : 657.981406211853\n",
      "Critic_Loss : 0.5753107070922852\n",
      "Actor_Loss : -9896.79296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 37 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 60.56312942504883\n",
      "Eval_StdReturn : 22.67901611328125\n",
      "Eval_MaxReturn : 83.81576538085938\n",
      "Eval_MinReturn : 21.10234260559082\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 61.48469924926758\n",
      "Train_StdReturn : 24.674713134765625\n",
      "Train_MaxReturn : 107.95429229736328\n",
      "Train_MinReturn : -29.41463851928711\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1141862\n",
      "TimeSinceStart : 675.2642824649811\n",
      "Critic_Loss : 0.5920546650886536\n",
      "Actor_Loss : -10620.814453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 38 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 58.2288818359375\n",
      "Eval_StdReturn : 28.88418960571289\n",
      "Eval_MaxReturn : 90.63502502441406\n",
      "Eval_MinReturn : -2.4562950134277344\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 66.54720306396484\n",
      "Train_StdReturn : 26.502927780151367\n",
      "Train_MaxReturn : 117.71891784667969\n",
      "Train_MinReturn : -38.304100036621094\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1171911\n",
      "TimeSinceStart : 692.3246698379517\n",
      "Critic_Loss : 0.6748034954071045\n",
      "Actor_Loss : -9139.818359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 39 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 27.435760498046875\n",
      "Eval_StdReturn : 38.42161178588867\n",
      "Eval_MaxReturn : 86.05208587646484\n",
      "Eval_MinReturn : -40.66849136352539\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 63.49681854248047\n",
      "Train_StdReturn : 25.28321647644043\n",
      "Train_MaxReturn : 122.22651672363281\n",
      "Train_MinReturn : -64.6434097290039\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1201960\n",
      "TimeSinceStart : 709.3442964553833\n",
      "Critic_Loss : 1.7115966081619263\n",
      "Actor_Loss : -3535.6669921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 40 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 61.907691955566406\n",
      "Eval_StdReturn : 27.588428497314453\n",
      "Eval_MaxReturn : 104.36587524414062\n",
      "Eval_MinReturn : 3.4730143547058105\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 44.8878288269043\n",
      "Train_StdReturn : 27.9136962890625\n",
      "Train_MaxReturn : 109.43772888183594\n",
      "Train_MinReturn : -44.383602142333984\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1232009\n",
      "TimeSinceStart : 726.318794965744\n",
      "Critic_Loss : 1.037820816040039\n",
      "Actor_Loss : -6143.0380859375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 41 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 74.80860900878906\n",
      "Eval_StdReturn : 27.042436599731445\n",
      "Eval_MaxReturn : 95.51396179199219\n",
      "Eval_MinReturn : -3.7137036323547363\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 66.9495849609375\n",
      "Train_StdReturn : 21.915319442749023\n",
      "Train_MaxReturn : 105.87080383300781\n",
      "Train_MinReturn : -24.104904174804688\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1262058\n",
      "TimeSinceStart : 743.4182469844818\n",
      "Critic_Loss : 0.7001981735229492\n",
      "Actor_Loss : -7691.189453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 42 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 80.35124206542969\n",
      "Eval_StdReturn : 12.254841804504395\n",
      "Eval_MaxReturn : 92.92384338378906\n",
      "Eval_MinReturn : 49.073211669921875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 72.72574615478516\n",
      "Train_StdReturn : 28.908340454101562\n",
      "Train_MaxReturn : 114.20783233642578\n",
      "Train_MinReturn : -27.527240753173828\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1292107\n",
      "TimeSinceStart : 760.6137669086456\n",
      "Critic_Loss : 0.583516538143158\n",
      "Actor_Loss : -8445.697265625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 43 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 76.56590270996094\n",
      "Eval_StdReturn : 14.766863822937012\n",
      "Eval_MaxReturn : 97.60185241699219\n",
      "Eval_MinReturn : 45.05600357055664\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 71.65050506591797\n",
      "Train_StdReturn : 21.008211135864258\n",
      "Train_MaxReturn : 115.28895568847656\n",
      "Train_MinReturn : -10.62984848022461\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1322156\n",
      "TimeSinceStart : 777.70139336586\n",
      "Critic_Loss : 0.51458740234375\n",
      "Actor_Loss : -9493.416015625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 44 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 91.93948364257812\n",
      "Eval_StdReturn : 17.691476821899414\n",
      "Eval_MaxReturn : 110.84471130371094\n",
      "Eval_MinReturn : 50.88591003417969\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 84.89519500732422\n",
      "Train_StdReturn : 14.858616828918457\n",
      "Train_MaxReturn : 120.861328125\n",
      "Train_MinReturn : 18.07667350769043\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1352205\n",
      "TimeSinceStart : 794.6183543205261\n",
      "Critic_Loss : 0.5527991056442261\n",
      "Actor_Loss : -8731.537109375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 45 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 73.29192352294922\n",
      "Eval_StdReturn : 30.136064529418945\n",
      "Eval_MaxReturn : 128.46951293945312\n",
      "Eval_MinReturn : 6.468621253967285\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 80.3563003540039\n",
      "Train_StdReturn : 21.38043785095215\n",
      "Train_MaxReturn : 125.82530212402344\n",
      "Train_MinReturn : -1.3800296783447266\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1382254\n",
      "TimeSinceStart : 811.6207373142242\n",
      "Critic_Loss : 0.7671447992324829\n",
      "Actor_Loss : -6281.77880859375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 46 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 108.18707275390625\n",
      "Eval_StdReturn : 12.518471717834473\n",
      "Eval_MaxReturn : 127.08355712890625\n",
      "Eval_MinReturn : 84.64892578125\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 73.65057373046875\n",
      "Train_StdReturn : 25.192216873168945\n",
      "Train_MaxReturn : 127.31853485107422\n",
      "Train_MinReturn : -20.331491470336914\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1412303\n",
      "TimeSinceStart : 828.7280864715576\n",
      "Critic_Loss : 0.9727343916893005\n",
      "Actor_Loss : -6473.32275390625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 47 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 114.76560974121094\n",
      "Eval_StdReturn : 16.403955459594727\n",
      "Eval_MaxReturn : 144.13748168945312\n",
      "Eval_MinReturn : 78.78079223632812\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 95.39418029785156\n",
      "Train_StdReturn : 22.090803146362305\n",
      "Train_MaxReturn : 146.90728759765625\n",
      "Train_MinReturn : 30.920204162597656\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1442352\n",
      "TimeSinceStart : 845.7447125911713\n",
      "Critic_Loss : 0.9437242150306702\n",
      "Actor_Loss : -5175.2119140625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 48 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 96.4990005493164\n",
      "Eval_StdReturn : 11.321499824523926\n",
      "Eval_MaxReturn : 117.74708557128906\n",
      "Eval_MinReturn : 74.50900268554688\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 99.84506225585938\n",
      "Train_StdReturn : 24.000076293945312\n",
      "Train_MaxReturn : 147.62417602539062\n",
      "Train_MinReturn : -20.298084259033203\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1472401\n",
      "TimeSinceStart : 862.789267539978\n",
      "Critic_Loss : 0.7591284513473511\n",
      "Actor_Loss : -6857.0859375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 49 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 94.74071502685547\n",
      "Eval_StdReturn : 8.654601097106934\n",
      "Eval_MaxReturn : 109.7044677734375\n",
      "Eval_MinReturn : 76.32963562011719\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 91.49666595458984\n",
      "Train_StdReturn : 27.56459617614746\n",
      "Train_MaxReturn : 132.81039428710938\n",
      "Train_MinReturn : -18.35860824584961\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1502450\n",
      "TimeSinceStart : 879.8762521743774\n",
      "Critic_Loss : 0.6751731634140015\n",
      "Actor_Loss : -7590.5166015625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 50 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 104.9013671875\n",
      "Eval_StdReturn : 25.161659240722656\n",
      "Eval_MaxReturn : 135.93829345703125\n",
      "Eval_MinReturn : 42.70977020263672\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 98.37013244628906\n",
      "Train_StdReturn : 20.88529396057129\n",
      "Train_MaxReturn : 138.6777801513672\n",
      "Train_MinReturn : 6.401920318603516\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1532499\n",
      "TimeSinceStart : 896.7997229099274\n",
      "Critic_Loss : 0.6301131844520569\n",
      "Actor_Loss : -8273.16796875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 51 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 98.85359191894531\n",
      "Eval_StdReturn : 16.187326431274414\n",
      "Eval_MaxReturn : 120.62539672851562\n",
      "Eval_MinReturn : 72.13558959960938\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 97.5625\n",
      "Train_StdReturn : 14.316838264465332\n",
      "Train_MaxReturn : 128.9322967529297\n",
      "Train_MinReturn : 34.05136489868164\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1562548\n",
      "TimeSinceStart : 913.849413394928\n",
      "Critic_Loss : 0.5748476982116699\n",
      "Actor_Loss : -7744.86669921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 52 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 108.63447570800781\n",
      "Eval_StdReturn : 13.40453052520752\n",
      "Eval_MaxReturn : 140.33099365234375\n",
      "Eval_MinReturn : 88.12153625488281\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 92.10491180419922\n",
      "Train_StdReturn : 19.73216438293457\n",
      "Train_MaxReturn : 142.67376708984375\n",
      "Train_MinReturn : -26.858747482299805\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1592597\n",
      "TimeSinceStart : 931.0159986019135\n",
      "Critic_Loss : 0.5406376123428345\n",
      "Actor_Loss : -7926.875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 53 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 96.48936462402344\n",
      "Eval_StdReturn : 39.412071228027344\n",
      "Eval_MaxReturn : 127.00907135009766\n",
      "Eval_MinReturn : 9.252081871032715\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 103.05233001708984\n",
      "Train_StdReturn : 21.01883316040039\n",
      "Train_MaxReturn : 146.44082641601562\n",
      "Train_MinReturn : 9.094438552856445\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1622646\n",
      "TimeSinceStart : 947.9163649082184\n",
      "Critic_Loss : 0.6258746385574341\n",
      "Actor_Loss : -6163.39453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 54 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 110.6900863647461\n",
      "Eval_StdReturn : 22.444808959960938\n",
      "Eval_MaxReturn : 154.73289489746094\n",
      "Eval_MinReturn : 80.7403564453125\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 109.15782165527344\n",
      "Train_StdReturn : 23.688695907592773\n",
      "Train_MaxReturn : 151.63751220703125\n",
      "Train_MinReturn : 6.625812530517578\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1652695\n",
      "TimeSinceStart : 964.886157989502\n",
      "Critic_Loss : 0.7446554899215698\n",
      "Actor_Loss : -6386.45068359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 55 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 111.26480865478516\n",
      "Eval_StdReturn : 17.269872665405273\n",
      "Eval_MaxReturn : 135.1712646484375\n",
      "Eval_MinReturn : 86.9449462890625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 115.23739624023438\n",
      "Train_StdReturn : 22.422103881835938\n",
      "Train_MaxReturn : 169.2139892578125\n",
      "Train_MinReturn : 13.8032865524292\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1682744\n",
      "TimeSinceStart : 981.8754732608795\n",
      "Critic_Loss : 0.7699391841888428\n",
      "Actor_Loss : -6040.783203125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 56 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 125.6841049194336\n",
      "Eval_StdReturn : 19.209854125976562\n",
      "Eval_MaxReturn : 160.5552520751953\n",
      "Eval_MinReturn : 93.50947570800781\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 114.64366912841797\n",
      "Train_StdReturn : 23.0107364654541\n",
      "Train_MaxReturn : 162.3724365234375\n",
      "Train_MinReturn : -6.047819137573242\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1712793\n",
      "TimeSinceStart : 998.9789159297943\n",
      "Critic_Loss : 0.8842450380325317\n",
      "Actor_Loss : -4390.1552734375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 57 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 114.43756103515625\n",
      "Eval_StdReturn : 23.068805694580078\n",
      "Eval_MaxReturn : 144.54058837890625\n",
      "Eval_MinReturn : 73.71125793457031\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 115.84473419189453\n",
      "Train_StdReturn : 21.83479118347168\n",
      "Train_MaxReturn : 159.6051025390625\n",
      "Train_MinReturn : 7.7276692390441895\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1742842\n",
      "TimeSinceStart : 1015.9823458194733\n",
      "Critic_Loss : 1.064895749092102\n",
      "Actor_Loss : -4745.1318359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 58 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 125.4207763671875\n",
      "Eval_StdReturn : 10.631783485412598\n",
      "Eval_MaxReturn : 142.650390625\n",
      "Eval_MinReturn : 107.69805908203125\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 116.22405242919922\n",
      "Train_StdReturn : 24.26888656616211\n",
      "Train_MaxReturn : 168.14401245117188\n",
      "Train_MinReturn : -15.557222366333008\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1772891\n",
      "TimeSinceStart : 1033.1739761829376\n",
      "Critic_Loss : 0.7813611626625061\n",
      "Actor_Loss : -5321.5166015625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 59 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 114.45072937011719\n",
      "Eval_StdReturn : 11.290412902832031\n",
      "Eval_MaxReturn : 134.5289764404297\n",
      "Eval_MinReturn : 100.14657592773438\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 118.76919555664062\n",
      "Train_StdReturn : 20.51993751525879\n",
      "Train_MaxReturn : 159.7195587158203\n",
      "Train_MinReturn : 37.8200569152832\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1802940\n",
      "TimeSinceStart : 1050.0384080410004\n",
      "Critic_Loss : 0.7909047603607178\n",
      "Actor_Loss : -4661.28955078125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 60 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 109.88521575927734\n",
      "Eval_StdReturn : 9.777981758117676\n",
      "Eval_MaxReturn : 126.16973876953125\n",
      "Eval_MinReturn : 95.67767333984375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 113.45206451416016\n",
      "Train_StdReturn : 18.181856155395508\n",
      "Train_MaxReturn : 156.96844482421875\n",
      "Train_MinReturn : 28.299667358398438\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1832989\n",
      "TimeSinceStart : 1067.0465681552887\n",
      "Critic_Loss : 0.7242826223373413\n",
      "Actor_Loss : -5487.599609375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 61 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 117.96195220947266\n",
      "Eval_StdReturn : 18.097171783447266\n",
      "Eval_MaxReturn : 136.20895385742188\n",
      "Eval_MinReturn : 86.32618713378906\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 109.9581298828125\n",
      "Train_StdReturn : 17.441377639770508\n",
      "Train_MaxReturn : 137.02865600585938\n",
      "Train_MinReturn : 36.214332580566406\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1863038\n",
      "TimeSinceStart : 1084.1656403541565\n",
      "Critic_Loss : 0.7346225380897522\n",
      "Actor_Loss : -5173.72412109375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 62 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 125.68572998046875\n",
      "Eval_StdReturn : 13.244762420654297\n",
      "Eval_MaxReturn : 150.37747192382812\n",
      "Eval_MinReturn : 101.44573211669922\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 118.24242401123047\n",
      "Train_StdReturn : 14.734710693359375\n",
      "Train_MaxReturn : 153.39666748046875\n",
      "Train_MinReturn : 30.570510864257812\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1893087\n",
      "TimeSinceStart : 1101.061146736145\n",
      "Critic_Loss : 0.7181673645973206\n",
      "Actor_Loss : -5459.81005859375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 63 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 128.19117736816406\n",
      "Eval_StdReturn : 14.563745498657227\n",
      "Eval_MaxReturn : 148.31150817871094\n",
      "Eval_MinReturn : 105.04292297363281\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 122.90834045410156\n",
      "Train_StdReturn : 15.333257675170898\n",
      "Train_MaxReturn : 152.82569885253906\n",
      "Train_MinReturn : 60.657039642333984\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1923136\n",
      "TimeSinceStart : 1117.9823062419891\n",
      "Critic_Loss : 0.692251980304718\n",
      "Actor_Loss : -4662.80078125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 64 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.64895629882812\n",
      "Eval_StdReturn : 17.9193115234375\n",
      "Eval_MaxReturn : 170.01068115234375\n",
      "Eval_MinReturn : 113.93115234375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 127.57276916503906\n",
      "Train_StdReturn : 21.33409309387207\n",
      "Train_MaxReturn : 163.9369354248047\n",
      "Train_MinReturn : -11.921655654907227\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1953185\n",
      "TimeSinceStart : 1135.2469272613525\n",
      "Critic_Loss : 0.7154644131660461\n",
      "Actor_Loss : -4264.048828125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 65 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.3446502685547\n",
      "Eval_StdReturn : 8.872762680053711\n",
      "Eval_MaxReturn : 154.62945556640625\n",
      "Eval_MinReturn : 120.46403503417969\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 131.49462890625\n",
      "Train_StdReturn : 17.515892028808594\n",
      "Train_MaxReturn : 171.51084899902344\n",
      "Train_MinReturn : 60.43114471435547\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 1983234\n",
      "TimeSinceStart : 1152.4602513313293\n",
      "Critic_Loss : 0.7883464694023132\n",
      "Actor_Loss : -4527.1328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 66 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.55096435546875\n",
      "Eval_StdReturn : 12.151557922363281\n",
      "Eval_MaxReturn : 150.6487274169922\n",
      "Eval_MinReturn : 117.13414001464844\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 131.51663208007812\n",
      "Train_StdReturn : 20.869192123413086\n",
      "Train_MaxReturn : 171.51290893554688\n",
      "Train_MinReturn : 39.447837829589844\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2013283\n",
      "TimeSinceStart : 1169.4743309020996\n",
      "Critic_Loss : 0.9703549742698669\n",
      "Actor_Loss : -3074.61181640625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 67 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 127.67195892333984\n",
      "Eval_StdReturn : 14.796137809753418\n",
      "Eval_MaxReturn : 156.57650756835938\n",
      "Eval_MinReturn : 101.34541320800781\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 133.88343811035156\n",
      "Train_StdReturn : 19.262855529785156\n",
      "Train_MaxReturn : 181.6905975341797\n",
      "Train_MinReturn : 72.72489166259766\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2043332\n",
      "TimeSinceStart : 1186.570204257965\n",
      "Critic_Loss : 0.9483756422996521\n",
      "Actor_Loss : -4249.3466796875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 68 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 128.3038330078125\n",
      "Eval_StdReturn : 11.938810348510742\n",
      "Eval_MaxReturn : 147.42518615722656\n",
      "Eval_MinReturn : 112.55734252929688\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 129.4932861328125\n",
      "Train_StdReturn : 19.798912048339844\n",
      "Train_MaxReturn : 172.3374786376953\n",
      "Train_MinReturn : 31.07171630859375\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2073381\n",
      "TimeSinceStart : 1203.7955434322357\n",
      "Critic_Loss : 0.833278477191925\n",
      "Actor_Loss : -4574.51025390625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 69 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.5357208251953\n",
      "Eval_StdReturn : 23.508663177490234\n",
      "Eval_MaxReturn : 159.58155822753906\n",
      "Eval_MinReturn : 73.90995025634766\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 128.79640197753906\n",
      "Train_StdReturn : 19.821369171142578\n",
      "Train_MaxReturn : 173.1636199951172\n",
      "Train_MinReturn : 23.538721084594727\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2103430\n",
      "TimeSinceStart : 1220.9084451198578\n",
      "Critic_Loss : 0.825753927230835\n",
      "Actor_Loss : -4615.24169921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 70 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 125.48783874511719\n",
      "Eval_StdReturn : 21.206830978393555\n",
      "Eval_MaxReturn : 151.8822784423828\n",
      "Eval_MinReturn : 73.2922134399414\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 129.38003540039062\n",
      "Train_StdReturn : 20.611717224121094\n",
      "Train_MaxReturn : 174.25579833984375\n",
      "Train_MinReturn : 5.663810729980469\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2133479\n",
      "TimeSinceStart : 1238.0651133060455\n",
      "Critic_Loss : 0.8859667778015137\n",
      "Actor_Loss : -3994.54296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 71 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 128.28321838378906\n",
      "Eval_StdReturn : 21.671030044555664\n",
      "Eval_MaxReturn : 155.25576782226562\n",
      "Eval_MinReturn : 89.63005828857422\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 133.46107482910156\n",
      "Train_StdReturn : 17.193313598632812\n",
      "Train_MaxReturn : 172.76901245117188\n",
      "Train_MinReturn : 82.2327880859375\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2163528\n",
      "TimeSinceStart : 1255.17804312706\n",
      "Critic_Loss : 0.8628301024436951\n",
      "Actor_Loss : -4984.35693359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 72 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.91041564941406\n",
      "Eval_StdReturn : 10.889581680297852\n",
      "Eval_MaxReturn : 157.99539184570312\n",
      "Eval_MinReturn : 122.87075805664062\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 139.1682891845703\n",
      "Train_StdReturn : 17.631576538085938\n",
      "Train_MaxReturn : 176.40322875976562\n",
      "Train_MinReturn : 46.257789611816406\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2193577\n",
      "TimeSinceStart : 1272.1084470748901\n",
      "Critic_Loss : 0.871644914150238\n",
      "Actor_Loss : -4438.068359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 73 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 144.88685607910156\n",
      "Eval_StdReturn : 13.386380195617676\n",
      "Eval_MaxReturn : 168.85977172851562\n",
      "Eval_MinReturn : 123.07124328613281\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 141.8844451904297\n",
      "Train_StdReturn : 16.281980514526367\n",
      "Train_MaxReturn : 186.06077575683594\n",
      "Train_MinReturn : 52.222129821777344\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2223626\n",
      "TimeSinceStart : 1289.441390991211\n",
      "Critic_Loss : 0.8386010527610779\n",
      "Actor_Loss : -4158.2392578125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 74 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 138.45986938476562\n",
      "Eval_StdReturn : 10.691489219665527\n",
      "Eval_MaxReturn : 161.4246826171875\n",
      "Eval_MinReturn : 119.07150268554688\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 141.4694366455078\n",
      "Train_StdReturn : 18.707059860229492\n",
      "Train_MaxReturn : 181.39083862304688\n",
      "Train_MinReturn : 21.86842918395996\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2253675\n",
      "TimeSinceStart : 1306.433893918991\n",
      "Critic_Loss : 0.8097273707389832\n",
      "Actor_Loss : -5001.2626953125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 75 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 144.03118896484375\n",
      "Eval_StdReturn : 19.241933822631836\n",
      "Eval_MaxReturn : 168.18753051757812\n",
      "Eval_MinReturn : 109.08409118652344\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 143.06024169921875\n",
      "Train_StdReturn : 17.42523193359375\n",
      "Train_MaxReturn : 193.10757446289062\n",
      "Train_MinReturn : 84.1537094116211\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2283724\n",
      "TimeSinceStart : 1323.4988946914673\n",
      "Critic_Loss : 0.908727765083313\n",
      "Actor_Loss : -4060.17578125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 76 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.58499145507812\n",
      "Eval_StdReturn : 16.957609176635742\n",
      "Eval_MaxReturn : 167.99363708496094\n",
      "Eval_MinReturn : 112.13066101074219\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 146.00460815429688\n",
      "Train_StdReturn : 17.86031150817871\n",
      "Train_MaxReturn : 191.23080444335938\n",
      "Train_MinReturn : 63.73439025878906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2313773\n",
      "TimeSinceStart : 1340.6515998840332\n",
      "Critic_Loss : 0.9887759685516357\n",
      "Actor_Loss : -3927.95458984375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 77 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 142.2281494140625\n",
      "Eval_StdReturn : 10.696371078491211\n",
      "Eval_MaxReturn : 166.76092529296875\n",
      "Eval_MinReturn : 125.66981506347656\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 145.0264434814453\n",
      "Train_StdReturn : 20.042724609375\n",
      "Train_MaxReturn : 190.6124267578125\n",
      "Train_MinReturn : 60.568748474121094\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2343822\n",
      "TimeSinceStart : 1357.806735277176\n",
      "Critic_Loss : 0.9736859202384949\n",
      "Actor_Loss : -3164.956298828125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 78 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 123.43106842041016\n",
      "Eval_StdReturn : 51.65937042236328\n",
      "Eval_MaxReturn : 183.53208923339844\n",
      "Eval_MinReturn : -11.76634407043457\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 143.05035400390625\n",
      "Train_StdReturn : 20.223665237426758\n",
      "Train_MaxReturn : 185.71463012695312\n",
      "Train_MinReturn : 51.6806640625\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2373871\n",
      "TimeSinceStart : 1375.3410987854004\n",
      "Critic_Loss : 0.9803190231323242\n",
      "Actor_Loss : -3590.5439453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 79 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 154.990234375\n",
      "Eval_StdReturn : 11.6980562210083\n",
      "Eval_MaxReturn : 170.1143798828125\n",
      "Eval_MinReturn : 127.23028564453125\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 144.65089416503906\n",
      "Train_StdReturn : 26.200254440307617\n",
      "Train_MaxReturn : 186.76039123535156\n",
      "Train_MinReturn : 2.4118824005126953\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2403920\n",
      "TimeSinceStart : 1392.8588905334473\n",
      "Critic_Loss : 0.916148841381073\n",
      "Actor_Loss : -3593.04638671875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 80 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 140.0277557373047\n",
      "Eval_StdReturn : 23.644454956054688\n",
      "Eval_MaxReturn : 169.61984252929688\n",
      "Eval_MinReturn : 95.30107879638672\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 155.28680419921875\n",
      "Train_StdReturn : 19.814077377319336\n",
      "Train_MaxReturn : 189.70791625976562\n",
      "Train_MinReturn : 46.32186508178711\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2433969\n",
      "TimeSinceStart : 1410.5881841182709\n",
      "Critic_Loss : 1.0186573266983032\n",
      "Actor_Loss : -3071.412109375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 81 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 158.23452758789062\n",
      "Eval_StdReturn : 24.77335548400879\n",
      "Eval_MaxReturn : 189.0655059814453\n",
      "Eval_MinReturn : 98.64915466308594\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 148.5956268310547\n",
      "Train_StdReturn : 25.472759246826172\n",
      "Train_MaxReturn : 192.64459228515625\n",
      "Train_MinReturn : 12.062129020690918\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2464018\n",
      "TimeSinceStart : 1428.4977073669434\n",
      "Critic_Loss : 1.097900390625\n",
      "Actor_Loss : -2485.0185546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 82 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.93470764160156\n",
      "Eval_StdReturn : 36.93232727050781\n",
      "Eval_MaxReturn : 179.83529663085938\n",
      "Eval_MinReturn : 39.30280303955078\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 151.85162353515625\n",
      "Train_StdReturn : 26.517419815063477\n",
      "Train_MaxReturn : 196.68502807617188\n",
      "Train_MinReturn : -26.44138526916504\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2494067\n",
      "TimeSinceStart : 1448.1702048778534\n",
      "Critic_Loss : 1.0057698488235474\n",
      "Actor_Loss : -3421.974609375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 83 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 153.4300079345703\n",
      "Eval_StdReturn : 11.42228889465332\n",
      "Eval_MaxReturn : 169.47125244140625\n",
      "Eval_MinReturn : 133.80218505859375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 148.81253051757812\n",
      "Train_StdReturn : 28.7988224029541\n",
      "Train_MaxReturn : 195.76490783691406\n",
      "Train_MinReturn : 0.23773765563964844\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2524116\n",
      "TimeSinceStart : 1465.4380764961243\n",
      "Critic_Loss : 1.062342882156372\n",
      "Actor_Loss : -3746.088134765625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 84 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.8157958984375\n",
      "Eval_StdReturn : 9.760313034057617\n",
      "Eval_MaxReturn : 156.86712646484375\n",
      "Eval_MinReturn : 126.39151763916016\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 150.03050231933594\n",
      "Train_StdReturn : 18.0811710357666\n",
      "Train_MaxReturn : 197.30772399902344\n",
      "Train_MinReturn : 64.38289642333984\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2554165\n",
      "TimeSinceStart : 1482.8204898834229\n",
      "Critic_Loss : 0.9626451730728149\n",
      "Actor_Loss : -3442.588623046875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 85 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.4203643798828\n",
      "Eval_StdReturn : 14.660456657409668\n",
      "Eval_MaxReturn : 177.34591674804688\n",
      "Eval_MinReturn : 122.36480712890625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 143.10389709472656\n",
      "Train_StdReturn : 18.225061416625977\n",
      "Train_MaxReturn : 179.48532104492188\n",
      "Train_MinReturn : 41.028663635253906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2584214\n",
      "TimeSinceStart : 1500.1386470794678\n",
      "Critic_Loss : 0.9741387367248535\n",
      "Actor_Loss : -2960.997802734375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 86 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 149.95895385742188\n",
      "Eval_StdReturn : 21.838266372680664\n",
      "Eval_MaxReturn : 176.24252319335938\n",
      "Eval_MinReturn : 102.43684387207031\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 142.50721740722656\n",
      "Train_StdReturn : 19.381391525268555\n",
      "Train_MaxReturn : 189.94317626953125\n",
      "Train_MinReturn : 55.37150955200195\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2614263\n",
      "TimeSinceStart : 1517.6123790740967\n",
      "Critic_Loss : 0.8776388168334961\n",
      "Actor_Loss : -4043.632568359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 87 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 156.57473754882812\n",
      "Eval_StdReturn : 12.342267990112305\n",
      "Eval_MaxReturn : 174.07386779785156\n",
      "Eval_MinReturn : 126.23008728027344\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 156.89710998535156\n",
      "Train_StdReturn : 19.753253936767578\n",
      "Train_MaxReturn : 200.87991333007812\n",
      "Train_MinReturn : 13.24990463256836\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2644312\n",
      "TimeSinceStart : 1534.8627564907074\n",
      "Critic_Loss : 1.013803482055664\n",
      "Actor_Loss : -3790.1279296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 88 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 139.4698028564453\n",
      "Eval_StdReturn : 36.979217529296875\n",
      "Eval_MaxReturn : 181.93251037597656\n",
      "Eval_MinReturn : 69.764404296875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 158.75128173828125\n",
      "Train_StdReturn : 20.684768676757812\n",
      "Train_MaxReturn : 198.6396484375\n",
      "Train_MinReturn : 40.304534912109375\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2674361\n",
      "TimeSinceStart : 1552.1343879699707\n",
      "Critic_Loss : 1.0804064273834229\n",
      "Actor_Loss : -3228.0234375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 89 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 156.98226928710938\n",
      "Eval_StdReturn : 16.26839828491211\n",
      "Eval_MaxReturn : 189.47164916992188\n",
      "Eval_MinReturn : 132.55194091796875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 156.95648193359375\n",
      "Train_StdReturn : 24.88768768310547\n",
      "Train_MaxReturn : 204.63587951660156\n",
      "Train_MinReturn : 47.85140609741211\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2704410\n",
      "TimeSinceStart : 1569.6803121566772\n",
      "Critic_Loss : 1.2153986692428589\n",
      "Actor_Loss : -2803.246826171875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 90 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 149.2534637451172\n",
      "Eval_StdReturn : 25.276775360107422\n",
      "Eval_MaxReturn : 186.82875061035156\n",
      "Eval_MinReturn : 108.29450988769531\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 157.37197875976562\n",
      "Train_StdReturn : 21.525409698486328\n",
      "Train_MaxReturn : 202.3497314453125\n",
      "Train_MinReturn : 46.478721618652344\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2734459\n",
      "TimeSinceStart : 1586.7698469161987\n",
      "Critic_Loss : 1.010618805885315\n",
      "Actor_Loss : -3491.8408203125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 91 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 152.7660675048828\n",
      "Eval_StdReturn : 8.77509593963623\n",
      "Eval_MaxReturn : 160.2433624267578\n",
      "Eval_MinReturn : 129.34359741210938\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 147.93759155273438\n",
      "Train_StdReturn : 17.78831672668457\n",
      "Train_MaxReturn : 187.66769409179688\n",
      "Train_MinReturn : 78.88573455810547\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2764508\n",
      "TimeSinceStart : 1604.0927410125732\n",
      "Critic_Loss : 1.0625098943710327\n",
      "Actor_Loss : -3044.143798828125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 92 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 143.55450439453125\n",
      "Eval_StdReturn : 32.708709716796875\n",
      "Eval_MaxReturn : 167.50125122070312\n",
      "Eval_MinReturn : 71.40680694580078\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 144.68222045898438\n",
      "Train_StdReturn : 15.567950248718262\n",
      "Train_MaxReturn : 186.9797821044922\n",
      "Train_MinReturn : 88.54318237304688\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2794557\n",
      "TimeSinceStart : 1621.9231889247894\n",
      "Critic_Loss : 0.8927397131919861\n",
      "Actor_Loss : -2973.54736328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 93 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.82107543945312\n",
      "Eval_StdReturn : 36.770179748535156\n",
      "Eval_MaxReturn : 168.83578491210938\n",
      "Eval_MinReturn : 36.998260498046875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 147.38748168945312\n",
      "Train_StdReturn : 23.90386199951172\n",
      "Train_MaxReturn : 190.4918670654297\n",
      "Train_MinReturn : 36.1751594543457\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2824606\n",
      "TimeSinceStart : 1639.1361434459686\n",
      "Critic_Loss : 0.9579555988311768\n",
      "Actor_Loss : -2722.0869140625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 94 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 160.9236602783203\n",
      "Eval_StdReturn : 12.706056594848633\n",
      "Eval_MaxReturn : 178.49453735351562\n",
      "Eval_MinReturn : 141.0240020751953\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 155.9031982421875\n",
      "Train_StdReturn : 15.378242492675781\n",
      "Train_MaxReturn : 191.70550537109375\n",
      "Train_MinReturn : 110.15583038330078\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2854655\n",
      "TimeSinceStart : 1656.4511771202087\n",
      "Critic_Loss : 0.9488980770111084\n",
      "Actor_Loss : -4253.431640625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 95 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 155.68099975585938\n",
      "Eval_StdReturn : 16.463762283325195\n",
      "Eval_MaxReturn : 179.07492065429688\n",
      "Eval_MinReturn : 120.19712829589844\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 157.28106689453125\n",
      "Train_StdReturn : 18.92190933227539\n",
      "Train_MaxReturn : 197.3113250732422\n",
      "Train_MinReturn : 68.14766693115234\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2884704\n",
      "TimeSinceStart : 1673.5236129760742\n",
      "Critic_Loss : 1.1022776365280151\n",
      "Actor_Loss : -2398.44189453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 96 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 150.8871307373047\n",
      "Eval_StdReturn : 20.259559631347656\n",
      "Eval_MaxReturn : 179.82008361816406\n",
      "Eval_MinReturn : 104.86312866210938\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 154.59141540527344\n",
      "Train_StdReturn : 20.77998924255371\n",
      "Train_MaxReturn : 192.01385498046875\n",
      "Train_MinReturn : 49.26831817626953\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2914753\n",
      "TimeSinceStart : 1690.9525985717773\n",
      "Critic_Loss : 1.0363248586654663\n",
      "Actor_Loss : -2933.45458984375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 97 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 132.5847930908203\n",
      "Eval_StdReturn : 26.504655838012695\n",
      "Eval_MaxReturn : 172.84024047851562\n",
      "Eval_MinReturn : 93.90292358398438\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 146.68226623535156\n",
      "Train_StdReturn : 19.10122299194336\n",
      "Train_MaxReturn : 196.48651123046875\n",
      "Train_MinReturn : 84.3576431274414\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2944802\n",
      "TimeSinceStart : 1708.4088082313538\n",
      "Critic_Loss : 1.0629611015319824\n",
      "Actor_Loss : -2326.5166015625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 98 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.95883178710938\n",
      "Eval_StdReturn : 10.422518730163574\n",
      "Eval_MaxReturn : 154.69361877441406\n",
      "Eval_MinReturn : 117.1094741821289\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 136.1795654296875\n",
      "Train_StdReturn : 22.624530792236328\n",
      "Train_MaxReturn : 183.25747680664062\n",
      "Train_MinReturn : 50.69851303100586\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 2974851\n",
      "TimeSinceStart : 1725.5574395656586\n",
      "Critic_Loss : 0.9408597946166992\n",
      "Actor_Loss : -2240.79150390625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 99 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 137.41903686523438\n",
      "Eval_StdReturn : 17.554773330688477\n",
      "Eval_MaxReturn : 164.74285888671875\n",
      "Eval_MinReturn : 118.84382629394531\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 136.9001007080078\n",
      "Train_StdReturn : 23.90389060974121\n",
      "Train_MaxReturn : 192.22500610351562\n",
      "Train_MinReturn : 35.43808364868164\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3004900\n",
      "TimeSinceStart : 1742.8896481990814\n",
      "Critic_Loss : 1.1991791725158691\n",
      "Actor_Loss : -3108.58349609375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 100 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 150.28468322753906\n",
      "Eval_StdReturn : 11.738526344299316\n",
      "Eval_MaxReturn : 165.4146270751953\n",
      "Eval_MinReturn : 130.6659698486328\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 136.80972290039062\n",
      "Train_StdReturn : 17.92770004272461\n",
      "Train_MaxReturn : 185.22471618652344\n",
      "Train_MinReturn : 70.40291595458984\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3034949\n",
      "TimeSinceStart : 1760.1840617656708\n",
      "Critic_Loss : 0.9331545829772949\n",
      "Actor_Loss : -2627.092529296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 101 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 180.78871154785156\n",
      "Eval_StdReturn : 16.491790771484375\n",
      "Eval_MaxReturn : 206.23089599609375\n",
      "Eval_MinReturn : 150.0156707763672\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 159.45533752441406\n",
      "Train_StdReturn : 22.851177215576172\n",
      "Train_MaxReturn : 211.6748046875\n",
      "Train_MinReturn : 22.137788772583008\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3064998\n",
      "TimeSinceStart : 1777.087471961975\n",
      "Critic_Loss : 1.019253134727478\n",
      "Actor_Loss : -2387.498291015625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 102 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 157.04095458984375\n",
      "Eval_StdReturn : 22.430688858032227\n",
      "Eval_MaxReturn : 191.7462615966797\n",
      "Eval_MinReturn : 120.58226013183594\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 167.44496154785156\n",
      "Train_StdReturn : 24.490205764770508\n",
      "Train_MaxReturn : 211.15591430664062\n",
      "Train_MinReturn : 14.89321517944336\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3095047\n",
      "TimeSinceStart : 1794.591937303543\n",
      "Critic_Loss : 1.204854130744934\n",
      "Actor_Loss : -2372.45703125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 103 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 146.55056762695312\n",
      "Eval_StdReturn : 19.43450164794922\n",
      "Eval_MaxReturn : 189.59231567382812\n",
      "Eval_MinReturn : 114.74264526367188\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 170.61154174804688\n",
      "Train_StdReturn : 23.135299682617188\n",
      "Train_MaxReturn : 217.30502319335938\n",
      "Train_MinReturn : 49.090938568115234\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3125096\n",
      "TimeSinceStart : 1811.924213886261\n",
      "Critic_Loss : 1.295520544052124\n",
      "Actor_Loss : -1860.203857421875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 104 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 153.3065643310547\n",
      "Eval_StdReturn : 23.76190948486328\n",
      "Eval_MaxReturn : 183.19973754882812\n",
      "Eval_MinReturn : 99.49331665039062\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 144.6770782470703\n",
      "Train_StdReturn : 29.208019256591797\n",
      "Train_MaxReturn : 202.14932250976562\n",
      "Train_MinReturn : 10.7285737991333\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3155145\n",
      "TimeSinceStart : 1829.0076580047607\n",
      "Critic_Loss : 1.7948706150054932\n",
      "Actor_Loss : -1625.6246337890625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 105 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 149.0402069091797\n",
      "Eval_StdReturn : 20.485733032226562\n",
      "Eval_MaxReturn : 190.86932373046875\n",
      "Eval_MinReturn : 121.54423522949219\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 143.62144470214844\n",
      "Train_StdReturn : 24.036273956298828\n",
      "Train_MaxReturn : 194.37518310546875\n",
      "Train_MinReturn : 53.689693450927734\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3185194\n",
      "TimeSinceStart : 1846.6158163547516\n",
      "Critic_Loss : 1.0218605995178223\n",
      "Actor_Loss : -2176.09765625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 106 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 157.54736328125\n",
      "Eval_StdReturn : 9.535719871520996\n",
      "Eval_MaxReturn : 168.79782104492188\n",
      "Eval_MinReturn : 142.231201171875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 151.9893798828125\n",
      "Train_StdReturn : 21.20145606994629\n",
      "Train_MaxReturn : 193.87484741210938\n",
      "Train_MinReturn : 61.51966094970703\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3215243\n",
      "TimeSinceStart : 1864.0131976604462\n",
      "Critic_Loss : 0.9624250531196594\n",
      "Actor_Loss : -2622.510986328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 107 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 154.98141479492188\n",
      "Eval_StdReturn : 9.987921714782715\n",
      "Eval_MaxReturn : 177.34371948242188\n",
      "Eval_MinReturn : 142.49684143066406\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 154.44956970214844\n",
      "Train_StdReturn : 16.457748413085938\n",
      "Train_MaxReturn : 196.06504821777344\n",
      "Train_MinReturn : 102.23056030273438\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3245292\n",
      "TimeSinceStart : 1881.066243171692\n",
      "Critic_Loss : 0.9583476781845093\n",
      "Actor_Loss : -2173.293701171875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 108 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 160.05279541015625\n",
      "Eval_StdReturn : 16.893232345581055\n",
      "Eval_MaxReturn : 187.49139404296875\n",
      "Eval_MinReturn : 123.2703857421875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 157.04586791992188\n",
      "Train_StdReturn : 19.036331176757812\n",
      "Train_MaxReturn : 199.31314086914062\n",
      "Train_MinReturn : 71.26190185546875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3275341\n",
      "TimeSinceStart : 1898.0532133579254\n",
      "Critic_Loss : 1.028125524520874\n",
      "Actor_Loss : -2373.78857421875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 109 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 149.86795043945312\n",
      "Eval_StdReturn : 12.682750701904297\n",
      "Eval_MaxReturn : 168.88034057617188\n",
      "Eval_MinReturn : 124.52951049804688\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 157.02908325195312\n",
      "Train_StdReturn : 19.710662841796875\n",
      "Train_MaxReturn : 209.45172119140625\n",
      "Train_MinReturn : 41.6170654296875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3305390\n",
      "TimeSinceStart : 1915.2678911685944\n",
      "Critic_Loss : 1.1182547807693481\n",
      "Actor_Loss : -2064.35791015625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 110 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 149.5054473876953\n",
      "Eval_StdReturn : 13.48631477355957\n",
      "Eval_MaxReturn : 170.7205810546875\n",
      "Eval_MinReturn : 122.4109115600586\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 147.7709503173828\n",
      "Train_StdReturn : 20.16657066345215\n",
      "Train_MaxReturn : 200.1627197265625\n",
      "Train_MinReturn : 44.16116714477539\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3335439\n",
      "TimeSinceStart : 1932.475817680359\n",
      "Critic_Loss : 0.9861168265342712\n",
      "Actor_Loss : -2699.3681640625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 111 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 157.27365112304688\n",
      "Eval_StdReturn : 14.065929412841797\n",
      "Eval_MaxReturn : 185.85780334472656\n",
      "Eval_MinReturn : 136.64071655273438\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 143.06991577148438\n",
      "Train_StdReturn : 23.715587615966797\n",
      "Train_MaxReturn : 194.00027465820312\n",
      "Train_MinReturn : 1.7321109771728516\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3365488\n",
      "TimeSinceStart : 1949.5964722633362\n",
      "Critic_Loss : 0.9143460392951965\n",
      "Actor_Loss : -2360.66943359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 112 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 151.22848510742188\n",
      "Eval_StdReturn : 19.78802490234375\n",
      "Eval_MaxReturn : 181.74563598632812\n",
      "Eval_MinReturn : 109.43413543701172\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 146.17942810058594\n",
      "Train_StdReturn : 24.383697509765625\n",
      "Train_MaxReturn : 188.4120330810547\n",
      "Train_MinReturn : 29.426420211791992\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3395537\n",
      "TimeSinceStart : 1966.9424080848694\n",
      "Critic_Loss : 0.8669398427009583\n",
      "Actor_Loss : -2462.04736328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 113 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 158.73089599609375\n",
      "Eval_StdReturn : 7.047281742095947\n",
      "Eval_MaxReturn : 172.89657592773438\n",
      "Eval_MinReturn : 147.70501708984375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 149.56033325195312\n",
      "Train_StdReturn : 21.84113883972168\n",
      "Train_MaxReturn : 189.8284912109375\n",
      "Train_MinReturn : 30.0035343170166\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3425586\n",
      "TimeSinceStart : 1984.316379070282\n",
      "Critic_Loss : 0.9744911193847656\n",
      "Actor_Loss : -2254.8779296875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 114 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 165.85630798339844\n",
      "Eval_StdReturn : 12.741114616394043\n",
      "Eval_MaxReturn : 194.43096923828125\n",
      "Eval_MinReturn : 152.2876739501953\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 156.15924072265625\n",
      "Train_StdReturn : 13.261906623840332\n",
      "Train_MaxReturn : 190.6697540283203\n",
      "Train_MinReturn : 117.65733337402344\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3455635\n",
      "TimeSinceStart : 2001.7209413051605\n",
      "Critic_Loss : 0.8515263199806213\n",
      "Actor_Loss : -2506.8515625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 115 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 162.00241088867188\n",
      "Eval_StdReturn : 12.805082321166992\n",
      "Eval_MaxReturn : 188.78839111328125\n",
      "Eval_MinReturn : 144.0929412841797\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 157.8883514404297\n",
      "Train_StdReturn : 17.081375122070312\n",
      "Train_MaxReturn : 198.50140380859375\n",
      "Train_MinReturn : 11.930071830749512\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3485684\n",
      "TimeSinceStart : 2019.0484683513641\n",
      "Critic_Loss : 0.9508076906204224\n",
      "Actor_Loss : -2589.43212890625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 116 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 164.5152587890625\n",
      "Eval_StdReturn : 10.842745780944824\n",
      "Eval_MaxReturn : 184.96524047851562\n",
      "Eval_MinReturn : 145.8797607421875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 159.64157104492188\n",
      "Train_StdReturn : 18.439592361450195\n",
      "Train_MaxReturn : 194.23304748535156\n",
      "Train_MinReturn : 42.71437072753906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3515733\n",
      "TimeSinceStart : 2036.49480843544\n",
      "Critic_Loss : 0.9543649554252625\n",
      "Actor_Loss : -2663.41748046875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 117 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 168.50576782226562\n",
      "Eval_StdReturn : 19.16065788269043\n",
      "Eval_MaxReturn : 199.19992065429688\n",
      "Eval_MinReturn : 140.53271484375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 164.44786071777344\n",
      "Train_StdReturn : 14.838725090026855\n",
      "Train_MaxReturn : 201.56103515625\n",
      "Train_MinReturn : 107.892333984375\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3545782\n",
      "TimeSinceStart : 2053.679662704468\n",
      "Critic_Loss : 1.2824193239212036\n",
      "Actor_Loss : -2483.669921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 118 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 180.1326446533203\n",
      "Eval_StdReturn : 12.80726432800293\n",
      "Eval_MaxReturn : 198.5506591796875\n",
      "Eval_MinReturn : 156.20925903320312\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 167.7382354736328\n",
      "Train_StdReturn : 20.349918365478516\n",
      "Train_MaxReturn : 211.5425567626953\n",
      "Train_MinReturn : 61.425601959228516\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3575831\n",
      "TimeSinceStart : 2071.134117603302\n",
      "Critic_Loss : 1.0481276512145996\n",
      "Actor_Loss : -2037.315185546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 119 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 171.50534057617188\n",
      "Eval_StdReturn : 17.18453025817871\n",
      "Eval_MaxReturn : 200.17059326171875\n",
      "Eval_MinReturn : 130.37139892578125\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 174.3722381591797\n",
      "Train_StdReturn : 18.17559814453125\n",
      "Train_MaxReturn : 224.27853393554688\n",
      "Train_MinReturn : 91.20689392089844\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3605880\n",
      "TimeSinceStart : 2088.8217527866364\n",
      "Critic_Loss : 1.2340242862701416\n",
      "Actor_Loss : -2328.473388671875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 120 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 145.57888793945312\n",
      "Eval_StdReturn : 43.77056884765625\n",
      "Eval_MaxReturn : 182.4545135498047\n",
      "Eval_MinReturn : 20.312177658081055\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 171.9744873046875\n",
      "Train_StdReturn : 18.803403854370117\n",
      "Train_MaxReturn : 213.5869140625\n",
      "Train_MinReturn : 87.59090423583984\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3635929\n",
      "TimeSinceStart : 2106.0792849063873\n",
      "Critic_Loss : 1.3344712257385254\n",
      "Actor_Loss : -1849.9783935546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 121 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 171.7467041015625\n",
      "Eval_StdReturn : 16.991870880126953\n",
      "Eval_MaxReturn : 192.12799072265625\n",
      "Eval_MinReturn : 134.82220458984375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 163.72996520996094\n",
      "Train_StdReturn : 23.566036224365234\n",
      "Train_MaxReturn : 211.30772399902344\n",
      "Train_MinReturn : 69.19151306152344\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3665978\n",
      "TimeSinceStart : 2123.279217481613\n",
      "Critic_Loss : 1.249306321144104\n",
      "Actor_Loss : -2044.322998046875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 122 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.5626983642578\n",
      "Eval_StdReturn : 22.228910446166992\n",
      "Eval_MaxReturn : 195.87640380859375\n",
      "Eval_MinReturn : 125.37542724609375\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 152.67001342773438\n",
      "Train_StdReturn : 30.854990005493164\n",
      "Train_MaxReturn : 201.03590393066406\n",
      "Train_MinReturn : -36.8758544921875\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3696027\n",
      "TimeSinceStart : 2140.3593809604645\n",
      "Critic_Loss : 1.3019750118255615\n",
      "Actor_Loss : -1648.3184814453125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 123 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 129.23057556152344\n",
      "Eval_StdReturn : 24.639814376831055\n",
      "Eval_MaxReturn : 155.5594940185547\n",
      "Eval_MinReturn : 70.41193389892578\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 149.13015747070312\n",
      "Train_StdReturn : 27.896991729736328\n",
      "Train_MaxReturn : 215.3917694091797\n",
      "Train_MinReturn : 10.626819610595703\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3726076\n",
      "TimeSinceStart : 2157.3984541893005\n",
      "Critic_Loss : 1.234432339668274\n",
      "Actor_Loss : -1344.260986328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 124 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 134.00738525390625\n",
      "Eval_StdReturn : 17.33760643005371\n",
      "Eval_MaxReturn : 154.97189331054688\n",
      "Eval_MinReturn : 108.34654235839844\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 142.56285095214844\n",
      "Train_StdReturn : 27.842370986938477\n",
      "Train_MaxReturn : 201.7426300048828\n",
      "Train_MinReturn : 16.989593505859375\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3756125\n",
      "TimeSinceStart : 2174.8046073913574\n",
      "Critic_Loss : 1.1831125020980835\n",
      "Actor_Loss : -1346.71533203125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 125 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 161.60452270507812\n",
      "Eval_StdReturn : 14.289287567138672\n",
      "Eval_MaxReturn : 179.03549194335938\n",
      "Eval_MinReturn : 133.0769805908203\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 145.59930419921875\n",
      "Train_StdReturn : 21.6268310546875\n",
      "Train_MaxReturn : 196.80555725097656\n",
      "Train_MinReturn : 64.12765502929688\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3786174\n",
      "TimeSinceStart : 2192.2095472812653\n",
      "Critic_Loss : 1.0571616888046265\n",
      "Actor_Loss : -2320.796875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 126 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 156.5809326171875\n",
      "Eval_StdReturn : 22.58163833618164\n",
      "Eval_MaxReturn : 182.7242889404297\n",
      "Eval_MinReturn : 98.00956726074219\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 151.9362030029297\n",
      "Train_StdReturn : 20.62126922607422\n",
      "Train_MaxReturn : 195.68177795410156\n",
      "Train_MinReturn : 72.92312622070312\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3816223\n",
      "TimeSinceStart : 2209.5410647392273\n",
      "Critic_Loss : 1.1163043975830078\n",
      "Actor_Loss : -1583.5972900390625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 127 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 162.3253936767578\n",
      "Eval_StdReturn : 12.369633674621582\n",
      "Eval_MaxReturn : 180.44357299804688\n",
      "Eval_MinReturn : 139.70730590820312\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 159.541748046875\n",
      "Train_StdReturn : 20.451873779296875\n",
      "Train_MaxReturn : 207.72280883789062\n",
      "Train_MinReturn : 36.08491516113281\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3846272\n",
      "TimeSinceStart : 2227.0175466537476\n",
      "Critic_Loss : 0.9822890758514404\n",
      "Actor_Loss : -1903.430419921875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 128 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 141.72084045410156\n",
      "Eval_StdReturn : 33.53828811645508\n",
      "Eval_MaxReturn : 189.51016235351562\n",
      "Eval_MinReturn : 89.29126739501953\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 164.12158203125\n",
      "Train_StdReturn : 19.20464324951172\n",
      "Train_MaxReturn : 216.71832275390625\n",
      "Train_MinReturn : 60.46027755737305\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3876321\n",
      "TimeSinceStart : 2244.581615447998\n",
      "Critic_Loss : 1.0610064268112183\n",
      "Actor_Loss : -2034.1048583984375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 129 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 119.8723373413086\n",
      "Eval_StdReturn : 45.82617950439453\n",
      "Eval_MaxReturn : 169.28152465820312\n",
      "Eval_MinReturn : 29.028270721435547\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 126.08311462402344\n",
      "Train_StdReturn : 56.26191711425781\n",
      "Train_MaxReturn : 200.73170471191406\n",
      "Train_MinReturn : 0.5462417602539062\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3906370\n",
      "TimeSinceStart : 2263.8873331546783\n",
      "Critic_Loss : 1.0559898614883423\n",
      "Actor_Loss : -1836.66455078125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 130 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 135.92617797851562\n",
      "Eval_StdReturn : 15.91934585571289\n",
      "Eval_MaxReturn : 164.81382751464844\n",
      "Eval_MinReturn : 114.36565399169922\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 102.65850067138672\n",
      "Train_StdReturn : 59.93108367919922\n",
      "Train_MaxReturn : 196.2227325439453\n",
      "Train_MinReturn : -19.677967071533203\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3936419\n",
      "TimeSinceStart : 2282.1835064888\n",
      "Critic_Loss : 0.9439066648483276\n",
      "Actor_Loss : -1740.0235595703125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 131 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 129.17935180664062\n",
      "Eval_StdReturn : 15.438584327697754\n",
      "Eval_MaxReturn : 155.08157348632812\n",
      "Eval_MinReturn : 92.77410888671875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 133.71763610839844\n",
      "Train_StdReturn : 19.944976806640625\n",
      "Train_MaxReturn : 178.98388671875\n",
      "Train_MinReturn : 42.51721954345703\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3966468\n",
      "TimeSinceStart : 2299.9418020248413\n",
      "Critic_Loss : 0.8674482703208923\n",
      "Actor_Loss : -2396.298828125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 132 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 157.90817260742188\n",
      "Eval_StdReturn : 13.995079040527344\n",
      "Eval_MaxReturn : 177.07882690429688\n",
      "Eval_MinReturn : 137.09799194335938\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 128.91375732421875\n",
      "Train_StdReturn : 21.343795776367188\n",
      "Train_MaxReturn : 167.79615783691406\n",
      "Train_MinReturn : 11.350016593933105\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 3996517\n",
      "TimeSinceStart : 2317.8317811489105\n",
      "Critic_Loss : 0.8106587529182434\n",
      "Actor_Loss : -1771.6995849609375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 133 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.32749938964844\n",
      "Eval_StdReturn : 12.141526222229004\n",
      "Eval_MaxReturn : 175.19638061523438\n",
      "Eval_MinReturn : 134.639404296875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 150.64549255371094\n",
      "Train_StdReturn : 18.730918884277344\n",
      "Train_MaxReturn : 199.15841674804688\n",
      "Train_MinReturn : 89.93067932128906\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4026566\n",
      "TimeSinceStart : 2335.8285093307495\n",
      "Critic_Loss : 0.9357020854949951\n",
      "Actor_Loss : -2288.69677734375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 134 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 166.2760467529297\n",
      "Eval_StdReturn : 12.52120590209961\n",
      "Eval_MaxReturn : 183.28219604492188\n",
      "Eval_MinReturn : 147.00204467773438\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 162.94667053222656\n",
      "Train_StdReturn : 27.20269012451172\n",
      "Train_MaxReturn : 225.20147705078125\n",
      "Train_MinReturn : 37.22850036621094\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4056615\n",
      "TimeSinceStart : 2353.783292531967\n",
      "Critic_Loss : 1.0967656373977661\n",
      "Actor_Loss : -1892.314208984375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 135 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 156.9065704345703\n",
      "Eval_StdReturn : 27.645347595214844\n",
      "Eval_MaxReturn : 194.78433227539062\n",
      "Eval_MinReturn : 93.39067077636719\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 161.89645385742188\n",
      "Train_StdReturn : 28.134109497070312\n",
      "Train_MaxReturn : 210.80850219726562\n",
      "Train_MinReturn : 37.527366638183594\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4086664\n",
      "TimeSinceStart : 2371.1418294906616\n",
      "Critic_Loss : 1.1195409297943115\n",
      "Actor_Loss : -2110.16943359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 136 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 173.69349670410156\n",
      "Eval_StdReturn : 12.032386779785156\n",
      "Eval_MaxReturn : 197.68154907226562\n",
      "Eval_MinReturn : 158.27334594726562\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 165.24351501464844\n",
      "Train_StdReturn : 24.30972671508789\n",
      "Train_MaxReturn : 223.28482055664062\n",
      "Train_MinReturn : 63.196292877197266\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4116713\n",
      "TimeSinceStart : 2388.6223146915436\n",
      "Critic_Loss : 1.1829241514205933\n",
      "Actor_Loss : -1305.4241943359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 137 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 149.08692932128906\n",
      "Eval_StdReturn : 27.13719367980957\n",
      "Eval_MaxReturn : 186.75161743164062\n",
      "Eval_MinReturn : 83.41928100585938\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 160.43055725097656\n",
      "Train_StdReturn : 29.062564849853516\n",
      "Train_MaxReturn : 212.38949584960938\n",
      "Train_MinReturn : 40.98999786376953\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4146762\n",
      "TimeSinceStart : 2406.0223982334137\n",
      "Critic_Loss : 1.1875046491622925\n",
      "Actor_Loss : -2156.455810546875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 138 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 175.08285522460938\n",
      "Eval_StdReturn : 17.97368621826172\n",
      "Eval_MaxReturn : 196.348388671875\n",
      "Eval_MinReturn : 136.8870086669922\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 165.59629821777344\n",
      "Train_StdReturn : 21.465003967285156\n",
      "Train_MaxReturn : 207.442138671875\n",
      "Train_MinReturn : 46.472957611083984\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4176811\n",
      "TimeSinceStart : 2423.5148956775665\n",
      "Critic_Loss : 1.1331830024719238\n",
      "Actor_Loss : -1803.777099609375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 139 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 178.30662536621094\n",
      "Eval_StdReturn : 12.795477867126465\n",
      "Eval_MaxReturn : 198.36619567871094\n",
      "Eval_MinReturn : 154.39755249023438\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 179.60060119628906\n",
      "Train_StdReturn : 18.736560821533203\n",
      "Train_MaxReturn : 220.79486083984375\n",
      "Train_MinReturn : 85.0723648071289\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4206860\n",
      "TimeSinceStart : 2441.109874486923\n",
      "Critic_Loss : 1.1557379961013794\n",
      "Actor_Loss : -2340.323486328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 140 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 181.7815704345703\n",
      "Eval_StdReturn : 12.07016372680664\n",
      "Eval_MaxReturn : 204.21444702148438\n",
      "Eval_MinReturn : 166.48818969726562\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 178.9642791748047\n",
      "Train_StdReturn : 16.737340927124023\n",
      "Train_MaxReturn : 228.35025024414062\n",
      "Train_MinReturn : 126.68801879882812\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4236909\n",
      "TimeSinceStart : 2458.6926140785217\n",
      "Critic_Loss : 1.1317017078399658\n",
      "Actor_Loss : -2509.18798828125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 141 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 159.70773315429688\n",
      "Eval_StdReturn : 32.22531509399414\n",
      "Eval_MaxReturn : 207.531982421875\n",
      "Eval_MinReturn : 96.1607437133789\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 180.1880645751953\n",
      "Train_StdReturn : 21.82818031311035\n",
      "Train_MaxReturn : 217.9161376953125\n",
      "Train_MinReturn : 15.251842498779297\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4266958\n",
      "TimeSinceStart : 2475.9891176223755\n",
      "Critic_Loss : 1.2126116752624512\n",
      "Actor_Loss : -2073.99072265625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 142 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 185.27493286132812\n",
      "Eval_StdReturn : 14.931174278259277\n",
      "Eval_MaxReturn : 203.04518127441406\n",
      "Eval_MinReturn : 148.62049865722656\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 174.2757568359375\n",
      "Train_StdReturn : 25.34322738647461\n",
      "Train_MaxReturn : 225.8583221435547\n",
      "Train_MinReturn : 20.762317657470703\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4297007\n",
      "TimeSinceStart : 2493.4816484451294\n",
      "Critic_Loss : 1.3210302591323853\n",
      "Actor_Loss : -1947.1868896484375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 143 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 169.48410034179688\n",
      "Eval_StdReturn : 8.106781005859375\n",
      "Eval_MaxReturn : 180.86036682128906\n",
      "Eval_MinReturn : 156.80474853515625\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 169.09744262695312\n",
      "Train_StdReturn : 32.18326950073242\n",
      "Train_MaxReturn : 222.92816162109375\n",
      "Train_MinReturn : 2.904407501220703\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4327056\n",
      "TimeSinceStart : 2510.5240120887756\n",
      "Critic_Loss : 1.3053874969482422\n",
      "Actor_Loss : -1605.678955078125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 144 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 163.5152587890625\n",
      "Eval_StdReturn : 14.850406646728516\n",
      "Eval_MaxReturn : 189.3385009765625\n",
      "Eval_MinReturn : 138.92742919921875\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 165.75372314453125\n",
      "Train_StdReturn : 26.60498809814453\n",
      "Train_MaxReturn : 208.48373413085938\n",
      "Train_MinReturn : 32.290794372558594\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4357105\n",
      "TimeSinceStart : 2527.697246313095\n",
      "Critic_Loss : 1.1740822792053223\n",
      "Actor_Loss : -1448.103759765625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 145 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 170.7764129638672\n",
      "Eval_StdReturn : 18.45806884765625\n",
      "Eval_MaxReturn : 195.8526611328125\n",
      "Eval_MinReturn : 139.93812561035156\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 167.96153259277344\n",
      "Train_StdReturn : 18.73198699951172\n",
      "Train_MaxReturn : 218.56773376464844\n",
      "Train_MinReturn : 57.1480598449707\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4387154\n",
      "TimeSinceStart : 2544.9817576408386\n",
      "Critic_Loss : 1.0616481304168701\n",
      "Actor_Loss : -1827.8682861328125\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 146 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 189.5286865234375\n",
      "Eval_StdReturn : 14.471610069274902\n",
      "Eval_MaxReturn : 211.59588623046875\n",
      "Eval_MinReturn : 171.0843048095703\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 173.10862731933594\n",
      "Train_StdReturn : 21.884016036987305\n",
      "Train_MaxReturn : 225.12689208984375\n",
      "Train_MinReturn : 63.1554069519043\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4417203\n",
      "TimeSinceStart : 2562.3905489444733\n",
      "Critic_Loss : 1.213827133178711\n",
      "Actor_Loss : -2161.519775390625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 147 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 193.95628356933594\n",
      "Eval_StdReturn : 15.633794784545898\n",
      "Eval_MaxReturn : 213.6809844970703\n",
      "Eval_MinReturn : 156.15003967285156\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 180.95465087890625\n",
      "Train_StdReturn : 20.779748916625977\n",
      "Train_MaxReturn : 223.66592407226562\n",
      "Train_MinReturn : 90.72618103027344\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4447252\n",
      "TimeSinceStart : 2579.830950975418\n",
      "Critic_Loss : 1.1870726346969604\n",
      "Actor_Loss : -2272.44140625\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 148 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 175.92733764648438\n",
      "Eval_StdReturn : 16.320398330688477\n",
      "Eval_MaxReturn : 204.29283142089844\n",
      "Eval_MinReturn : 141.03482055664062\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 188.185302734375\n",
      "Train_StdReturn : 23.571949005126953\n",
      "Train_MaxReturn : 237.17770385742188\n",
      "Train_MinReturn : 26.2537784576416\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4477301\n",
      "TimeSinceStart : 2597.293249607086\n",
      "Critic_Loss : 1.4228161573410034\n",
      "Actor_Loss : -2129.695068359375\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 149 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "At timestep:     30049 / 30000\n",
      "Training agent...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 182.6451416015625\n",
      "Eval_StdReturn : 16.247568130493164\n",
      "Eval_MaxReturn : 210.62899780273438\n",
      "Eval_MinReturn : 149.84390258789062\n",
      "Eval_AverageEpLen : 151.0\n",
      "Train_AverageReturn : 180.74305725097656\n",
      "Train_StdReturn : 25.208223342895508\n",
      "Train_MaxReturn : 228.5779266357422\n",
      "Train_MinReturn : 51.19797897338867\n",
      "Train_AverageEpLen : 151.0\n",
      "Train_EnvstepsSoFar : 4507350\n",
      "TimeSinceStart : 2614.575474023819\n",
      "Critic_Loss : 1.3330912590026855\n",
      "Actor_Loss : -1944.89013671875\n",
      "Initial_DataCollection_AverageReturn : -89.08087921142578\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python ./run_hw3_actor_critic.py  --env_name HalfCheetah-v2 --ep_len 150 --discount 0.90 \\\n",
    "--scalar_log_freq 1 -n 150 -l 2 -s 32 -b 30000 -eb 1500 \\\n",
    "-lr 0.02 --exp_name q5_10_10 -ntu 10 -ngsptu 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
