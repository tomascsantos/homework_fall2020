{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_ant_Ant-v2_14-09-2020_16-31-33\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4719.7041015625\n",
      "Eval_StdReturn : 99.9489974975586\n",
      "Eval_MaxReturn : 4868.5830078125\n",
      "Eval_MinReturn : 4563.48291015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4713.6533203125\n",
      "Train_StdReturn : 12.196533203125\n",
      "Train_MaxReturn : 4725.849609375\n",
      "Train_MinReturn : 4701.45654296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 58.42513728141785\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Ant.pkl \\\n",
    "--env_name Ant-v2 --exp_name bc_ant --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Ant-v2.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/dagger_dagger_ant_Ant-v2_14-09-2020_19-29-59\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Ant.pkl\n",
      "obs (1, 111) (1, 111)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4719.7041015625\n",
      "Eval_StdReturn : 99.9489974975586\n",
      "Eval_MaxReturn : 4868.5830078125\n",
      "Eval_MinReturn : 4563.48291015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4713.6533203125\n",
      "Train_StdReturn : 12.196533203125\n",
      "Train_MaxReturn : 4725.849609375\n",
      "Train_MinReturn : 4701.45654296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 59.508328437805176\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4738.12890625\n",
      "Eval_StdReturn : 74.01468658447266\n",
      "Eval_MaxReturn : 4809.4228515625\n",
      "Eval_MinReturn : 4612.65966796875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4804.29296875\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4804.29296875\n",
      "Train_MinReturn : 4804.29296875\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 1000\n",
      "TimeSinceStart : 129.42536616325378\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4805.0205078125\n",
      "Eval_StdReturn : 98.07062530517578\n",
      "Eval_MaxReturn : 4935.046875\n",
      "Eval_MinReturn : 4640.369140625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4833.3173828125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4833.3173828125\n",
      "Train_MinReturn : 4833.3173828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 2000\n",
      "TimeSinceStart : 203.06596302986145\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4608.8935546875\n",
      "Eval_StdReturn : 56.81592559814453\n",
      "Eval_MaxReturn : 4697.14501953125\n",
      "Eval_MinReturn : 4518.30419921875\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4575.0478515625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4575.0478515625\n",
      "Train_MinReturn : 4575.0478515625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 3000\n",
      "TimeSinceStart : 276.1405942440033\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 4715.4375\n",
      "Eval_StdReturn : 85.0668716430664\n",
      "Eval_MaxReturn : 4864.2314453125\n",
      "Eval_MinReturn : 4622.353515625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 4565.759765625\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 4565.759765625\n",
      "Train_MinReturn : 4565.759765625\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 4000\n",
      "TimeSinceStart : 348.54553866386414\n",
      "Initial_DataCollection_AverageReturn : 4713.6533203125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Ant.pkl \\\n",
    "--env_name Ant-v2 --exp_name dagger_ant --n_iter 5 \\\n",
    "--do_dagger --expert_data ../expert_data/expert_data_Ant-v2.pkl \\\n",
    "--video_log_freq -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_human_Humanoid-v2_14-09-2020_11-33-50\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 269.69244384765625\n",
      "Eval_StdReturn : 12.920839309692383\n",
      "Eval_MaxReturn : 298.03424072265625\n",
      "Eval_MinReturn : 244.2460174560547\n",
      "Eval_AverageEpLen : 49.38095238095238\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 34.46126890182495\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Humanoid.pkl \\\n",
    "--env_name Humanoid-v2 --exp_name bc_human --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Humanoid-v2.pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/dagger_bc_human_Humanoid-v2_14-09-2020_11-35-50\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Humanoid.pkl\n",
      "obs (1, 376) (1, 376)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 269.69244384765625\n",
      "Eval_StdReturn : 12.920839309692383\n",
      "Eval_MaxReturn : 298.03424072265625\n",
      "Eval_MinReturn : 244.2460174560547\n",
      "Eval_AverageEpLen : 49.38095238095238\n",
      "Train_AverageReturn : 10344.517578125\n",
      "Train_StdReturn : 20.9814453125\n",
      "Train_MaxReturn : 10365.4990234375\n",
      "Train_MinReturn : 10323.5361328125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 34.69640851020813\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 358.47412109375\n",
      "Eval_StdReturn : 88.8533935546875\n",
      "Eval_MaxReturn : 543.5570068359375\n",
      "Eval_MinReturn : 228.78477478027344\n",
      "Eval_AverageEpLen : 69.46666666666667\n",
      "Train_AverageReturn : 267.37530517578125\n",
      "Train_StdReturn : 12.149537086486816\n",
      "Train_MaxReturn : 290.7381286621094\n",
      "Train_MinReturn : 242.67294311523438\n",
      "Train_AverageEpLen : 49.0\n",
      "Train_EnvstepsSoFar : 1029\n",
      "TimeSinceStart : 100.99987006187439\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 277.77154541015625\n",
      "Eval_StdReturn : 10.698477745056152\n",
      "Eval_MaxReturn : 302.670654296875\n",
      "Eval_MinReturn : 259.8998107910156\n",
      "Eval_AverageEpLen : 51.25\n",
      "Train_AverageReturn : 350.2350769042969\n",
      "Train_StdReturn : 78.59197998046875\n",
      "Train_MaxReturn : 549.4072875976562\n",
      "Train_MinReturn : 254.7103271484375\n",
      "Train_AverageEpLen : 68.66666666666667\n",
      "Train_EnvstepsSoFar : 2059\n",
      "TimeSinceStart : 166.8427004814148\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 269.84405517578125\n",
      "Eval_StdReturn : 63.929107666015625\n",
      "Eval_MaxReturn : 452.1174011230469\n",
      "Eval_MinReturn : 205.96791076660156\n",
      "Eval_AverageEpLen : 50.85\n",
      "Train_AverageReturn : 279.0166320800781\n",
      "Train_StdReturn : 12.329371452331543\n",
      "Train_MaxReturn : 311.4200744628906\n",
      "Train_MinReturn : 255.54983520507812\n",
      "Train_AverageEpLen : 51.6\n",
      "Train_EnvstepsSoFar : 3091\n",
      "TimeSinceStart : 232.27566599845886\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 353.8238525390625\n",
      "Eval_StdReturn : 22.751102447509766\n",
      "Eval_MaxReturn : 393.23681640625\n",
      "Eval_MinReturn : 316.7642517089844\n",
      "Eval_AverageEpLen : 63.6875\n",
      "Train_AverageReturn : 290.1070251464844\n",
      "Train_StdReturn : 51.6397819519043\n",
      "Train_MaxReturn : 421.82305908203125\n",
      "Train_MinReturn : 215.04644775390625\n",
      "Train_AverageEpLen : 54.1578947368421\n",
      "Train_EnvstepsSoFar : 4120\n",
      "TimeSinceStart : 298.5873031616211\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 370.4897766113281\n",
      "Eval_StdReturn : 65.955078125\n",
      "Eval_MaxReturn : 537.6630249023438\n",
      "Eval_MinReturn : 276.02227783203125\n",
      "Eval_AverageEpLen : 64.8125\n",
      "Train_AverageReturn : 344.41748046875\n",
      "Train_StdReturn : 27.8019962310791\n",
      "Train_MaxReturn : 402.3319091796875\n",
      "Train_MinReturn : 286.41363525390625\n",
      "Train_AverageEpLen : 61.88235294117647\n",
      "Train_EnvstepsSoFar : 5172\n",
      "TimeSinceStart : 365.11086916923523\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 323.95379638671875\n",
      "Eval_StdReturn : 32.72086715698242\n",
      "Eval_MaxReturn : 402.1181640625\n",
      "Eval_MinReturn : 281.4880676269531\n",
      "Eval_AverageEpLen : 57.55555555555556\n",
      "Train_AverageReturn : 337.3688659667969\n",
      "Train_StdReturn : 44.085784912109375\n",
      "Train_MaxReturn : 487.90045166015625\n",
      "Train_MinReturn : 293.1354675292969\n",
      "Train_AverageEpLen : 60.470588235294116\n",
      "Train_EnvstepsSoFar : 6200\n",
      "TimeSinceStart : 430.90983629226685\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 354.09326171875\n",
      "Eval_StdReturn : 31.274446487426758\n",
      "Eval_MaxReturn : 414.17718505859375\n",
      "Eval_MinReturn : 303.56927490234375\n",
      "Eval_AverageEpLen : 62.625\n",
      "Train_AverageReturn : 323.7090759277344\n",
      "Train_StdReturn : 36.96019744873047\n",
      "Train_MaxReturn : 418.0140075683594\n",
      "Train_MinReturn : 258.6935729980469\n",
      "Train_AverageEpLen : 57.55555555555556\n",
      "Train_EnvstepsSoFar : 7236\n",
      "TimeSinceStart : 497.7591826915741\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 410.4820251464844\n",
      "Eval_StdReturn : 72.88915252685547\n",
      "Eval_MaxReturn : 553.28564453125\n",
      "Eval_MinReturn : 311.6127014160156\n",
      "Eval_AverageEpLen : 69.8\n",
      "Train_AverageReturn : 362.6021728515625\n",
      "Train_StdReturn : 35.5299186706543\n",
      "Train_MaxReturn : 427.3341064453125\n",
      "Train_MinReturn : 295.5928955078125\n",
      "Train_AverageEpLen : 63.875\n",
      "Train_EnvstepsSoFar : 8258\n",
      "TimeSinceStart : 565.8071000576019\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 367.968505859375\n",
      "Eval_StdReturn : 101.66085052490234\n",
      "Eval_MaxReturn : 731.7183227539062\n",
      "Eval_MinReturn : 257.6667175292969\n",
      "Eval_AverageEpLen : 63.3125\n",
      "Train_AverageReturn : 390.96661376953125\n",
      "Train_StdReturn : 64.59114074707031\n",
      "Train_MaxReturn : 528.1702270507812\n",
      "Train_MinReturn : 276.6187744140625\n",
      "Train_AverageEpLen : 67.73333333333333\n",
      "Train_EnvstepsSoFar : 9274\n",
      "TimeSinceStart : 632.595769405365\n",
      "Initial_DataCollection_AverageReturn : 10344.517578125\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Humanoid.pkl \\\n",
    "--env_name Humanoid-v2 --exp_name bc_human --n_iter 10 \\\n",
    "--do_dagger --expert_data ../expert_data/expert_data_Humanoid-v2.pkl \\\n",
    "--video_log_freq -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC modify 1 hyperparameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-43-41\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 540.7135009765625\n",
      "Eval_StdReturn : 137.88099670410156\n",
      "Eval_MaxReturn : 715.5106201171875\n",
      "Eval_MinReturn : 339.98956298828125\n",
      "Eval_AverageEpLen : 192.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 133.64268565177917\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'video_log_freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5f64f79a2419>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python run_hw1.py --expert_policy_file ../policies/experts/Hopper.pkl --env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 --expert_data ../expert_data/expert_data_Hopper-v2.pkl --size 10'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m-\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mvideo_log_freq\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'video_log_freq' is not defined"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-46-47\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 403.0879211425781\n",
      "Eval_StdReturn : 39.00880813598633\n",
      "Eval_MaxReturn : 608.7576904296875\n",
      "Eval_MinReturn : 378.8196105957031\n",
      "Eval_AverageEpLen : 163.70967741935485\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 127.87359309196472\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-48-57\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 830.5953369140625\n",
      "Eval_StdReturn : 104.34095001220703\n",
      "Eval_MaxReturn : 1066.2957763671875\n",
      "Eval_MinReturn : 683.8030395507812\n",
      "Eval_AverageEpLen : 284.6666666666667\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 128.6981906890869\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-51-07\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 918.2941284179688\n",
      "Eval_StdReturn : 154.85711669921875\n",
      "Eval_MaxReturn : 1431.818603515625\n",
      "Eval_MinReturn : 815.4390869140625\n",
      "Eval_AverageEpLen : 269.89473684210526\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 129.9939670562744\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-53-18\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1202.1177978515625\n",
      "Eval_StdReturn : 194.42539978027344\n",
      "Eval_MaxReturn : 1834.183349609375\n",
      "Eval_MinReturn : 993.3158569335938\n",
      "Eval_AverageEpLen : 340.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 128.41237330436707\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-55-28\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1258.7733154296875\n",
      "Eval_StdReturn : 115.64126586914062\n",
      "Eval_MaxReturn : 1501.3494873046875\n",
      "Eval_MinReturn : 1135.1298828125\n",
      "Eval_AverageEpLen : 357.1333333333333\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 134.35438871383667\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 60\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-57-43\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 828.2223510742188\n",
      "Eval_StdReturn : 53.603004455566406\n",
      "Eval_MaxReturn : 1002.5972900390625\n",
      "Eval_MinReturn : 794.07177734375\n",
      "Eval_AverageEpLen : 247.85714285714286\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 130.18300342559814\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 70\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_19-59-55\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1078.15283203125\n",
      "Eval_StdReturn : 87.89102172851562\n",
      "Eval_MaxReturn : 1155.6878662109375\n",
      "Eval_MinReturn : 891.336181640625\n",
      "Eval_AverageEpLen : 308.70588235294116\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 130.83448719978333\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 80\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/bc_bc_hopper_Hopper-v2_14-09-2020_20-02-07\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 927.2973022460938\n",
      "Eval_StdReturn : 75.6983413696289\n",
      "Eval_MaxReturn : 1109.190185546875\n",
      "Eval_MinReturn : 870.095703125\n",
      "Eval_AverageEpLen : 269.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 128.7093906402588\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name bc_hopper --n_iter 1 \\\n",
    "--expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--size 90\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dagger Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########################\n",
      "logging outputs to  /home/tomas/Documents/cs285/homework_fall2020/hw1/cs285/scripts/../data/dagger_dagger_hopper_Hopper-v2_14-09-2020_18-39-03\n",
      "########################\n",
      "Loading expert policy from... ../policies/experts/Hopper.pkl\n",
      "obs (1, 11) (1, 11)\n",
      "Done restoring expert policy...\n",
      "\n",
      "\n",
      "********** Iteration 0 ************\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1046.32666015625\n",
      "Eval_StdReturn : 306.4642639160156\n",
      "Eval_MaxReturn : 1695.890380859375\n",
      "Eval_MinReturn : 786.0277099609375\n",
      "Eval_AverageEpLen : 329.0\n",
      "Train_AverageReturn : 3772.67041015625\n",
      "Train_StdReturn : 1.9483642578125\n",
      "Train_MaxReturn : 3774.61865234375\n",
      "Train_MinReturn : 3770.721923828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 0\n",
      "TimeSinceStart : 138.14878845214844\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 1 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 945.1819458007812\n",
      "Eval_StdReturn : 46.915794372558594\n",
      "Eval_MaxReturn : 1045.01708984375\n",
      "Eval_MinReturn : 902.0718994140625\n",
      "Eval_AverageEpLen : 275.57894736842104\n",
      "Train_AverageReturn : 1002.3211669921875\n",
      "Train_StdReturn : 181.30259704589844\n",
      "Train_MaxReturn : 1315.8173828125\n",
      "Train_MinReturn : 886.5050048828125\n",
      "Train_AverageEpLen : 315.75\n",
      "Train_EnvstepsSoFar : 1263\n",
      "TimeSinceStart : 306.08193731307983\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 2 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 2086.4267578125\n",
      "Eval_StdReturn : 525.3326416015625\n",
      "Eval_MaxReturn : 3057.556884765625\n",
      "Eval_MinReturn : 1643.72314453125\n",
      "Eval_AverageEpLen : 580.7777777777778\n",
      "Train_AverageReturn : 908.7122802734375\n",
      "Train_StdReturn : 4.478139400482178\n",
      "Train_MaxReturn : 916.455810546875\n",
      "Train_MinReturn : 905.8795166015625\n",
      "Train_AverageEpLen : 265.5\n",
      "Train_EnvstepsSoFar : 2325\n",
      "TimeSinceStart : 471.7945954799652\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 3 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3753.572265625\n",
      "Eval_StdReturn : 11.26281452178955\n",
      "Eval_MaxReturn : 3770.2275390625\n",
      "Eval_MinReturn : 3736.2041015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1735.177734375\n",
      "Train_StdReturn : 32.58062744140625\n",
      "Train_MaxReturn : 1767.7584228515625\n",
      "Train_MinReturn : 1702.59716796875\n",
      "Train_AverageEpLen : 501.5\n",
      "Train_EnvstepsSoFar : 3328\n",
      "TimeSinceStart : 633.902049779892\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 4 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 1781.979736328125\n",
      "Eval_StdReturn : 193.17575073242188\n",
      "Eval_MaxReturn : 2076.21240234375\n",
      "Eval_MinReturn : 1522.578369140625\n",
      "Eval_AverageEpLen : 484.54545454545456\n",
      "Train_AverageReturn : 3743.942626953125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3743.942626953125\n",
      "Train_MinReturn : 3743.942626953125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 4328\n",
      "TimeSinceStart : 822.0792441368103\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 5 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3768.35498046875\n",
      "Eval_StdReturn : 5.6855597496032715\n",
      "Eval_MaxReturn : 3775.90625\n",
      "Eval_MinReturn : 3760.1552734375\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 1895.669189453125\n",
      "Train_StdReturn : 238.256591796875\n",
      "Train_MaxReturn : 2133.92578125\n",
      "Train_MinReturn : 1657.41259765625\n",
      "Train_AverageEpLen : 509.5\n",
      "Train_EnvstepsSoFar : 5347\n",
      "TimeSinceStart : 967.7744345664978\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 6 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3771.73974609375\n",
      "Eval_StdReturn : 3.8683979511260986\n",
      "Eval_MaxReturn : 3777.361328125\n",
      "Eval_MinReturn : 3767.56103515625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3767.2333984375\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3767.2333984375\n",
      "Train_MinReturn : 3767.2333984375\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 6347\n",
      "TimeSinceStart : 1112.181839466095\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 7 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3773.164794921875\n",
      "Eval_StdReturn : 2.260593891143799\n",
      "Eval_MaxReturn : 3776.264892578125\n",
      "Eval_MinReturn : 3770.0244140625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3607.59814453125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3607.59814453125\n",
      "Train_MinReturn : 3607.59814453125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 7347\n",
      "TimeSinceStart : 1256.5625541210175\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 8 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3770.781982421875\n",
      "Eval_StdReturn : 4.1792707443237305\n",
      "Eval_MaxReturn : 3776.83349609375\n",
      "Eval_MinReturn : 3765.092041015625\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3772.675048828125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3772.675048828125\n",
      "Train_MinReturn : 3772.675048828125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 8347\n",
      "TimeSinceStart : 1399.632211446762\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "********** Iteration 9 ************\n",
      "\n",
      "Collecting data to be used for training...\n",
      "\n",
      "Relabelling collected observations with labels from an expert policy...\n",
      "\n",
      "Training agent using sampled data from replay buffer...\n",
      "\n",
      "Beginning logging procedure...\n",
      "\n",
      "Collecting data for eval...\n",
      "Eval_AverageReturn : 3782.703125\n",
      "Eval_StdReturn : 3.594449520111084\n",
      "Eval_MaxReturn : 3787.17822265625\n",
      "Eval_MinReturn : 3777.6220703125\n",
      "Eval_AverageEpLen : 1000.0\n",
      "Train_AverageReturn : 3775.553955078125\n",
      "Train_StdReturn : 0.0\n",
      "Train_MaxReturn : 3775.553955078125\n",
      "Train_MinReturn : 3775.553955078125\n",
      "Train_AverageEpLen : 1000.0\n",
      "Train_EnvstepsSoFar : 9347\n",
      "TimeSinceStart : 1543.7525362968445\n",
      "Initial_DataCollection_AverageReturn : 3772.67041015625\n",
      "Done logging...\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run_hw1.py \\\n",
    "--expert_policy_file ../policies/experts/Hopper.pkl \\\n",
    "--env_name Hopper-v2 --exp_name dagger_hopper --n_iter 10 \\\n",
    "--do_dagger --expert_data ../expert_data/expert_data_Hopper-v2.pkl \\\n",
    "--video_log_freq -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "run_hw1.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
